{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jy5OeJu9o8Rt"
   },
   "source": [
    "# üè¥‚Äç‚ò†Ô∏è Pirate Pain Classification Challenge\n",
    "\n",
    "> ‚öì *\"Even pirates feel pain ‚Äî let's teach the model to feel it too.\"*\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Table of Contents\n",
    "0. [README](#readme)  \n",
    "1. [Setup & Configuration](#setup)  \n",
    "2. [Data Loading](#data-loading)  \n",
    "3. [Import Libraries](#import-libraries)  \n",
    "4. [Data Preprocessing](#data-preprocessing)  \n",
    "5. [Sequence Building](#sequence-building)  \n",
    "6. [DataLoaders](#dataloaders)  \n",
    "7. [Network Hyperparameters](#hyperparameters)\n",
    "8. [Model Architecture](#model-architecture)  \n",
    "9. [Training Functions](#training-functions)  \n",
    "10. [Model Training](#model-training)  \n",
    "11. [Evaluation & Metrics](#evaluation)  \n",
    "12. [Model Loading & Final Testing](#model-loading)  \n",
    "13. [Competition Submission](#submission)\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è Quick Configuration Map\n",
    "\n",
    "> üß≠ *\"If ye seek to tweak the code, here be where to look!\"*\n",
    "\n",
    "- üß∫ **Batch Size:** ‚Üí [DataLoaders](#dataloaders)  \n",
    "- ‚öóÔ∏è **Hyperparameters:** ‚Üí [Network Hyperparameters](#hyperparameters)  \n",
    "- ü™û **Window Size & Stride:** ‚Üí [Sequence Building](#sequence-building)  \n",
    "- ‚öôÔ∏è **Model Type:** ‚Üí [Setup & Configuration](#setup)  \n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### üí∞ Treasure Storage ‚Äî Models & Submissions\n",
    "> üè¥‚Äç‚ò†Ô∏è *\"A wise pirate always knows where his treasure be buried ‚Äî guard yer models and submissions well!\"*\n",
    "\n",
    "- üíæ **Model & Submission Save/Load Path:** ‚Üí [Setup & Configuration](#setup)  \n",
    "  - üóÇÔ∏è Models be saved in a **`models/`** folder with the name:\n",
    "    **`experiment_name_dd-mm-HH-MM.pt`** (day-month-hour-minute).\n",
    "  - üìú Submissions be saved in a **`submissions/`** folder with the filename format:  \n",
    "    **`experiment_name_dd-mm-HH-MM.csv`** .\n",
    "  - üî° All related model parameters are saved in **`models/`** folder with the  name **`experiment_name_dd-mm-HH-MM_config.json`** .\n",
    "\n",
    "  \n",
    "  *‚ùóThe experiment name is set as **`RnnType_Bi_dd-mm-HH-MM`** or **`RnnType_dd-mm-HH-MM`** depending on if it is bidirectional or not*\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4oU_xMZwJUrZ"
   },
   "source": [
    "<a id=\"readme\"></a>\n",
    "## 0. Info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mjy_NO-5HPw3"
   },
   "source": [
    "\n",
    "\n",
    "This section lists all the main parameters that can be modified to control data loading, model behavior, and training.\n",
    "\n",
    "---\n",
    "\n",
    "### üìÅ File Paths\n",
    "| Variable | Description | Default Value |\n",
    "|-----------|--------------|----------------|\n",
    "| `TRAIN_DATA_PATH` | Training features | `'pirate_pain_train.csv'` |\n",
    "| `TRAIN_LABELS_PATH` | Training labels | `'pirate_pain_train_labels.csv'` |\n",
    "| `TEST_DATA_PATH` | Test set for inference | `'pirate_pain_test.csv'` *(optional)* |\n",
    "| `MODEL_SAVE_PATH` | Output model file | `'pirate_model.pt'` |\n",
    "| `RESULTS_FILE` | CSV for predictions | `'results_<date-time>.csv'` |\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Model & Architecture\n",
    "| Parameter | Description | Typical Values |\n",
    "|------------|--------------|----------------|\n",
    "| `model_type` | Choose model class | `'RNN'`, `'LSTM'`, `'GRU'`, `'ANN'` |\n",
    "| `input_size` | Number of features per time step | *auto-detected from data* |\n",
    "| `hidden_size` | Hidden layer size | `64`, `128`, `256` |\n",
    "| `num_layers` | Number of RNN layers | `1-4` |\n",
    "| `dropout` | Dropout probability | `0.2‚Äì0.5` |\n",
    "| `num_classes` | Output classes (pain levels) | *from label set* |\n",
    "\n",
    "---\n",
    "\n",
    "### üèãÔ∏è Training Hyperparameters\n",
    "| Parameter | Description | Default / Range |\n",
    "|------------|--------------|-----------------|\n",
    "| `batch_size` | Samples per batch | `512/2^n` |\n",
    "| `learning_rate` | Optimizer learning rate | `1e-3` |\n",
    "| `num_epochs` | Training iterations | `500` |\n",
    "| `optimizer` | Optimization algorithm | `'AdamW'` |\n",
    "| `criterion` | Loss function | `CrossEntropyLoss()` |\n",
    "| `seed` | Random seed for reproducibility | `42` |\n",
    "\n",
    "---\n",
    "\n",
    "### üì§ Inference\n",
    "| Parameter | Description |\n",
    "|------------|--------------|\n",
    "| `LOAD_MODEL_PATH` | Path to pretrained `.pt` model (optional) |\n",
    "| `save_results` | Whether to write output CSV | `True` |\n",
    "\n",
    "---\n",
    "\n",
    "> üí° *Tip:* Adjust hyperparameters in the ‚ÄúConfiguration‚Äù or ‚ÄúTraining Setup‚Äù cell before running the notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BZLBQ6tJrcBB"
   },
   "source": [
    "<a id=\"setup\"></a>\n",
    "## 1. Setup & Configuration\n",
    "\n",
    "*Optional: Connect to Google Drive (for Colab users)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nig16xZNnmnz",
    "outputId": "9ed125ad-b3f2-42bf-bab2-a47c01e23134"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount(\"/gdrive\")\n",
    "# current_dir = \"/gdrive/My\\\\ Drive/[2025 - 2026]\\\\ AN2DL/Challenge 1/Personal Challenge 1\"\n",
    "# %cd $current_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZL1iYHipaeMD"
   },
   "source": [
    "*Set Model Type*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "uWaTlLkTKgk5"
   },
   "outputs": [],
   "source": [
    "RNN_TYPE = 'LSTM'            # 'RNN', 'LSTM', or 'GRU'\n",
    "BIDIRECTIONAL = True        # True / False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "up7Qo6v-o8Ru"
   },
   "source": [
    "*Set Model Save Name*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wkBnTJHuo8Rv",
    "outputId": "2a2110ee-6044-4a9b-a42f-8b4a9ccddb3f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment name: LSTM_bi_13-11-17-17\n",
      "Submission filename: LSTM_bi_13-11-17-17.csv\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Get current date and time for submission filename\n",
    "current_datetime = datetime.now().strftime(\"%d-%m-%H-%M\")\n",
    "\n",
    "if BIDIRECTIONAL:\n",
    "    EXPERIMENT_NAME = f\"{RNN_TYPE}_bi_{current_datetime}\"\n",
    "else:\n",
    "    EXPERIMENT_NAME = f\"{RNN_TYPE}_{current_datetime}\"\n",
    "\n",
    "SUBMISSION_FILENAME = f\"{EXPERIMENT_NAME}.csv\"\n",
    "print(f\"Experiment name: {EXPERIMENT_NAME}\")\n",
    "print(f\"Submission filename: {SUBMISSION_FILENAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VdaKCgHVvvHX"
   },
   "source": [
    "<a id=\"data-loading\"></a>\n",
    "## 2. Data Loading\n",
    "\n",
    "Load training and test datasets from CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "oLyI938Jvn-J"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "X_train = pd.read_csv('an2dl2526c1/pirate_pain_train.csv')\n",
    "y_train = pd.read_csv('an2dl2526c1/pirate_pain_train_labels.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p3Lre5NWwCyk"
   },
   "source": [
    "<a id=\"import-libraries\"></a>\n",
    "## 3. Import Libraries\n",
    "\n",
    "Set random seeds for reproducibility and import all necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nt3AnE8SwJg1",
    "outputId": "c14a149d-422f-440c-8432-e1e6d7e46435"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'pkill' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "A subdirectory or file -p already exists.\n",
      "Error occurred while processing: -p.\n",
      "A subdirectory or file models already exists.\n",
      "Error occurred while processing: models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.10.0.dev20251109+cu128\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Set seed for reproducibility\n",
    "SEED = 42\n",
    "\n",
    "# Import necessary libraries\n",
    "import os\n",
    "\n",
    "# Set environment variables before importing modules\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "os.environ['MPLCONFIGDIR'] = os.getcwd() + '/configs/'\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=Warning)\n",
    "\n",
    "# Import necessary modules\n",
    "import logging\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Set seeds for random number generators in NumPy and Python\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# Import PyTorch\n",
    "import torch\n",
    "torch.manual_seed(SEED)\n",
    "from torch import nn\n",
    "from torchsummary import summary\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Directory configuration\n",
    "logs_dir = \"tensorboard\"\n",
    "models_dir = \"models\"\n",
    "\n",
    "\n",
    "\n",
    "# Model save/load paths\n",
    "MODEL_SAVE_PATH = f\"{models_dir}/{EXPERIMENT_NAME}_model.pt\"\n",
    "MODEL_LOAD_PATH = f\"{models_dir}/{EXPERIMENT_NAME}_model.pt\"\n",
    "\n",
    "!pkill -f tensorboard\n",
    "%load_ext tensorboard\n",
    "!mkdir -p {models_dir}\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Import other libraries\n",
    "import copy\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "from itertools import product\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "\n",
    "# Configure plot display settings\n",
    "sns.set(font_scale=1.4)\n",
    "sns.set_style('white')\n",
    "plt.rc('font', size=14)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vWnQz-p-xyhD"
   },
   "source": [
    "<a id=\"data-preprocessing\"></a>\n",
    "## 4. Data Preprocessing\n",
    "\n",
    "Explore data, split into train/val/test sets, normalize features, and encode labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 603
    },
    "id": "hHK2Aw7Ix4S8",
    "outputId": "2f8b219c-ba90-4688-94e5-40a17786f4bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (105760, 40)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_index</th>\n",
       "      <th>time</th>\n",
       "      <th>pain_survey_1</th>\n",
       "      <th>pain_survey_2</th>\n",
       "      <th>pain_survey_3</th>\n",
       "      <th>pain_survey_4</th>\n",
       "      <th>n_legs</th>\n",
       "      <th>n_hands</th>\n",
       "      <th>n_eyes</th>\n",
       "      <th>joint_00</th>\n",
       "      <th>...</th>\n",
       "      <th>joint_21</th>\n",
       "      <th>joint_22</th>\n",
       "      <th>joint_23</th>\n",
       "      <th>joint_24</th>\n",
       "      <th>joint_25</th>\n",
       "      <th>joint_26</th>\n",
       "      <th>joint_27</th>\n",
       "      <th>joint_28</th>\n",
       "      <th>joint_29</th>\n",
       "      <th>joint_30</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>two</td>\n",
       "      <td>two</td>\n",
       "      <td>two</td>\n",
       "      <td>1.094705</td>\n",
       "      <td>...</td>\n",
       "      <td>3.499558e-06</td>\n",
       "      <td>1.945042e-06</td>\n",
       "      <td>3.999558e-06</td>\n",
       "      <td>1.153299e-05</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.017592</td>\n",
       "      <td>0.013508</td>\n",
       "      <td>0.026798</td>\n",
       "      <td>0.027815</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>two</td>\n",
       "      <td>two</td>\n",
       "      <td>two</td>\n",
       "      <td>1.135183</td>\n",
       "      <td>...</td>\n",
       "      <td>3.976952e-07</td>\n",
       "      <td>6.765107e-07</td>\n",
       "      <td>6.019627e-06</td>\n",
       "      <td>4.643774e-08</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013352</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013377</td>\n",
       "      <td>0.013716</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>two</td>\n",
       "      <td>two</td>\n",
       "      <td>two</td>\n",
       "      <td>1.080745</td>\n",
       "      <td>...</td>\n",
       "      <td>1.533820e-07</td>\n",
       "      <td>1.698525e-07</td>\n",
       "      <td>1.446051e-06</td>\n",
       "      <td>2.424536e-06</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.016225</td>\n",
       "      <td>0.008110</td>\n",
       "      <td>0.024097</td>\n",
       "      <td>0.023105</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>two</td>\n",
       "      <td>two</td>\n",
       "      <td>two</td>\n",
       "      <td>0.938017</td>\n",
       "      <td>...</td>\n",
       "      <td>1.006865e-05</td>\n",
       "      <td>5.511079e-07</td>\n",
       "      <td>1.847597e-06</td>\n",
       "      <td>5.432416e-08</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011832</td>\n",
       "      <td>0.007450</td>\n",
       "      <td>0.028613</td>\n",
       "      <td>0.024648</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>two</td>\n",
       "      <td>two</td>\n",
       "      <td>two</td>\n",
       "      <td>1.090185</td>\n",
       "      <td>...</td>\n",
       "      <td>4.437266e-06</td>\n",
       "      <td>1.735459e-07</td>\n",
       "      <td>1.552722e-06</td>\n",
       "      <td>5.825366e-08</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.005360</td>\n",
       "      <td>0.002532</td>\n",
       "      <td>0.033026</td>\n",
       "      <td>0.025328</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>two</td>\n",
       "      <td>two</td>\n",
       "      <td>two</td>\n",
       "      <td>1.146031</td>\n",
       "      <td>...</td>\n",
       "      <td>1.073167e-06</td>\n",
       "      <td>1.753837e-07</td>\n",
       "      <td>2.957340e-07</td>\n",
       "      <td>6.217311e-08</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.006150</td>\n",
       "      <td>0.006444</td>\n",
       "      <td>0.033101</td>\n",
       "      <td>0.023767</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>two</td>\n",
       "      <td>two</td>\n",
       "      <td>two</td>\n",
       "      <td>1.025870</td>\n",
       "      <td>...</td>\n",
       "      <td>1.074800e-06</td>\n",
       "      <td>1.772156e-07</td>\n",
       "      <td>1.976558e-06</td>\n",
       "      <td>1.576086e-06</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.006495</td>\n",
       "      <td>0.006421</td>\n",
       "      <td>0.031804</td>\n",
       "      <td>0.019056</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>two</td>\n",
       "      <td>two</td>\n",
       "      <td>two</td>\n",
       "      <td>1.038597</td>\n",
       "      <td>...</td>\n",
       "      <td>8.829074e-07</td>\n",
       "      <td>1.790415e-07</td>\n",
       "      <td>2.210562e-06</td>\n",
       "      <td>1.485741e-06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015998</td>\n",
       "      <td>0.005397</td>\n",
       "      <td>0.035552</td>\n",
       "      <td>0.015732</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>two</td>\n",
       "      <td>two</td>\n",
       "      <td>two</td>\n",
       "      <td>0.984251</td>\n",
       "      <td>...</td>\n",
       "      <td>1.621055e-06</td>\n",
       "      <td>1.165161e-06</td>\n",
       "      <td>3.030164e-07</td>\n",
       "      <td>5.416678e-07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020539</td>\n",
       "      <td>0.008517</td>\n",
       "      <td>0.008635</td>\n",
       "      <td>0.015257</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>two</td>\n",
       "      <td>two</td>\n",
       "      <td>two</td>\n",
       "      <td>1.054999</td>\n",
       "      <td>...</td>\n",
       "      <td>1.609114e-06</td>\n",
       "      <td>3.959558e-06</td>\n",
       "      <td>2.017157e-06</td>\n",
       "      <td>1.154349e-06</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.007682</td>\n",
       "      <td>0.021383</td>\n",
       "      <td>0.034006</td>\n",
       "      <td>0.028966</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows √ó 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   sample_index  time  pain_survey_1  pain_survey_2  pain_survey_3  \\\n",
       "0             0     0              2              0              2   \n",
       "1             0     1              2              2              2   \n",
       "2             0     2              2              0              2   \n",
       "3             0     3              2              2              2   \n",
       "4             0     4              2              2              2   \n",
       "5             0     5              2              0              2   \n",
       "6             0     6              2              1              2   \n",
       "7             0     7              2              2              2   \n",
       "8             0     8              2              2              0   \n",
       "9             0     9              0              2              2   \n",
       "\n",
       "   pain_survey_4 n_legs n_hands n_eyes  joint_00  ...      joint_21  \\\n",
       "0              1    two     two    two  1.094705  ...  3.499558e-06   \n",
       "1              2    two     two    two  1.135183  ...  3.976952e-07   \n",
       "2              2    two     two    two  1.080745  ...  1.533820e-07   \n",
       "3              2    two     two    two  0.938017  ...  1.006865e-05   \n",
       "4              2    two     two    two  1.090185  ...  4.437266e-06   \n",
       "5              1    two     two    two  1.146031  ...  1.073167e-06   \n",
       "6              1    two     two    two  1.025870  ...  1.074800e-06   \n",
       "7              2    two     two    two  1.038597  ...  8.829074e-07   \n",
       "8              1    two     two    two  0.984251  ...  1.621055e-06   \n",
       "9              2    two     two    two  1.054999  ...  1.609114e-06   \n",
       "\n",
       "       joint_22      joint_23      joint_24  joint_25  joint_26  joint_27  \\\n",
       "0  1.945042e-06  3.999558e-06  1.153299e-05  0.000004  0.017592  0.013508   \n",
       "1  6.765107e-07  6.019627e-06  4.643774e-08  0.000000  0.013352  0.000000   \n",
       "2  1.698525e-07  1.446051e-06  2.424536e-06  0.000003  0.016225  0.008110   \n",
       "3  5.511079e-07  1.847597e-06  5.432416e-08  0.000000  0.011832  0.007450   \n",
       "4  1.735459e-07  1.552722e-06  5.825366e-08  0.000007  0.005360  0.002532   \n",
       "5  1.753837e-07  2.957340e-07  6.217311e-08  0.000007  0.006150  0.006444   \n",
       "6  1.772156e-07  1.976558e-06  1.576086e-06  0.000005  0.006495  0.006421   \n",
       "7  1.790415e-07  2.210562e-06  1.485741e-06  0.000000  0.015998  0.005397   \n",
       "8  1.165161e-06  3.030164e-07  5.416678e-07  0.000000  0.020539  0.008517   \n",
       "9  3.959558e-06  2.017157e-06  1.154349e-06  0.000007  0.007682  0.021383   \n",
       "\n",
       "   joint_28  joint_29  joint_30  \n",
       "0  0.026798  0.027815       0.5  \n",
       "1  0.013377  0.013716       0.5  \n",
       "2  0.024097  0.023105       0.5  \n",
       "3  0.028613  0.024648       0.5  \n",
       "4  0.033026  0.025328       0.5  \n",
       "5  0.033101  0.023767       0.5  \n",
       "6  0.031804  0.019056       0.5  \n",
       "7  0.035552  0.015732       0.5  \n",
       "8  0.008635  0.015257       0.5  \n",
       "9  0.034006  0.028966       0.5  \n",
       "\n",
       "[10 rows x 40 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the shape of the dataset\n",
    "print(f\"Dataset shape: {X_train.shape}\")\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "X_train.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GPMtvy5Fo8Rw"
   },
   "source": [
    "### 4.1 Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LfBKIdsrQDW3",
    "outputId": "eccea43a-a861-4fde-eb3c-e75ad62dcc47"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_legs': ['two', 'one+peg_leg'],\n",
       " 'n_hands': ['two', 'one+hook_hand'],\n",
       " 'n_eyes': ['two', 'one+eye_patch']}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Merge features and labels\n",
    "data = X_train.merge(y_train, on='sample_index')\n",
    "\n",
    "# Create a mapping dictionary to convert categorical labels to numerical values\n",
    "# map_dict = {'none': 0, 'one': 1, 'two': 2}\n",
    "# data['n_legs'] = data['n_legs'].map(map_dict)\n",
    "# data['n_hands'] = data['n_hands'].map(map_dict)\n",
    "# data['n_eyes'] = data['n_eyes'].map(map_dict)\n",
    "\n",
    "# print(\"Loading test dataset for final evaluation...\")\n",
    "\n",
    "cols = ['n_legs', 'n_hands', 'n_eyes']\n",
    "unique_values = {col: X_train[col].unique().tolist() for col in cols}\n",
    "\n",
    "unique_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wN-gWBGzuxfE",
    "outputId": "cbd124a1-e798-4357-ad4c-7e6b8e1fc0c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapped string columns to numeric values!\n",
      "   sample_index  time  pain_survey_1  pain_survey_2  pain_survey_3  \\\n",
      "0             0     0              2              0              2   \n",
      "1             0     1              2              2              2   \n",
      "2             0     2              2              0              2   \n",
      "3             0     3              2              2              2   \n",
      "4             0     4              2              2              2   \n",
      "\n",
      "   pain_survey_4  n_legs  n_hands  n_eyes  joint_00  ...      joint_22  \\\n",
      "0              1       2        2       2  1.094705  ...  1.945042e-06   \n",
      "1              2       2        2       2  1.135183  ...  6.765107e-07   \n",
      "2              2       2        2       2  1.080745  ...  1.698525e-07   \n",
      "3              2       2        2       2  0.938017  ...  5.511079e-07   \n",
      "4              2       2        2       2  1.090185  ...  1.735459e-07   \n",
      "\n",
      "   joint_23      joint_24  joint_25  joint_26  joint_27  joint_28  joint_29  \\\n",
      "0  0.000004  1.153299e-05  0.000004  0.017592  0.013508  0.026798  0.027815   \n",
      "1  0.000006  4.643774e-08  0.000000  0.013352  0.000000  0.013377  0.013716   \n",
      "2  0.000001  2.424536e-06  0.000003  0.016225  0.008110  0.024097  0.023105   \n",
      "3  0.000002  5.432416e-08  0.000000  0.011832  0.007450  0.028613  0.024648   \n",
      "4  0.000002  5.825366e-08  0.000007  0.005360  0.002532  0.033026  0.025328   \n",
      "\n",
      "   joint_30    label  \n",
      "0       0.5  no_pain  \n",
      "1       0.5  no_pain  \n",
      "2       0.5  no_pain  \n",
      "3       0.5  no_pain  \n",
      "4       0.5  no_pain  \n",
      "\n",
      "[5 rows x 41 columns]\n"
     ]
    }
   ],
   "source": [
    "map_dict = {'two': 2, 'one+peg_leg': 1}\n",
    "data['n_legs'] = data['n_legs'].map(map_dict)\n",
    "\n",
    "map_dict = {'two': 2, 'one+hook_hand': 1}\n",
    "data['n_hands'] = data['n_hands'].map(map_dict)\n",
    "\n",
    "map_dict = {'two': 2, 'one+eye_patch': 1}\n",
    "data['n_eyes'] = data['n_eyes'].map(map_dict)\n",
    "\n",
    "print(\"Mapped string columns to numeric values!\")\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 349
    },
    "id": "yiX54ULeXQdV",
    "outputId": "e878a7ba-ef8a-452c-99d9-ae0dda7bf952"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_index</th>\n",
       "      <th>time</th>\n",
       "      <th>pain_survey_1</th>\n",
       "      <th>pain_survey_2</th>\n",
       "      <th>pain_survey_3</th>\n",
       "      <th>pain_survey_4</th>\n",
       "      <th>n_legs</th>\n",
       "      <th>n_hands</th>\n",
       "      <th>n_eyes</th>\n",
       "      <th>joint_00</th>\n",
       "      <th>...</th>\n",
       "      <th>joint_21</th>\n",
       "      <th>joint_22</th>\n",
       "      <th>joint_23</th>\n",
       "      <th>joint_24</th>\n",
       "      <th>joint_25</th>\n",
       "      <th>joint_26</th>\n",
       "      <th>joint_27</th>\n",
       "      <th>joint_28</th>\n",
       "      <th>joint_29</th>\n",
       "      <th>joint_30</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>105760.000000</td>\n",
       "      <td>105760.000000</td>\n",
       "      <td>105760.000000</td>\n",
       "      <td>105760.000000</td>\n",
       "      <td>105760.000000</td>\n",
       "      <td>105760.000000</td>\n",
       "      <td>105760.000000</td>\n",
       "      <td>105760.000000</td>\n",
       "      <td>105760.000000</td>\n",
       "      <td>105760.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.057600e+05</td>\n",
       "      <td>1.057600e+05</td>\n",
       "      <td>1.057600e+05</td>\n",
       "      <td>1.057600e+05</td>\n",
       "      <td>1.057600e+05</td>\n",
       "      <td>105760.000000</td>\n",
       "      <td>105760.000000</td>\n",
       "      <td>105760.000000</td>\n",
       "      <td>105760.000000</td>\n",
       "      <td>105760.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>330.000000</td>\n",
       "      <td>79.500000</td>\n",
       "      <td>1.633746</td>\n",
       "      <td>1.654851</td>\n",
       "      <td>1.653640</td>\n",
       "      <td>1.663134</td>\n",
       "      <td>1.990923</td>\n",
       "      <td>1.990923</td>\n",
       "      <td>1.990923</td>\n",
       "      <td>0.943095</td>\n",
       "      <td>...</td>\n",
       "      <td>3.972126e-05</td>\n",
       "      <td>4.176794e-05</td>\n",
       "      <td>3.561780e-05</td>\n",
       "      <td>3.138109e-05</td>\n",
       "      <td>1.024604e-04</td>\n",
       "      <td>0.041905</td>\n",
       "      <td>0.058244</td>\n",
       "      <td>0.049886</td>\n",
       "      <td>0.062273</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>190.814948</td>\n",
       "      <td>46.187338</td>\n",
       "      <td>0.682423</td>\n",
       "      <td>0.669639</td>\n",
       "      <td>0.666649</td>\n",
       "      <td>0.661994</td>\n",
       "      <td>0.094841</td>\n",
       "      <td>0.094841</td>\n",
       "      <td>0.094841</td>\n",
       "      <td>0.202051</td>\n",
       "      <td>...</td>\n",
       "      <td>4.974496e-03</td>\n",
       "      <td>5.472244e-03</td>\n",
       "      <td>1.235450e-03</td>\n",
       "      <td>4.062914e-04</td>\n",
       "      <td>3.206128e-03</td>\n",
       "      <td>0.060293</td>\n",
       "      <td>0.079819</td>\n",
       "      <td>0.060773</td>\n",
       "      <td>0.072597</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.510494e-07</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.063144e-08</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000203</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>165.000000</td>\n",
       "      <td>39.750000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.828277</td>\n",
       "      <td>...</td>\n",
       "      <td>6.545878e-08</td>\n",
       "      <td>3.321650e-07</td>\n",
       "      <td>3.275038e-07</td>\n",
       "      <td>2.841805e-07</td>\n",
       "      <td>7.161332e-07</td>\n",
       "      <td>0.009885</td>\n",
       "      <td>0.012652</td>\n",
       "      <td>0.016290</td>\n",
       "      <td>0.019638</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>330.000000</td>\n",
       "      <td>79.500000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.005126</td>\n",
       "      <td>...</td>\n",
       "      <td>8.302747e-07</td>\n",
       "      <td>1.095971e-06</td>\n",
       "      <td>1.024209e-06</td>\n",
       "      <td>8.746147e-07</td>\n",
       "      <td>3.126723e-06</td>\n",
       "      <td>0.021898</td>\n",
       "      <td>0.031739</td>\n",
       "      <td>0.031843</td>\n",
       "      <td>0.039041</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>495.000000</td>\n",
       "      <td>119.250000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.081039</td>\n",
       "      <td>...</td>\n",
       "      <td>2.800090e-06</td>\n",
       "      <td>3.079465e-06</td>\n",
       "      <td>3.021830e-06</td>\n",
       "      <td>2.507548e-06</td>\n",
       "      <td>9.946107e-06</td>\n",
       "      <td>0.048579</td>\n",
       "      <td>0.071051</td>\n",
       "      <td>0.058741</td>\n",
       "      <td>0.079518</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>660.000000</td>\n",
       "      <td>159.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.407968</td>\n",
       "      <td>...</td>\n",
       "      <td>1.442198e+00</td>\n",
       "      <td>1.305001e+00</td>\n",
       "      <td>2.742411e-01</td>\n",
       "      <td>3.643074e-02</td>\n",
       "      <td>9.473540e-01</td>\n",
       "      <td>1.223617</td>\n",
       "      <td>1.187419</td>\n",
       "      <td>1.412037</td>\n",
       "      <td>1.370765</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows √ó 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        sample_index           time  pain_survey_1  pain_survey_2  \\\n",
       "count  105760.000000  105760.000000  105760.000000  105760.000000   \n",
       "mean      330.000000      79.500000       1.633746       1.654851   \n",
       "std       190.814948      46.187338       0.682423       0.669639   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%       165.000000      39.750000       2.000000       2.000000   \n",
       "50%       330.000000      79.500000       2.000000       2.000000   \n",
       "75%       495.000000     119.250000       2.000000       2.000000   \n",
       "max       660.000000     159.000000       2.000000       2.000000   \n",
       "\n",
       "       pain_survey_3  pain_survey_4         n_legs        n_hands  \\\n",
       "count  105760.000000  105760.000000  105760.000000  105760.000000   \n",
       "mean        1.653640       1.663134       1.990923       1.990923   \n",
       "std         0.666649       0.661994       0.094841       0.094841   \n",
       "min         0.000000       0.000000       1.000000       1.000000   \n",
       "25%         2.000000       2.000000       2.000000       2.000000   \n",
       "50%         2.000000       2.000000       2.000000       2.000000   \n",
       "75%         2.000000       2.000000       2.000000       2.000000   \n",
       "max         2.000000       2.000000       2.000000       2.000000   \n",
       "\n",
       "              n_eyes       joint_00  ...      joint_21      joint_22  \\\n",
       "count  105760.000000  105760.000000  ...  1.057600e+05  1.057600e+05   \n",
       "mean        1.990923       0.943095  ...  3.972126e-05  4.176794e-05   \n",
       "std         0.094841       0.202051  ...  4.974496e-03  5.472244e-03   \n",
       "min         1.000000       0.000000  ...  0.000000e+00  1.510494e-07   \n",
       "25%         2.000000       0.828277  ...  6.545878e-08  3.321650e-07   \n",
       "50%         2.000000       1.005126  ...  8.302747e-07  1.095971e-06   \n",
       "75%         2.000000       1.081039  ...  2.800090e-06  3.079465e-06   \n",
       "max         2.000000       1.407968  ...  1.442198e+00  1.305001e+00   \n",
       "\n",
       "           joint_23      joint_24      joint_25       joint_26       joint_27  \\\n",
       "count  1.057600e+05  1.057600e+05  1.057600e+05  105760.000000  105760.000000   \n",
       "mean   3.561780e-05  3.138109e-05  1.024604e-04       0.041905       0.058244   \n",
       "std    1.235450e-03  4.062914e-04  3.206128e-03       0.060293       0.079819   \n",
       "min    0.000000e+00  1.063144e-08  0.000000e+00       0.000203       0.000000   \n",
       "25%    3.275038e-07  2.841805e-07  7.161332e-07       0.009885       0.012652   \n",
       "50%    1.024209e-06  8.746147e-07  3.126723e-06       0.021898       0.031739   \n",
       "75%    3.021830e-06  2.507548e-06  9.946107e-06       0.048579       0.071051   \n",
       "max    2.742411e-01  3.643074e-02  9.473540e-01       1.223617       1.187419   \n",
       "\n",
       "            joint_28       joint_29  joint_30  \n",
       "count  105760.000000  105760.000000  105760.0  \n",
       "mean        0.049886       0.062273       0.5  \n",
       "std         0.060773       0.072597       0.0  \n",
       "min         0.000000       0.000000       0.5  \n",
       "25%         0.016290       0.019638       0.5  \n",
       "50%         0.031843       0.039041       0.5  \n",
       "75%         0.058741       0.079518       0.5  \n",
       "max         1.412037       1.370765       0.5  \n",
       "\n",
       "[8 rows x 40 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "qx24EFjkGb4o"
   },
   "outputs": [],
   "source": [
    "list_to_remove = ['n_legs', 'n_hands', 'n_eyes', 'joint_13', 'joint_14', 'joint_15', 'joint_16', 'joint_17', 'joint_18', 'joint_19', 'joint_20', 'joint_21', 'joint_22', 'joint_23', 'joint_24', 'joint_25', 'joint_30']\n",
    "\n",
    "if data.columns.isin(list_to_remove).any():\n",
    "  data = data.drop(columns=['n_legs', 'n_hands', 'n_eyes', 'joint_13', 'joint_14', 'joint_15', 'joint_16', 'joint_17', 'joint_18', 'joint_19', 'joint_20', 'joint_21', 'joint_22', 'joint_23', 'joint_24', 'joint_25', 'joint_30'])\n",
    "  data.head()\n",
    "else:\n",
    "  print(\"Usless features already removed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continuous features: 17\n",
      "Categorical features: 4\n"
     ]
    }
   ],
   "source": [
    "# Count the continouse and categorical features\n",
    "\n",
    "continuous_cols = [col for col in data.columns if col.startswith('joint_')]\n",
    "categorical_cols = [col for col in data.columns if col.startswith('pain_survey_')]\n",
    "\n",
    "print(f\"Continuous features: {len(continuous_cols)}\")\n",
    "print(f\"Categorical features: {len(categorical_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns after removal:\n",
      "['sample_index', 'time', 'pain_survey_1', 'pain_survey_2', 'pain_survey_3', 'pain_survey_4', 'joint_00', 'joint_01', 'joint_02', 'joint_03', 'joint_04', 'joint_05', 'joint_06', 'joint_07', 'joint_08', 'joint_09', 'joint_10', 'joint_11', 'joint_12', 'joint_26', 'joint_27', 'joint_28', 'joint_29', 'label']\n",
      "\n",
      "Number of features remaining: 24\n",
      "\n",
      "Categorical features identified: ['pain_survey_1', 'pain_survey_2', 'pain_survey_3', 'pain_survey_4']\n",
      "Remaining joint features: ['joint_00', 'joint_01', 'joint_02', 'joint_03', 'joint_04', 'joint_05', 'joint_06', 'joint_07', 'joint_08', 'joint_09', 'joint_10', 'joint_11', 'joint_12', 'joint_26', 'joint_27', 'joint_28', 'joint_29']\n",
      "Number of joint features: 17\n"
     ]
    }
   ],
   "source": [
    "# Check what columns remain after removal\n",
    "print(\"Columns after removal:\")\n",
    "print(data.columns.tolist())\n",
    "print(f\"\\nNumber of features remaining: {len(data.columns)}\")\n",
    "\n",
    "# Identify categorical features (int64 pain_survey columns)\n",
    "categorical_features = []\n",
    "for col in data.columns:\n",
    "    if 'pain_survey' in col and data[col].dtype == 'int64':\n",
    "        categorical_features.append(col)\n",
    "\n",
    "print(f\"\\nCategorical features identified: {categorical_features}\")\n",
    "\n",
    "# Check the remaining joint features\n",
    "joint_features = [col for col in data.columns if col.startswith('joint_')]\n",
    "print(f\"Remaining joint features: {joint_features}\")\n",
    "print(f\"Number of joint features: {len(joint_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding dimensions: {'pain_survey_1': 2, 'pain_survey_2': 2, 'pain_survey_3': 2, 'pain_survey_4': 2}\n",
      "Total embedding output size: 8\n",
      "Continuous features (joints): 17\n",
      "Total input size to LSTM: 25\n"
     ]
    }
   ],
   "source": [
    "# Define embedding dimensions for categorical features\n",
    "# For 3 unique values (0, 1, 2), embedding dimension of 2 is reasonable\n",
    "embedding_dims = {\n",
    "    'pain_survey_1': 2,\n",
    "    'pain_survey_2': 2,\n",
    "    'pain_survey_3': 2,\n",
    "    'pain_survey_4': 2\n",
    "}\n",
    "\n",
    "print(f\"Embedding dimensions: {embedding_dims}\")\n",
    "print(f\"Total embedding output size: {sum(embedding_dims.values())}\")\n",
    "print(f\"Continuous features (joints): {len(joint_features)}\")\n",
    "print(f\"Total input size to LSTM: {sum(embedding_dims.values()) + len(joint_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "id": "6pqnMiAQHmlK",
    "outputId": "8c382c77-2a6c-4878-91cf-6e0db2036d78"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_index</th>\n",
       "      <th>time</th>\n",
       "      <th>pain_survey_1</th>\n",
       "      <th>pain_survey_2</th>\n",
       "      <th>pain_survey_3</th>\n",
       "      <th>pain_survey_4</th>\n",
       "      <th>joint_00</th>\n",
       "      <th>joint_01</th>\n",
       "      <th>joint_02</th>\n",
       "      <th>joint_03</th>\n",
       "      <th>...</th>\n",
       "      <th>joint_08</th>\n",
       "      <th>joint_09</th>\n",
       "      <th>joint_10</th>\n",
       "      <th>joint_11</th>\n",
       "      <th>joint_12</th>\n",
       "      <th>joint_26</th>\n",
       "      <th>joint_27</th>\n",
       "      <th>joint_28</th>\n",
       "      <th>joint_29</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.094705</td>\n",
       "      <td>0.985281</td>\n",
       "      <td>1.018302</td>\n",
       "      <td>1.010385</td>\n",
       "      <td>...</td>\n",
       "      <td>0.712989</td>\n",
       "      <td>1.050142</td>\n",
       "      <td>0.529555</td>\n",
       "      <td>0.447370</td>\n",
       "      <td>1.091046</td>\n",
       "      <td>0.017592</td>\n",
       "      <td>0.013508</td>\n",
       "      <td>0.026798</td>\n",
       "      <td>0.027815</td>\n",
       "      <td>no_pain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.135183</td>\n",
       "      <td>1.021175</td>\n",
       "      <td>0.994343</td>\n",
       "      <td>1.052364</td>\n",
       "      <td>...</td>\n",
       "      <td>0.722685</td>\n",
       "      <td>1.060313</td>\n",
       "      <td>0.446810</td>\n",
       "      <td>0.414432</td>\n",
       "      <td>1.045862</td>\n",
       "      <td>0.013352</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013377</td>\n",
       "      <td>0.013716</td>\n",
       "      <td>no_pain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.080745</td>\n",
       "      <td>0.962842</td>\n",
       "      <td>1.009588</td>\n",
       "      <td>0.977169</td>\n",
       "      <td>...</td>\n",
       "      <td>0.668043</td>\n",
       "      <td>1.011410</td>\n",
       "      <td>0.432499</td>\n",
       "      <td>0.431535</td>\n",
       "      <td>1.088221</td>\n",
       "      <td>0.016225</td>\n",
       "      <td>0.008110</td>\n",
       "      <td>0.024097</td>\n",
       "      <td>0.023105</td>\n",
       "      <td>no_pain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.938017</td>\n",
       "      <td>1.081592</td>\n",
       "      <td>0.998021</td>\n",
       "      <td>0.987283</td>\n",
       "      <td>...</td>\n",
       "      <td>0.702085</td>\n",
       "      <td>1.047223</td>\n",
       "      <td>0.478806</td>\n",
       "      <td>0.420665</td>\n",
       "      <td>1.096832</td>\n",
       "      <td>0.011832</td>\n",
       "      <td>0.007450</td>\n",
       "      <td>0.028613</td>\n",
       "      <td>0.024648</td>\n",
       "      <td>no_pain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.090185</td>\n",
       "      <td>1.032145</td>\n",
       "      <td>1.008710</td>\n",
       "      <td>0.963658</td>\n",
       "      <td>...</td>\n",
       "      <td>0.712197</td>\n",
       "      <td>1.044731</td>\n",
       "      <td>0.452906</td>\n",
       "      <td>0.476537</td>\n",
       "      <td>1.103968</td>\n",
       "      <td>0.005360</td>\n",
       "      <td>0.002532</td>\n",
       "      <td>0.033026</td>\n",
       "      <td>0.025328</td>\n",
       "      <td>no_pain</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   sample_index  time  pain_survey_1  pain_survey_2  pain_survey_3  \\\n",
       "0             0     0              2              0              2   \n",
       "1             0     1              2              2              2   \n",
       "2             0     2              2              0              2   \n",
       "3             0     3              2              2              2   \n",
       "4             0     4              2              2              2   \n",
       "\n",
       "   pain_survey_4  joint_00  joint_01  joint_02  joint_03  ...  joint_08  \\\n",
       "0              1  1.094705  0.985281  1.018302  1.010385  ...  0.712989   \n",
       "1              2  1.135183  1.021175  0.994343  1.052364  ...  0.722685   \n",
       "2              2  1.080745  0.962842  1.009588  0.977169  ...  0.668043   \n",
       "3              2  0.938017  1.081592  0.998021  0.987283  ...  0.702085   \n",
       "4              2  1.090185  1.032145  1.008710  0.963658  ...  0.712197   \n",
       "\n",
       "   joint_09  joint_10  joint_11  joint_12  joint_26  joint_27  joint_28  \\\n",
       "0  1.050142  0.529555  0.447370  1.091046  0.017592  0.013508  0.026798   \n",
       "1  1.060313  0.446810  0.414432  1.045862  0.013352  0.000000  0.013377   \n",
       "2  1.011410  0.432499  0.431535  1.088221  0.016225  0.008110  0.024097   \n",
       "3  1.047223  0.478806  0.420665  1.096832  0.011832  0.007450  0.028613   \n",
       "4  1.044731  0.452906  0.476537  1.103968  0.005360  0.002532  0.033026   \n",
       "\n",
       "   joint_29    label  \n",
       "0  0.027815  no_pain  \n",
       "1  0.013716  no_pain  \n",
       "2  0.023105  no_pain  \n",
       "3  0.024648  no_pain  \n",
       "4  0.025328  no_pain  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in data:\n",
      "['sample_index', 'time', 'pain_survey_1', 'pain_survey_2', 'pain_survey_3', 'pain_survey_4', 'joint_00', 'joint_01', 'joint_02', 'joint_03', 'joint_04', 'joint_05', 'joint_06', 'joint_07', 'joint_08', 'joint_09', 'joint_10', 'joint_11', 'joint_12', 'joint_26', 'joint_27', 'joint_28', 'joint_29', 'label']\n",
      "\n",
      "Data types:\n",
      "sample_index       int64\n",
      "time               int64\n",
      "pain_survey_1      int64\n",
      "pain_survey_2      int64\n",
      "pain_survey_3      int64\n",
      "pain_survey_4      int64\n",
      "joint_00         float64\n",
      "joint_01         float64\n",
      "joint_02         float64\n",
      "joint_03         float64\n",
      "joint_04         float64\n",
      "joint_05         float64\n",
      "joint_06         float64\n",
      "joint_07         float64\n",
      "joint_08         float64\n",
      "joint_09         float64\n",
      "joint_10         float64\n",
      "joint_11         float64\n",
      "joint_12         float64\n",
      "joint_26         float64\n",
      "joint_27         float64\n",
      "joint_28         float64\n",
      "joint_29         float64\n",
      "label             object\n",
      "dtype: object\n",
      "\n",
      "Pain survey columns: ['pain_survey_1', 'pain_survey_2', 'pain_survey_3', 'pain_survey_4']\n",
      "\n",
      "Unique values in pain_survey_1:\n",
      "[2 0 1]\n",
      "Number of unique values: 3\n",
      "\n",
      "Unique values in pain_survey_2:\n",
      "[0 2 1]\n",
      "Number of unique values: 3\n",
      "\n",
      "Unique values in pain_survey_3:\n",
      "[2 0 1]\n",
      "Number of unique values: 3\n",
      "\n",
      "Unique values in pain_survey_4:\n",
      "[1 2 0]\n",
      "Number of unique values: 3\n"
     ]
    }
   ],
   "source": [
    "# Check the columns and their types\n",
    "print(\"Columns in data:\")\n",
    "print(data.columns.tolist())\n",
    "print(\"\\nData types:\")\n",
    "print(data.dtypes)\n",
    "\n",
    "# Check for categorical features (pain_survey columns)\n",
    "pain_survey_cols = [col for col in data.columns if 'pain_survey' in col]\n",
    "print(f\"\\nPain survey columns: {pain_survey_cols}\")\n",
    "\n",
    "# Check unique values in pain_survey columns\n",
    "for col in pain_survey_cols:\n",
    "    print(f\"\\nUnique values in {col}:\")\n",
    "    print(data[col].unique())\n",
    "    print(f\"Number of unique values: {data[col].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ojlNM9r8u8x8"
   },
   "source": [
    "## Preprocessing testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tBEcYysvu8A9",
    "outputId": "70da8c96-e590-4aeb-886a-1ffebd59d8bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapped string columns to numeric values!\n",
      "   sample_index  time  pain_survey_1  pain_survey_2  pain_survey_3  \\\n",
      "0             0     0              2              2              2   \n",
      "1             0     1              2              2              2   \n",
      "2             0     2              2              2              2   \n",
      "3             0     3              1              2              2   \n",
      "4             0     4              2              2              2   \n",
      "\n",
      "   pain_survey_4  n_legs  n_hands  n_eyes  joint_00  ...  joint_21  joint_22  \\\n",
      "0              2       2        2       2  0.842535  ...  0.000003  0.000004   \n",
      "1              2       2        2       2  0.898836  ...  0.000003  0.000004   \n",
      "2              2       2        2       2  0.957765  ...  0.000006  0.000004   \n",
      "3              2       2        2       2  0.832596  ...  0.000005  0.000004   \n",
      "4              0       2        2       2  0.805971  ...  0.000006  0.000004   \n",
      "\n",
      "   joint_23  joint_24  joint_25  joint_26  joint_27  joint_28  joint_29  \\\n",
      "0  0.000003  0.000003  0.000068  0.019372  0.066324  0.022228  0.013576   \n",
      "1  0.000004  0.000003  0.000029  0.069747  0.080417  0.023650  0.038793   \n",
      "2  0.000009  0.000004  0.000008  0.054968  0.058811  0.027023  0.054202   \n",
      "3  0.000003  0.000004  0.000015  0.048695  0.047128  0.016151  0.024983   \n",
      "4  0.000003  0.000003  0.000008  0.019762  0.031116  0.015618  0.017931   \n",
      "\n",
      "   joint_30  \n",
      "0       0.5  \n",
      "1       0.5  \n",
      "2       0.5  \n",
      "3       0.5  \n",
      "4       0.5  \n",
      "\n",
      "[5 rows x 40 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load the actual test dataset (this doesn't have labels)\n",
    "X_test_final_df = pd.read_csv('an2dl2526c1/pirate_pain_test.csv')\n",
    "\n",
    "# Map string columns to numeric values first\n",
    "map_dict = {'two': 2, 'one+peg_leg': 1}\n",
    "X_test_final_df['n_legs'] = X_test_final_df['n_legs'].map(map_dict)\n",
    "\n",
    "map_dict = {'two': 2, 'one+hook_hand': 1}\n",
    "X_test_final_df['n_hands'] = X_test_final_df['n_hands'].map(map_dict)\n",
    "\n",
    "map_dict = {'two': 2, 'one+eye_patch': 1}\n",
    "X_test_final_df['n_eyes'] = X_test_final_df['n_eyes'].map(map_dict)\n",
    "\n",
    "print(\"Mapped string columns to numeric values!\")\n",
    "print(X_test_final_df.head())\n",
    "\n",
    "# Now convert inputs from float64 to float32\n",
    "X_test_final_df = X_test_final_df.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "OoKAZSkvvEam"
   },
   "outputs": [],
   "source": [
    "def build_sequences_test(df, window=200, stride=200):\n",
    "    assert window % stride == 0\n",
    "\n",
    "    dataset = []\n",
    "\n",
    "    # Get feature columns (exclude sample_index and time)\n",
    "    columns = [col for col in df.columns if col not in ['sample_index', 'time']]\n",
    "\n",
    "    for id in df['sample_index'].unique():\n",
    "        temp = df[df['sample_index'] == id][columns].values\n",
    "\n",
    "        # Padding\n",
    "        padding_len = (window - len(temp) % window) % window\n",
    "        padding = np.zeros((padding_len, len(columns)), dtype='float32')\n",
    "        temp = np.concatenate((temp, padding))\n",
    "\n",
    "        # Build windows\n",
    "        idx = 0\n",
    "        while idx + window <= len(temp):\n",
    "            dataset.append(temp[idx:idx + window])\n",
    "            idx += stride\n",
    "\n",
    "    return np.array(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "id": "EEu1WYIgvMvl",
    "outputId": "738c6bf8-cb8b-473c-f017-58efdd1323de"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_index</th>\n",
       "      <th>time</th>\n",
       "      <th>pain_survey_1</th>\n",
       "      <th>pain_survey_2</th>\n",
       "      <th>pain_survey_3</th>\n",
       "      <th>pain_survey_4</th>\n",
       "      <th>joint_00</th>\n",
       "      <th>joint_01</th>\n",
       "      <th>joint_02</th>\n",
       "      <th>joint_03</th>\n",
       "      <th>...</th>\n",
       "      <th>joint_07</th>\n",
       "      <th>joint_08</th>\n",
       "      <th>joint_09</th>\n",
       "      <th>joint_10</th>\n",
       "      <th>joint_11</th>\n",
       "      <th>joint_12</th>\n",
       "      <th>joint_26</th>\n",
       "      <th>joint_27</th>\n",
       "      <th>joint_28</th>\n",
       "      <th>joint_29</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.561563</td>\n",
       "      <td>0.553352</td>\n",
       "      <td>0.419037</td>\n",
       "      <td>0.270175</td>\n",
       "      <td>...</td>\n",
       "      <td>0.654303</td>\n",
       "      <td>0.737832</td>\n",
       "      <td>0.742275</td>\n",
       "      <td>0.100076</td>\n",
       "      <td>0.146564</td>\n",
       "      <td>0.745300</td>\n",
       "      <td>0.014909</td>\n",
       "      <td>0.045098</td>\n",
       "      <td>0.012882</td>\n",
       "      <td>0.010178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.599088</td>\n",
       "      <td>0.532067</td>\n",
       "      <td>0.461325</td>\n",
       "      <td>0.327922</td>\n",
       "      <td>...</td>\n",
       "      <td>0.684443</td>\n",
       "      <td>0.772454</td>\n",
       "      <td>0.710705</td>\n",
       "      <td>0.103457</td>\n",
       "      <td>0.174403</td>\n",
       "      <td>0.594262</td>\n",
       "      <td>0.053679</td>\n",
       "      <td>0.055375</td>\n",
       "      <td>0.013892</td>\n",
       "      <td>0.029085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.638365</td>\n",
       "      <td>0.583960</td>\n",
       "      <td>0.445804</td>\n",
       "      <td>0.308796</td>\n",
       "      <td>...</td>\n",
       "      <td>0.676488</td>\n",
       "      <td>0.799646</td>\n",
       "      <td>0.722061</td>\n",
       "      <td>0.143175</td>\n",
       "      <td>0.159973</td>\n",
       "      <td>0.652024</td>\n",
       "      <td>0.042305</td>\n",
       "      <td>0.039620</td>\n",
       "      <td>0.016286</td>\n",
       "      <td>0.040638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.554938</td>\n",
       "      <td>0.488719</td>\n",
       "      <td>0.443494</td>\n",
       "      <td>0.355023</td>\n",
       "      <td>...</td>\n",
       "      <td>0.650563</td>\n",
       "      <td>0.738087</td>\n",
       "      <td>0.709363</td>\n",
       "      <td>0.141007</td>\n",
       "      <td>0.167449</td>\n",
       "      <td>0.709558</td>\n",
       "      <td>0.037477</td>\n",
       "      <td>0.031101</td>\n",
       "      <td>0.008568</td>\n",
       "      <td>0.018730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.537192</td>\n",
       "      <td>0.528780</td>\n",
       "      <td>0.413159</td>\n",
       "      <td>0.363199</td>\n",
       "      <td>...</td>\n",
       "      <td>0.653239</td>\n",
       "      <td>0.703021</td>\n",
       "      <td>0.681513</td>\n",
       "      <td>0.140234</td>\n",
       "      <td>0.186249</td>\n",
       "      <td>0.590142</td>\n",
       "      <td>0.015210</td>\n",
       "      <td>0.019426</td>\n",
       "      <td>0.008189</td>\n",
       "      <td>0.013444</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   sample_index  time  pain_survey_1  pain_survey_2  pain_survey_3  \\\n",
       "0           0.0   0.0            1.0            1.0            1.0   \n",
       "1           0.0   1.0            1.0            1.0            1.0   \n",
       "2           0.0   2.0            1.0            1.0            1.0   \n",
       "3           0.0   3.0            0.5            1.0            1.0   \n",
       "4           0.0   4.0            1.0            1.0            1.0   \n",
       "\n",
       "   pain_survey_4  joint_00  joint_01  joint_02  joint_03  ...  joint_07  \\\n",
       "0            1.0  0.561563  0.553352  0.419037  0.270175  ...  0.654303   \n",
       "1            1.0  0.599088  0.532067  0.461325  0.327922  ...  0.684443   \n",
       "2            1.0  0.638365  0.583960  0.445804  0.308796  ...  0.676488   \n",
       "3            1.0  0.554938  0.488719  0.443494  0.355023  ...  0.650563   \n",
       "4            0.0  0.537192  0.528780  0.413159  0.363199  ...  0.653239   \n",
       "\n",
       "   joint_08  joint_09  joint_10  joint_11  joint_12  joint_26  joint_27  \\\n",
       "0  0.737832  0.742275  0.100076  0.146564  0.745300  0.014909  0.045098   \n",
       "1  0.772454  0.710705  0.103457  0.174403  0.594262  0.053679  0.055375   \n",
       "2  0.799646  0.722061  0.143175  0.159973  0.652024  0.042305  0.039620   \n",
       "3  0.738087  0.709363  0.141007  0.167449  0.709558  0.037477  0.031101   \n",
       "4  0.703021  0.681513  0.140234  0.186249  0.590142  0.015210  0.019426   \n",
       "\n",
       "   joint_28  joint_29  \n",
       "0  0.012882  0.010178  \n",
       "1  0.013892  0.029085  \n",
       "2  0.016286  0.040638  \n",
       "3  0.008568  0.018730  \n",
       "4  0.008189  0.013444  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ------ Normalize test data ------\n",
    "\n",
    "list_to_remove = ['n_legs', 'n_hands', 'n_eyes', 'joint_13', 'joint_14', 'joint_15', 'joint_16', 'joint_17', 'joint_18', 'joint_19', 'joint_20', 'joint_21', 'joint_22', 'joint_23', 'joint_24', 'joint_25', 'joint_30']\n",
    "\n",
    "if X_test_final_df.columns.isin(list_to_remove).any():\n",
    "    X_test_final_df = X_test_final_df.drop(columns=['n_legs', 'n_hands', 'n_eyes', 'joint_13', 'joint_14', 'joint_15', 'joint_16', 'joint_17', 'joint_18', 'joint_19', 'joint_20', 'joint_21', 'joint_22', 'joint_23', 'joint_24', 'joint_25', 'joint_30'])\n",
    "else:\n",
    "  print(\"Usless features already removed\")\n",
    "\n",
    "# --- Load and preprocess the actual test dataset ---\n",
    "# Define the columns to be normalised (use training statistics for proper normalization)\n",
    "# Exclude 'sample_index', 'time', and 'label' as they were excluded during training sequence building\n",
    "# Also exclude 'joint_30' as it was removed from training data\n",
    "scale_columns = [col for col in X_test_final_df.columns\n",
    "                 if col != 'sample_index' and col != 'time' and col != 'joint_30']\n",
    "\n",
    "# Calculate the minimum and maximum values from the training data only\n",
    "mins_train = X_test_final_df[scale_columns].min()\n",
    "maxs_train = X_test_final_df[scale_columns].max()\n",
    "\n",
    "# Apply normalisation to the specified columns in all datasets\n",
    "for column in scale_columns:\n",
    "    # Normalise the testing set\n",
    "    if maxs_train[column] != mins_train[column]:\n",
    "      X_test_final_df[column] = (X_test_final_df[column] - mins_train[column]) / (maxs_train[column] - mins_train[column])\n",
    "\n",
    "X_test_final_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NlDwxJ38o8Rw"
   },
   "source": [
    "### 4.2 Categorical Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UqptiNjNQDW3",
    "outputId": "e5bc0f24-f309-4049-e40e-f6c7c06dc1c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original label distribution: {'no_pain': 511, 'low_pain': 94, 'high_pain': 56}\n",
      "Train label counts before balancing: {'no_pain': 385, 'low_pain': 71, 'high_pain': 42}\n",
      "Train label counts after balancing: {'no_pain': 385, 'low_pain': 71, 'high_pain': 42}\n",
      "Validation and Test distributions:\n",
      "Validation label counts: {'no_pain': 124, 'low_pain': 22, 'high_pain': 14}\n",
      "Test label counts: {'no_pain': 2, 'low_pain': 1}\n",
      "Label proportions:\n",
      "Train:\n",
      " label\n",
      "no_pain      0.773092\n",
      "low_pain     0.142570\n",
      "high_pain    0.084337\n",
      "Name: proportion, dtype: float64\n",
      "Val:\n",
      " label\n",
      "no_pain      0.7750\n",
      "low_pain     0.1375\n",
      "high_pain    0.0875\n",
      "Name: proportion, dtype: float64\n",
      "Test:\n",
      " label\n",
      "no_pain     0.666667\n",
      "low_pain    0.333333\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# df has columns: ['sample_index', 'label']\n",
    "N_NO_PAIN_KEEP = 1000   # how many \"no_pain\" pirates to keep in the training set (lower it to have a more balanced distribution of the labels)\n",
    "N_LOW_PAIN_KEEP = 150   # how many \"low_pain\" pirates to keep in the training set\n",
    "N_VAL_USERS = 160\n",
    "N_TEST_USERS = 3\n",
    "\n",
    "# --- Step 1: Compute each user's dominant label (or label distribution)\n",
    "user_labels = (\n",
    "    data.groupby('sample_index')['label']\n",
    "    .agg(lambda x: x.value_counts().index[0])  # dominant label per user\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "print(\"Original label distribution:\", user_labels['label'].value_counts().to_dict())\n",
    "\n",
    "# --- Step 2: Split into train/val/test keeping real label proportions\n",
    "train_users, temp_users = train_test_split(\n",
    "    user_labels['sample_index'],\n",
    "    test_size=(N_VAL_USERS + N_TEST_USERS) / len(user_labels),\n",
    "    stratify=user_labels['label'],\n",
    "    random_state=None\n",
    ")\n",
    "\n",
    "# Split temp into val/test (also stratified)\n",
    "temp_labels = user_labels[user_labels['sample_index'].isin(temp_users)]\n",
    "\n",
    "val_users, test_users = train_test_split(\n",
    "    temp_labels['sample_index'],\n",
    "    test_size=N_TEST_USERS / (N_VAL_USERS + N_TEST_USERS),\n",
    "    stratify=temp_labels['label'],\n",
    "    random_state=None\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# === Step 3: Create a partially balanced training set keeping a fixed number of pirates per label ===\n",
    "train_labels = user_labels[user_labels['sample_index'].isin(train_users)]\n",
    "\n",
    "# Count of how many pirates are present for each label in training set\n",
    "label_counts = train_labels['label'].value_counts()\n",
    "print(\"Train label counts before balancing:\", label_counts.to_dict())\n",
    "\n",
    "# --- Custom undersampling logic ---\n",
    "rng = random.Random()\n",
    "\n",
    "no_pain_users = train_labels[train_labels['label'] == 'no_pain']['sample_index'].tolist()\n",
    "low_pain_users = train_labels[train_labels['label'] == 'low_pain']['sample_index'].tolist()\n",
    "high_pain_users = train_labels[train_labels['label'] == 'high_pain']['sample_index'].tolist()\n",
    "\n",
    "# Choose how many to keep for each label\n",
    "no_pain_keep = min(N_NO_PAIN_KEEP, len(no_pain_users))\n",
    "low_pain_keep = min(N_LOW_PAIN_KEEP, len(low_pain_users))\n",
    "high_pain_keep = len(high_pain_users)  # keep all high_pain pirates\n",
    "\n",
    "selected_no_pain = rng.sample(no_pain_users, no_pain_keep)\n",
    "selected_low_pain = rng.sample(low_pain_users, low_pain_keep)\n",
    "selected_high_pain = high_pain_users  # keep all\n",
    "\n",
    "# Combine the selected users\n",
    "selected_users = selected_no_pain + selected_low_pain + selected_high_pain\n",
    "balanced_train_labels = train_labels[train_labels['sample_index'].isin(selected_users)]\n",
    "\n",
    "print(\"Train label counts after balancing:\", balanced_train_labels['label'].value_counts().to_dict())\n",
    "\n",
    "train_users = balanced_train_labels['sample_index']\n",
    "\n",
    "# Compute validation and test label distributions\n",
    "val_label_counts = user_labels[user_labels['sample_index'].isin(val_users)]['label'].value_counts().to_dict()\n",
    "test_label_counts = user_labels[user_labels['sample_index'].isin(test_users)]['label'].value_counts().to_dict()\n",
    "print(\"Validation and Test distributions:\")\n",
    "print(\"Validation label counts:\", val_label_counts)\n",
    "print(\"Test label counts:\", test_label_counts)\n",
    "\n",
    "\n",
    "# --- Step 4: Filter your main df\n",
    "df_train = data[data['sample_index'].isin(train_users)]\n",
    "df_val = data[data['sample_index'].isin(val_users)]\n",
    "df_test = data[data['sample_index'].isin(test_users)]\n",
    "\n",
    "# --- Step 5: Check label proportions\n",
    "print(\"Label proportions:\")\n",
    "print(\"Train:\\n\", df_train['label'].value_counts(normalize=True))\n",
    "print(\"Val:\\n\", df_val['label'].value_counts(normalize=True))\n",
    "print(\"Test:\\n\", df_test['label'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AUwlOzWBo8Rx"
   },
   "source": [
    "### 4.3 Stratified Train/Val/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kI13WygTQDW4",
    "outputId": "670777dc-871d-499d-d272-aa88a763d79d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((79680, 24), (25600, 24), (480, 24))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape, df_val.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PsE1KsTTQDW4",
    "outputId": "ab26e77b-f6f1-48f8-e143-775fc2382e2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pirates in training set: 498\n",
      "Total pirates in validation set: 160\n",
      "Total pirates in test set: 3\n"
     ]
    }
   ],
   "source": [
    "# Print the total number of pirates for each dataset\n",
    "print(f\"Total pirates in training set: {df_train['sample_index'].nunique()}\")\n",
    "print(f\"Total pirates in validation set: {df_val['sample_index'].nunique()}\")\n",
    "print(f\"Total pirates in test set: {df_test['sample_index'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "id": "A35PiqdxJsLQ",
    "outputId": "9c343fb9-d4d3-4269-82f3-e7066f0b250a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_index</th>\n",
       "      <th>time</th>\n",
       "      <th>pain_survey_1</th>\n",
       "      <th>pain_survey_2</th>\n",
       "      <th>pain_survey_3</th>\n",
       "      <th>pain_survey_4</th>\n",
       "      <th>joint_00</th>\n",
       "      <th>joint_01</th>\n",
       "      <th>joint_02</th>\n",
       "      <th>joint_03</th>\n",
       "      <th>...</th>\n",
       "      <th>joint_08</th>\n",
       "      <th>joint_09</th>\n",
       "      <th>joint_10</th>\n",
       "      <th>joint_11</th>\n",
       "      <th>joint_12</th>\n",
       "      <th>joint_26</th>\n",
       "      <th>joint_27</th>\n",
       "      <th>joint_28</th>\n",
       "      <th>joint_29</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.094705</td>\n",
       "      <td>0.985281</td>\n",
       "      <td>1.018302</td>\n",
       "      <td>1.010385</td>\n",
       "      <td>...</td>\n",
       "      <td>0.712989</td>\n",
       "      <td>1.050142</td>\n",
       "      <td>0.529555</td>\n",
       "      <td>0.447370</td>\n",
       "      <td>1.091046</td>\n",
       "      <td>0.017592</td>\n",
       "      <td>0.013508</td>\n",
       "      <td>0.026798</td>\n",
       "      <td>0.027815</td>\n",
       "      <td>no_pain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.135183</td>\n",
       "      <td>1.021175</td>\n",
       "      <td>0.994343</td>\n",
       "      <td>1.052364</td>\n",
       "      <td>...</td>\n",
       "      <td>0.722685</td>\n",
       "      <td>1.060313</td>\n",
       "      <td>0.446810</td>\n",
       "      <td>0.414432</td>\n",
       "      <td>1.045862</td>\n",
       "      <td>0.013352</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013377</td>\n",
       "      <td>0.013716</td>\n",
       "      <td>no_pain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.080745</td>\n",
       "      <td>0.962842</td>\n",
       "      <td>1.009588</td>\n",
       "      <td>0.977169</td>\n",
       "      <td>...</td>\n",
       "      <td>0.668043</td>\n",
       "      <td>1.011410</td>\n",
       "      <td>0.432499</td>\n",
       "      <td>0.431535</td>\n",
       "      <td>1.088221</td>\n",
       "      <td>0.016225</td>\n",
       "      <td>0.008110</td>\n",
       "      <td>0.024097</td>\n",
       "      <td>0.023105</td>\n",
       "      <td>no_pain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.938017</td>\n",
       "      <td>1.081592</td>\n",
       "      <td>0.998021</td>\n",
       "      <td>0.987283</td>\n",
       "      <td>...</td>\n",
       "      <td>0.702085</td>\n",
       "      <td>1.047223</td>\n",
       "      <td>0.478806</td>\n",
       "      <td>0.420665</td>\n",
       "      <td>1.096832</td>\n",
       "      <td>0.011832</td>\n",
       "      <td>0.007450</td>\n",
       "      <td>0.028613</td>\n",
       "      <td>0.024648</td>\n",
       "      <td>no_pain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.090185</td>\n",
       "      <td>1.032145</td>\n",
       "      <td>1.008710</td>\n",
       "      <td>0.963658</td>\n",
       "      <td>...</td>\n",
       "      <td>0.712197</td>\n",
       "      <td>1.044731</td>\n",
       "      <td>0.452906</td>\n",
       "      <td>0.476537</td>\n",
       "      <td>1.103968</td>\n",
       "      <td>0.005360</td>\n",
       "      <td>0.002532</td>\n",
       "      <td>0.033026</td>\n",
       "      <td>0.025328</td>\n",
       "      <td>no_pain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105755</th>\n",
       "      <td>660</td>\n",
       "      <td>155</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.051738</td>\n",
       "      <td>0.906653</td>\n",
       "      <td>0.852813</td>\n",
       "      <td>0.714132</td>\n",
       "      <td>...</td>\n",
       "      <td>0.793007</td>\n",
       "      <td>0.818443</td>\n",
       "      <td>0.696164</td>\n",
       "      <td>0.676377</td>\n",
       "      <td>1.065835</td>\n",
       "      <td>0.007856</td>\n",
       "      <td>0.026876</td>\n",
       "      <td>0.173566</td>\n",
       "      <td>0.221921</td>\n",
       "      <td>no_pain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105756</th>\n",
       "      <td>660</td>\n",
       "      <td>156</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.027651</td>\n",
       "      <td>0.894214</td>\n",
       "      <td>0.834575</td>\n",
       "      <td>0.790003</td>\n",
       "      <td>...</td>\n",
       "      <td>0.742828</td>\n",
       "      <td>0.861304</td>\n",
       "      <td>0.642332</td>\n",
       "      <td>0.677491</td>\n",
       "      <td>1.021720</td>\n",
       "      <td>0.026795</td>\n",
       "      <td>0.012778</td>\n",
       "      <td>0.075945</td>\n",
       "      <td>0.116763</td>\n",
       "      <td>no_pain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105757</th>\n",
       "      <td>660</td>\n",
       "      <td>157</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.113381</td>\n",
       "      <td>0.803824</td>\n",
       "      <td>0.856149</td>\n",
       "      <td>0.659963</td>\n",
       "      <td>...</td>\n",
       "      <td>0.781024</td>\n",
       "      <td>0.872811</td>\n",
       "      <td>0.723307</td>\n",
       "      <td>0.751857</td>\n",
       "      <td>1.031213</td>\n",
       "      <td>0.036982</td>\n",
       "      <td>0.028014</td>\n",
       "      <td>0.075978</td>\n",
       "      <td>0.078339</td>\n",
       "      <td>no_pain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105758</th>\n",
       "      <td>660</td>\n",
       "      <td>158</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.058100</td>\n",
       "      <td>0.902272</td>\n",
       "      <td>0.787495</td>\n",
       "      <td>0.685756</td>\n",
       "      <td>...</td>\n",
       "      <td>0.759178</td>\n",
       "      <td>0.790487</td>\n",
       "      <td>0.702029</td>\n",
       "      <td>0.678239</td>\n",
       "      <td>1.045568</td>\n",
       "      <td>0.046405</td>\n",
       "      <td>0.017922</td>\n",
       "      <td>0.097109</td>\n",
       "      <td>0.106807</td>\n",
       "      <td>no_pain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105759</th>\n",
       "      <td>660</td>\n",
       "      <td>159</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.008509</td>\n",
       "      <td>0.951576</td>\n",
       "      <td>0.764090</td>\n",
       "      <td>0.767467</td>\n",
       "      <td>...</td>\n",
       "      <td>0.821724</td>\n",
       "      <td>0.874577</td>\n",
       "      <td>0.689898</td>\n",
       "      <td>0.701382</td>\n",
       "      <td>1.123457</td>\n",
       "      <td>0.033489</td>\n",
       "      <td>0.041909</td>\n",
       "      <td>0.084751</td>\n",
       "      <td>0.163532</td>\n",
       "      <td>no_pain</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>79680 rows √ó 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        sample_index  time  pain_survey_1  pain_survey_2  pain_survey_3  \\\n",
       "0                  0     0              2              0              2   \n",
       "1                  0     1              2              2              2   \n",
       "2                  0     2              2              0              2   \n",
       "3                  0     3              2              2              2   \n",
       "4                  0     4              2              2              2   \n",
       "...              ...   ...            ...            ...            ...   \n",
       "105755           660   155              2              2              0   \n",
       "105756           660   156              2              2              0   \n",
       "105757           660   157              0              2              2   \n",
       "105758           660   158              2              2              2   \n",
       "105759           660   159              2              2              2   \n",
       "\n",
       "        pain_survey_4  joint_00  joint_01  joint_02  joint_03  ...  joint_08  \\\n",
       "0                   1  1.094705  0.985281  1.018302  1.010385  ...  0.712989   \n",
       "1                   2  1.135183  1.021175  0.994343  1.052364  ...  0.722685   \n",
       "2                   2  1.080745  0.962842  1.009588  0.977169  ...  0.668043   \n",
       "3                   2  0.938017  1.081592  0.998021  0.987283  ...  0.702085   \n",
       "4                   2  1.090185  1.032145  1.008710  0.963658  ...  0.712197   \n",
       "...               ...       ...       ...       ...       ...  ...       ...   \n",
       "105755              0  1.051738  0.906653  0.852813  0.714132  ...  0.793007   \n",
       "105756              2  1.027651  0.894214  0.834575  0.790003  ...  0.742828   \n",
       "105757              2  1.113381  0.803824  0.856149  0.659963  ...  0.781024   \n",
       "105758              2  1.058100  0.902272  0.787495  0.685756  ...  0.759178   \n",
       "105759              0  1.008509  0.951576  0.764090  0.767467  ...  0.821724   \n",
       "\n",
       "        joint_09  joint_10  joint_11  joint_12  joint_26  joint_27  joint_28  \\\n",
       "0       1.050142  0.529555  0.447370  1.091046  0.017592  0.013508  0.026798   \n",
       "1       1.060313  0.446810  0.414432  1.045862  0.013352  0.000000  0.013377   \n",
       "2       1.011410  0.432499  0.431535  1.088221  0.016225  0.008110  0.024097   \n",
       "3       1.047223  0.478806  0.420665  1.096832  0.011832  0.007450  0.028613   \n",
       "4       1.044731  0.452906  0.476537  1.103968  0.005360  0.002532  0.033026   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "105755  0.818443  0.696164  0.676377  1.065835  0.007856  0.026876  0.173566   \n",
       "105756  0.861304  0.642332  0.677491  1.021720  0.026795  0.012778  0.075945   \n",
       "105757  0.872811  0.723307  0.751857  1.031213  0.036982  0.028014  0.075978   \n",
       "105758  0.790487  0.702029  0.678239  1.045568  0.046405  0.017922  0.097109   \n",
       "105759  0.874577  0.689898  0.701382  1.123457  0.033489  0.041909  0.084751   \n",
       "\n",
       "        joint_29    label  \n",
       "0       0.027815  no_pain  \n",
       "1       0.013716  no_pain  \n",
       "2       0.023105  no_pain  \n",
       "3       0.024648  no_pain  \n",
       "4       0.025328  no_pain  \n",
       "...          ...      ...  \n",
       "105755  0.221921  no_pain  \n",
       "105756  0.116763  no_pain  \n",
       "105757  0.078339  no_pain  \n",
       "105758  0.106807  no_pain  \n",
       "105759  0.163532  no_pain  \n",
       "\n",
       "[79680 rows x 24 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "YkF51CmhQDW5"
   },
   "outputs": [],
   "source": [
    "scale_columns = [col for col in data.columns\n",
    "                 if col != 'sample_index' and col != 'joint_30' and col != 'label']\n",
    "\n",
    "# Calculate the minimum and maximum values from the training data only\n",
    "mins_train = df_train[scale_columns].min()\n",
    "maxs_train = df_train[scale_columns].max()\n",
    "\n",
    "mins_val = df_val[scale_columns].min()\n",
    "maxs_val = df_val[scale_columns].max()\n",
    "\n",
    "mins_test = df_test[scale_columns].min()\n",
    "maxs_test = df_test[scale_columns].max()\n",
    "\n",
    "# Apply normalisation to the specified columns in all datasets\n",
    "for column in scale_columns:\n",
    "    if maxs_train[column] != mins_train[column] and mins_val[column] != maxs_val[column] and mins_test[column] != maxs_test[column]:\n",
    "      df_train[column] = (df_train[column] - mins_train[column]) / (maxs_train[column] - mins_train[column])\n",
    "\n",
    "      # Normalise the validation set\n",
    "      df_val[column] = (df_val[column] - mins_val[column]) / (maxs_val[column] - mins_val[column])\n",
    "\n",
    "      # Normalise the test set\n",
    "      df_test[column] = (df_test[column] - mins_test[column]) / (maxs_test[column] - mins_test[column])\n",
    "\n",
    "    # elif column == 'n_hands' or column == 'n_eyes' or column == 'n_legs':\n",
    "    #   df_train[column] = 0.5\n",
    "    #   df_val[column] = 0.5\n",
    "    #   df_test[column] = 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "id": "R-cyjeAuYt4D",
    "outputId": "c7c283e3-0182-4533-fc93-b2f5d01d3089"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_index</th>\n",
       "      <th>time</th>\n",
       "      <th>pain_survey_1</th>\n",
       "      <th>pain_survey_2</th>\n",
       "      <th>pain_survey_3</th>\n",
       "      <th>pain_survey_4</th>\n",
       "      <th>joint_00</th>\n",
       "      <th>joint_01</th>\n",
       "      <th>joint_02</th>\n",
       "      <th>joint_03</th>\n",
       "      <th>...</th>\n",
       "      <th>joint_08</th>\n",
       "      <th>joint_09</th>\n",
       "      <th>joint_10</th>\n",
       "      <th>joint_11</th>\n",
       "      <th>joint_12</th>\n",
       "      <th>joint_26</th>\n",
       "      <th>joint_27</th>\n",
       "      <th>joint_28</th>\n",
       "      <th>joint_29</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.777046</td>\n",
       "      <td>0.738252</td>\n",
       "      <td>0.779512</td>\n",
       "      <td>0.804419</td>\n",
       "      <td>...</td>\n",
       "      <td>0.478382</td>\n",
       "      <td>0.753815</td>\n",
       "      <td>0.272106</td>\n",
       "      <td>0.269510</td>\n",
       "      <td>0.762947</td>\n",
       "      <td>0.014214</td>\n",
       "      <td>0.011376</td>\n",
       "      <td>0.018978</td>\n",
       "      <td>0.020291</td>\n",
       "      <td>no_pain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.006289</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.805855</td>\n",
       "      <td>0.765147</td>\n",
       "      <td>0.761153</td>\n",
       "      <td>0.838021</td>\n",
       "      <td>...</td>\n",
       "      <td>0.486231</td>\n",
       "      <td>0.761224</td>\n",
       "      <td>0.217448</td>\n",
       "      <td>0.245846</td>\n",
       "      <td>0.727910</td>\n",
       "      <td>0.010748</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009473</td>\n",
       "      <td>0.010006</td>\n",
       "      <td>no_pain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.012579</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.767110</td>\n",
       "      <td>0.721439</td>\n",
       "      <td>0.772834</td>\n",
       "      <td>0.777832</td>\n",
       "      <td>...</td>\n",
       "      <td>0.441994</td>\n",
       "      <td>0.725601</td>\n",
       "      <td>0.207995</td>\n",
       "      <td>0.258133</td>\n",
       "      <td>0.760757</td>\n",
       "      <td>0.013097</td>\n",
       "      <td>0.006830</td>\n",
       "      <td>0.017065</td>\n",
       "      <td>0.016856</td>\n",
       "      <td>no_pain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.018868</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.665528</td>\n",
       "      <td>0.810416</td>\n",
       "      <td>0.763971</td>\n",
       "      <td>0.785928</td>\n",
       "      <td>...</td>\n",
       "      <td>0.469554</td>\n",
       "      <td>0.751688</td>\n",
       "      <td>0.238584</td>\n",
       "      <td>0.250324</td>\n",
       "      <td>0.767434</td>\n",
       "      <td>0.009505</td>\n",
       "      <td>0.006274</td>\n",
       "      <td>0.020264</td>\n",
       "      <td>0.017981</td>\n",
       "      <td>no_pain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.025157</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.773829</td>\n",
       "      <td>0.773366</td>\n",
       "      <td>0.772162</td>\n",
       "      <td>0.767017</td>\n",
       "      <td>...</td>\n",
       "      <td>0.477740</td>\n",
       "      <td>0.749873</td>\n",
       "      <td>0.221475</td>\n",
       "      <td>0.290464</td>\n",
       "      <td>0.772967</td>\n",
       "      <td>0.004216</td>\n",
       "      <td>0.002132</td>\n",
       "      <td>0.023389</td>\n",
       "      <td>0.018477</td>\n",
       "      <td>no_pain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105755</th>\n",
       "      <td>660</td>\n",
       "      <td>0.974843</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.746465</td>\n",
       "      <td>0.679338</td>\n",
       "      <td>0.652703</td>\n",
       "      <td>0.567290</td>\n",
       "      <td>...</td>\n",
       "      <td>0.543164</td>\n",
       "      <td>0.585036</td>\n",
       "      <td>0.382161</td>\n",
       "      <td>0.434035</td>\n",
       "      <td>0.743398</td>\n",
       "      <td>0.006255</td>\n",
       "      <td>0.022634</td>\n",
       "      <td>0.122919</td>\n",
       "      <td>0.161896</td>\n",
       "      <td>no_pain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105756</th>\n",
       "      <td>660</td>\n",
       "      <td>0.981132</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.729322</td>\n",
       "      <td>0.670018</td>\n",
       "      <td>0.638728</td>\n",
       "      <td>0.628019</td>\n",
       "      <td>...</td>\n",
       "      <td>0.502539</td>\n",
       "      <td>0.616258</td>\n",
       "      <td>0.346602</td>\n",
       "      <td>0.434835</td>\n",
       "      <td>0.709191</td>\n",
       "      <td>0.021736</td>\n",
       "      <td>0.010761</td>\n",
       "      <td>0.053784</td>\n",
       "      <td>0.085181</td>\n",
       "      <td>no_pain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105757</th>\n",
       "      <td>660</td>\n",
       "      <td>0.987421</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.790338</td>\n",
       "      <td>0.602290</td>\n",
       "      <td>0.655260</td>\n",
       "      <td>0.523931</td>\n",
       "      <td>...</td>\n",
       "      <td>0.533463</td>\n",
       "      <td>0.624639</td>\n",
       "      <td>0.400091</td>\n",
       "      <td>0.488262</td>\n",
       "      <td>0.716552</td>\n",
       "      <td>0.030063</td>\n",
       "      <td>0.023592</td>\n",
       "      <td>0.053808</td>\n",
       "      <td>0.057150</td>\n",
       "      <td>no_pain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105758</th>\n",
       "      <td>660</td>\n",
       "      <td>0.993711</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.750993</td>\n",
       "      <td>0.676055</td>\n",
       "      <td>0.602652</td>\n",
       "      <td>0.544576</td>\n",
       "      <td>...</td>\n",
       "      <td>0.515776</td>\n",
       "      <td>0.564671</td>\n",
       "      <td>0.386035</td>\n",
       "      <td>0.435373</td>\n",
       "      <td>0.727682</td>\n",
       "      <td>0.037765</td>\n",
       "      <td>0.015093</td>\n",
       "      <td>0.068772</td>\n",
       "      <td>0.077918</td>\n",
       "      <td>no_pain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105759</th>\n",
       "      <td>660</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.715698</td>\n",
       "      <td>0.712997</td>\n",
       "      <td>0.584718</td>\n",
       "      <td>0.609981</td>\n",
       "      <td>...</td>\n",
       "      <td>0.566413</td>\n",
       "      <td>0.625926</td>\n",
       "      <td>0.378022</td>\n",
       "      <td>0.451999</td>\n",
       "      <td>0.788080</td>\n",
       "      <td>0.027207</td>\n",
       "      <td>0.035294</td>\n",
       "      <td>0.060020</td>\n",
       "      <td>0.119300</td>\n",
       "      <td>no_pain</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>79680 rows √ó 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        sample_index      time  pain_survey_1  pain_survey_2  pain_survey_3  \\\n",
       "0                  0  0.000000            1.0            0.0            1.0   \n",
       "1                  0  0.006289            1.0            1.0            1.0   \n",
       "2                  0  0.012579            1.0            0.0            1.0   \n",
       "3                  0  0.018868            1.0            1.0            1.0   \n",
       "4                  0  0.025157            1.0            1.0            1.0   \n",
       "...              ...       ...            ...            ...            ...   \n",
       "105755           660  0.974843            1.0            1.0            0.0   \n",
       "105756           660  0.981132            1.0            1.0            0.0   \n",
       "105757           660  0.987421            0.0            1.0            1.0   \n",
       "105758           660  0.993711            1.0            1.0            1.0   \n",
       "105759           660  1.000000            1.0            1.0            1.0   \n",
       "\n",
       "        pain_survey_4  joint_00  joint_01  joint_02  joint_03  ...  joint_08  \\\n",
       "0                 0.5  0.777046  0.738252  0.779512  0.804419  ...  0.478382   \n",
       "1                 1.0  0.805855  0.765147  0.761153  0.838021  ...  0.486231   \n",
       "2                 1.0  0.767110  0.721439  0.772834  0.777832  ...  0.441994   \n",
       "3                 1.0  0.665528  0.810416  0.763971  0.785928  ...  0.469554   \n",
       "4                 1.0  0.773829  0.773366  0.772162  0.767017  ...  0.477740   \n",
       "...               ...       ...       ...       ...       ...  ...       ...   \n",
       "105755            0.0  0.746465  0.679338  0.652703  0.567290  ...  0.543164   \n",
       "105756            1.0  0.729322  0.670018  0.638728  0.628019  ...  0.502539   \n",
       "105757            1.0  0.790338  0.602290  0.655260  0.523931  ...  0.533463   \n",
       "105758            1.0  0.750993  0.676055  0.602652  0.544576  ...  0.515776   \n",
       "105759            0.0  0.715698  0.712997  0.584718  0.609981  ...  0.566413   \n",
       "\n",
       "        joint_09  joint_10  joint_11  joint_12  joint_26  joint_27  joint_28  \\\n",
       "0       0.753815  0.272106  0.269510  0.762947  0.014214  0.011376  0.018978   \n",
       "1       0.761224  0.217448  0.245846  0.727910  0.010748  0.000000  0.009473   \n",
       "2       0.725601  0.207995  0.258133  0.760757  0.013097  0.006830  0.017065   \n",
       "3       0.751688  0.238584  0.250324  0.767434  0.009505  0.006274  0.020264   \n",
       "4       0.749873  0.221475  0.290464  0.772967  0.004216  0.002132  0.023389   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "105755  0.585036  0.382161  0.434035  0.743398  0.006255  0.022634  0.122919   \n",
       "105756  0.616258  0.346602  0.434835  0.709191  0.021736  0.010761  0.053784   \n",
       "105757  0.624639  0.400091  0.488262  0.716552  0.030063  0.023592  0.053808   \n",
       "105758  0.564671  0.386035  0.435373  0.727682  0.037765  0.015093  0.068772   \n",
       "105759  0.625926  0.378022  0.451999  0.788080  0.027207  0.035294  0.060020   \n",
       "\n",
       "        joint_29    label  \n",
       "0       0.020291  no_pain  \n",
       "1       0.010006  no_pain  \n",
       "2       0.016856  no_pain  \n",
       "3       0.017981  no_pain  \n",
       "4       0.018477  no_pain  \n",
       "...          ...      ...  \n",
       "105755  0.161896  no_pain  \n",
       "105756  0.085181  no_pain  \n",
       "105757  0.057150  no_pain  \n",
       "105758  0.077918  no_pain  \n",
       "105759  0.119300  no_pain  \n",
       "\n",
       "[79680 rows x 24 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "NiqXdpBj0089"
   },
   "outputs": [],
   "source": [
    "# Define a function to inspect sensor data for a specific label\n",
    "def inspect_label(label, df):\n",
    "    # Filter the DataFrame for the specified label and limit to 159 rows\n",
    "    data = df[df['label'] == label][['pain_survey_1', 'pain_survey_2', 'pain_survey_3', 'pain_survey_4']][:159]\n",
    "\n",
    "    # Plot the sensor data for each axis\n",
    "    axis = data.plot(subplots=True, figsize=(17, 9), title=label)\n",
    "\n",
    "    # Adjust legend position for each subplot\n",
    "    for ax in axis:\n",
    "        ax.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "PUBCloMc3Fyt"
   },
   "outputs": [],
   "source": [
    "# Define a function to inspect sensor data for a specific label\n",
    "def inspect_label(label, df):\n",
    "    # Filter the DataFrame for the specified label and limit to 159 rows\n",
    "    data = df[df['label'] == label][['joint_00', 'joint_01', 'joint_02', 'joint_03', 'joint_04', 'joint_05' , 'joint_06', 'joint_07' , 'joint_08', 'joint_09' , 'joint_10', 'joint_11' , 'joint_12']][:159]\n",
    "\n",
    "    # Plot the sensor data for each axis\n",
    "    axis = data.plot(subplots=True, figsize=(17, 9), title=label)\n",
    "\n",
    "    # Adjust legend position for each subplot\n",
    "    for ax in axis:\n",
    "        ax.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X9Pp8l1bo8Rx"
   },
   "source": [
    "### 4.4 Feature Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gDhUJUrkQDW6",
    "outputId": "c869c3b6-e5b2-4c8e-e989-dd8515b89861"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training labels: {'no_pain': 385, 'low_pain': 71, 'high_pain': 42}\n",
      "Validation labels: {'no_pain': 124, 'low_pain': 22, 'high_pain': 14}\n",
      "Test labels: {'no_pain': 2, 'low_pain': 1, 'high_pain': 0}\n"
     ]
    }
   ],
   "source": [
    "# Initialise a dictionary to count occurrences of each activity in the training set\n",
    "training_labels = {\n",
    "    'no_pain': 0,\n",
    "    'low_pain': 0,\n",
    "    'high_pain': 0\n",
    "}\n",
    "\n",
    "# Count occurrences of each activity for unique IDs in the training set\n",
    "for id in df_train['sample_index'].unique():\n",
    "    label = df_train[df_train['sample_index'] == id]['label'].values[0]\n",
    "    training_labels[label] += 1\n",
    "\n",
    "\n",
    "#if 'joint_30' in df_train.columns:\n",
    "#    df_train = df_train.drop(columns=['joint_30']) # we deleted this joint since has no final effect during training\n",
    "\n",
    "# Print the distribution of training labels\n",
    "print('Training labels:', training_labels)\n",
    "\n",
    "# Initialise a dictionary to count occurrences of each activity in the training set\n",
    "val_labels = {\n",
    "    'no_pain': 0,\n",
    "    'low_pain': 0,\n",
    "    'high_pain': 0\n",
    "}\n",
    "\n",
    "# Count occurrences of each activity for unique IDs in the training set\n",
    "for id in df_val['sample_index'].unique():\n",
    "    label = df_val[df_val['sample_index'] == id]['label'].values[0]\n",
    "    val_labels[label] += 1\n",
    "\n",
    "\n",
    "#if 'joint_30' in df_val.columns:\n",
    "#    df_val = df_val.drop(columns=['joint_30']) # we deleted this joint since has no final effect during training\n",
    "\n",
    "# Print the distribution of validation labels\n",
    "print('Validation labels:', val_labels)\n",
    "\n",
    "# Initialise a dictionary to count occurrences of each activity in the test set\n",
    "test_labels = {\n",
    "    'no_pain': 0,\n",
    "    'low_pain': 0,\n",
    "    'high_pain': 0\n",
    "}\n",
    "\n",
    "# Count occurrences of each activity for unique IDs in the test set\n",
    "for id in df_test['sample_index'].unique():\n",
    "    label = df_test[df_test['sample_index'] == id]['label'].values[0]\n",
    "    test_labels[label] += 1\n",
    "#if 'joint_30' in df_test.columns:\n",
    "#    df_test = df_test.drop(columns=['joint_30']) # we deleted this joint since has no final effect during training\n",
    "\n",
    "# Print the distribution of test labels\n",
    "print('Test labels:', test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cDuw27RUo8Rx"
   },
   "source": [
    "### 4.5 Label Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "DT3wxqdLQDW7"
   },
   "outputs": [],
   "source": [
    "# Define a training mapping of label names to integer labels\n",
    "train_label_mapping = {\n",
    "    'no_pain': 0,\n",
    "    'low_pain': 1,\n",
    "    'high_pain': 2\n",
    "}\n",
    "\n",
    "# Map label names to integers in the training set\n",
    "df_train['label'] = df_train['label'].map(train_label_mapping)\n",
    "\n",
    "# Define a validation mapping of label names to integer labels\n",
    "val_label_mapping = {\n",
    "    'no_pain': 0,\n",
    "    'low_pain': 1,\n",
    "    'high_pain': 2\n",
    "}\n",
    "\n",
    "# Map label names to integers in the validation set\n",
    "df_val['label'] = df_val['label'].map(val_label_mapping)\n",
    "\n",
    "test_label_mapping = {\n",
    "    'no_pain': 0,\n",
    "    'low_pain': 1,\n",
    "    'high_pain': 2\n",
    "}\n",
    "\n",
    "# Map label names to integers in the test set\n",
    "df_test['label'] = df_test['label'].map(test_label_mapping)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hOi0Yx5bo8Ry"
   },
   "source": [
    "<a id=\"sequence-building\"></a>\n",
    "## 5. Sequence Building\n",
    "\n",
    "Convert variable-length time-series into fixed-size windows for RNN input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "3qv_eAbDQDW7"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define window and stride boolean variable -> if True, during training we will visit more time the same pirate with overlapping windows\n",
    "# if False, each pirate will be visited only once during training\n",
    "one_pirate_window = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "sbmKJyH-QDW7"
   },
   "outputs": [],
   "source": [
    "if one_pirate_window:\n",
    "    # Define the window size\n",
    "    WINDOW_SIZE = 100      #68 \n",
    "\n",
    "    # Stride size\n",
    "    STRIDE = 20           #17\n",
    "else:\n",
    "    # Define the window size -> select an higher window size in order to get more pirates\n",
    "    WINDOW_SIZE = 160\n",
    "\n",
    "    # Stride size\n",
    "    STRIDE = 160"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ut3JifRfo8Ry"
   },
   "source": [
    "### 5.1 Window & Stride Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "dY-ksbv0QDW8"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "  GENERAL COMMENTS:\n",
    "  in this function we are restricting for each user data the number of samples of recorded data to a constant\n",
    "  value (window size), since every user data could be composed by different numbers of timestep. Therefore we are\n",
    "  \"normalizing\" the timesteps of a constant window size. Additionally is also defined a stride variable, which if is equal to\n",
    "  the window size, then we are not taking overlapping timestamp samples, instead if stride < window, we are letting some samples\n",
    "  to overlap in such a way that the RNN or other kind of NN architecture will analyze better the context.\n",
    "\"\"\"\n",
    "\n",
    "# Define a function to build sequences from the dataset\n",
    "def build_sequences(df, window=200, stride=200):\n",
    "    # Sanity check to ensure the window is divisible by the stride\n",
    "    assert window % stride == 0 # checks if the window size is divisible by the stride\n",
    "\n",
    "    # Initialise lists to store sequences and their corresponding labels\n",
    "    dataset = []\n",
    "    labels = []\n",
    "\n",
    "    # Iterate over unique IDs in the DataFrame\n",
    "    for id in df['sample_index'].unique():\n",
    "\n",
    "        # Extract pirate sample index data for the current sample index\n",
    "        columns = [col for col in df.columns if col not in ['sample_index', 'label', 'time']]\n",
    "\n",
    "        temp = df[df['sample_index'] == id][columns].values\n",
    "\n",
    "        # Retrieve the label for the current pirate\n",
    "        label = df[df['sample_index'] == id]['label'].values[0]\n",
    "\n",
    "        # Calculate padding length to ensure full windows\n",
    "        padding_len = window - len(temp) % window\n",
    "\n",
    "        # Create zero padding and concatenate with the data\n",
    "        padding = np.zeros((padding_len, len(columns)), dtype='float32')\n",
    "        temp = np.concatenate((temp, padding))\n",
    "\n",
    "        # Build feature windows and associate them with labels\n",
    "        idx = 0\n",
    "        while idx + window <= len(temp):\n",
    "            dataset.append(temp[idx:idx + window])\n",
    "            labels.append(label)\n",
    "            idx += stride\n",
    "\n",
    "    # Convert lists to numpy arrays for further processing\n",
    "    dataset = np.array(dataset)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    return dataset, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fUdcu3oGo8Ry"
   },
   "source": [
    "### 5.2 Build Sequences Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TPVEgwEmQDW8",
    "outputId": "98737a2e-46cf-4328-b785-5cb69294d138"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2988, 100, 21), (2988,), (960, 100, 21), (960,), (18, 100, 21), (18,))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate sequences and labels for the training set\n",
    "X_train, y_train = build_sequences(df_train, WINDOW_SIZE, STRIDE)\n",
    "\n",
    "# Generate sequences and labels for the validation set\n",
    "X_val, y_val = build_sequences(df_val, WINDOW_SIZE, STRIDE)\n",
    "\n",
    "# Generate sequences and labels for the test set\n",
    "X_test, y_test = build_sequences(df_test, WINDOW_SIZE, STRIDE)\n",
    "\n",
    "# Print the shapes of the generated datasets and their labels\n",
    "X_train.shape, y_train.shape, X_val.shape, y_val.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "utsTrSH5o8Ry"
   },
   "source": [
    "### 5.3 Generate Sequences for Train/Val/Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "X1mmtqJwQDW8"
   },
   "outputs": [],
   "source": [
    "# Convert dataset into float32 for PyTorch compatibility\n",
    "X_train = X_train.astype('float32')\n",
    "X_val = X_val.astype('float32')\n",
    "X_test = X_test.astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0jLnJ0YMo8Rz"
   },
   "source": [
    "### 5.4 Data Type Conversion & Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "nxfz2MegQDW8"
   },
   "outputs": [],
   "source": [
    "# Define the input shape based on the training data\n",
    "input_shape = X_train.shape[1:]\n",
    "\n",
    "# Define the number of classes based on the categorical labels\n",
    "num_classes = len(np.unique(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "JcSpP3DBQDW8"
   },
   "outputs": [],
   "source": [
    "# Discard nan values from the dataset\n",
    "if np.isnan(X_train).any() or np.isnan(X_val).any() or np.isnan(X_test).any():\n",
    "    X_train = np.nan_to_num(X_train)\n",
    "    X_val = np.nan_to_num(X_val)\n",
    "    X_test = np.nan_to_num(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "s45svGakQDW8"
   },
   "outputs": [],
   "source": [
    "# Convert numpy arrays to PyTorch datasets (pairs features with labels)\n",
    "train_ds = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n",
    "val_ds   = TensorDataset(torch.from_numpy(X_val), torch.from_numpy(y_val))\n",
    "test_ds  = TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5b-SGD2to8Rz"
   },
   "source": [
    "<a id=\"dataloaders\"></a>\n",
    "## 6. DataLoaders\n",
    "\n",
    "Create PyTorch DataLoaders for efficient batching and parallel loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "a2dd26aqQDW8"
   },
   "outputs": [],
   "source": [
    "# Define the batch size, which is the number of samples in each batch\n",
    "BATCH_SIZE = 480   # before: 512 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "8CPWMPHOQDW9"
   },
   "outputs": [],
   "source": [
    "def make_loader(ds, batch_size, shuffle, drop_last, sampler=None):\n",
    "    # Determine optimal number of worker processes for data loading\n",
    "    cpu_cores = os.cpu_count() or 2\n",
    "    num_workers = max(2, min(4, cpu_cores))\n",
    "\n",
    "    # Create DataLoader with performance optimizations\n",
    "    return DataLoader(\n",
    "        ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle if sampler is None else False,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,  # Faster GPU transfer\n",
    "        pin_memory_device=\"cuda\" if torch.cuda.is_available() else \"\",\n",
    "        prefetch_factor=4,  # Load 4 batches ahead\n",
    "        sampler=sampler\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "cPlQI3R8QDW9"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "# Compute class counts\n",
    "class_counts = np.bincount(y_train)\n",
    "class_weights = 1. / class_counts\n",
    "sample_weights = class_weights[y_train]\n",
    "sampler = WeightedRandomSampler(weights=torch.tensor(sample_weights, dtype=torch.float32), num_samples=len(sample_weights), replacement=True)\n",
    "\n",
    "# Create data loaders with different settings for each phase\n",
    "train_loader = make_loader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=False, sampler=sampler)\n",
    "val_loader   = make_loader(val_ds, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "test_loader  = make_loader(test_ds, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "89dc9b38",
    "outputId": "546c849b-0617-4231-824e-780bfe815273"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulated batch distribution: Counter({1: 1023, 0: 986, 2: 979})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "batch_labels = []\n",
    "for _, yb in train_loader:\n",
    "    batch_labels.extend(yb.tolist())\n",
    "    if len(batch_labels) > 5000:  # just check some batches\n",
    "        break\n",
    "\n",
    "print(\"Simulated batch distribution:\", Counter(batch_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "33LHsppfQDW-",
    "outputId": "bc9c1c56-3063-431e-e00a-b4456520d2b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features batch shape: torch.Size([480, 100, 21])\n",
      "Labels batch shape: torch.Size([480])\n"
     ]
    }
   ],
   "source": [
    "# Get one batch from the training data loader\n",
    "for xb, yb in train_loader:\n",
    "    print(\"Features batch shape:\", xb.shape)\n",
    "    print(\"Labels batch shape:\", yb.shape)\n",
    "    break # Stop after getting one batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "Jl4n83GrQDW-"
   },
   "outputs": [],
   "source": [
    "def recurrent_summary(model, input_size):\n",
    "    \"\"\"\n",
    "    Custom summary function that emulates torchinfo's output while correctly\n",
    "    counting parameters for RNN/GRU/LSTM layers.\n",
    "\n",
    "    This function is designed for models whose direct children are\n",
    "    nn.Linear, nn.RNN, nn.GRU, or nn.LSTM layers.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The model to analyze.\n",
    "        input_size (tuple): Shape of the input tensor (e.g., (seq_len, features)).\n",
    "    \"\"\"\n",
    "\n",
    "    # Dictionary to store output shapes captured by forward hooks\n",
    "    output_shapes = {}\n",
    "    # List to track hook handles for later removal\n",
    "    hooks = []\n",
    "\n",
    "    def get_hook(name):\n",
    "        \"\"\"Factory function to create a forward hook for a specific module.\"\"\"\n",
    "        def hook(module, input, output):\n",
    "            # Handle RNN layer outputs (returns a tuple)\n",
    "            if isinstance(output, tuple):\n",
    "                # output[0]: all hidden states with shape (batch, seq_len, hidden*directions)\n",
    "                shape1 = list(output[0].shape)\n",
    "                shape1[0] = -1  # Replace batch dimension with -1\n",
    "\n",
    "                # output[1]: final hidden state h_n (or tuple (h_n, c_n) for LSTM)\n",
    "                if isinstance(output[1], tuple):  # LSTM case: (h_n, c_n)\n",
    "                    shape2 = list(output[1][0].shape)  # Extract h_n only\n",
    "                else:  # RNN/GRU case: h_n only\n",
    "                    shape2 = list(output[1].shape)\n",
    "\n",
    "                # Replace batch dimension (middle position) with -1\n",
    "                shape2[1] = -1\n",
    "\n",
    "                output_shapes[name] = f\"[{shape1}, {shape2}]\"\n",
    "\n",
    "            # Handle standard layer outputs (e.g., Linear)\n",
    "            else:\n",
    "                shape = list(output.shape)\n",
    "                shape[0] = -1  # Replace batch dimension with -1\n",
    "                output_shapes[name] = f\"{shape}\"\n",
    "        return hook\n",
    "\n",
    "    # 1. Determine the device where model parameters reside\n",
    "    try:\n",
    "        device = next(model.parameters()).device\n",
    "    except StopIteration:\n",
    "        device = torch.device(\"cpu\")  # Fallback for models without parameters\n",
    "\n",
    "    # 2. Create a dummy input tensor with batch_size=1\n",
    "    dummy_input = torch.randn(1, *input_size).to(device)\n",
    "\n",
    "    # 3. Register forward hooks on target layers\n",
    "    # Iterate through direct children of the model (e.g., self.rnn, self.classifier)\n",
    "    for name, module in model.named_children():\n",
    "        if isinstance(module, (nn.Linear, nn.RNN, nn.GRU, nn.LSTM)):\n",
    "            # Register the hook and store its handle for cleanup\n",
    "            hook_handle = module.register_forward_hook(get_hook(name))\n",
    "            hooks.append(hook_handle)\n",
    "\n",
    "    # 4. Execute a dummy forward pass in evaluation mode\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            model(dummy_input)\n",
    "        except Exception as e:\n",
    "            print(f\"Error during dummy forward pass: {e}\")\n",
    "            # Clean up hooks even if an error occurs\n",
    "            for h in hooks:\n",
    "                h.remove()\n",
    "            return\n",
    "\n",
    "    # 5. Remove all registered hooks\n",
    "    for h in hooks:\n",
    "        h.remove()\n",
    "\n",
    "    # --- 6. Print the summary table ---\n",
    "\n",
    "    print(\"-\" * 79)\n",
    "    # Column headers\n",
    "    print(f\"{'Layer (type)':<25} {'Output Shape':<28} {'Param #':<18}\")\n",
    "    print(\"=\" * 79)\n",
    "\n",
    "    total_params = 0\n",
    "    total_trainable_params = 0\n",
    "\n",
    "    # Iterate through modules again to collect and display parameter information\n",
    "    for name, module in model.named_children():\n",
    "        if name in output_shapes:\n",
    "            # Count total and trainable parameters for this module\n",
    "            module_params = sum(p.numel() for p in module.parameters())\n",
    "            trainable_params = sum(p.numel() for p in module.parameters() if p.requires_grad)\n",
    "\n",
    "            total_params += module_params\n",
    "            total_trainable_params += trainable_params\n",
    "\n",
    "            # Format strings for display\n",
    "            layer_name = f\"{name} ({type(module).__name__})\"\n",
    "            output_shape_str = str(output_shapes[name])\n",
    "            params_str = f\"{trainable_params:,}\"\n",
    "\n",
    "            print(f\"{layer_name:<25} {output_shape_str:<28} {params_str:<15}\")\n",
    "\n",
    "    print(\"=\" * 79)\n",
    "    print(f\"Total params: {total_params:,}\")\n",
    "    print(f\"Trainable params: {total_trainable_params:,}\")\n",
    "    print(f\"Non-trainable params: {total_params - total_trainable_params:,}\")\n",
    "    print(\"-\" * 79)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o5VULB4UQDW_"
   },
   "source": [
    "<a id=\"hyperparameters\"></a>\n",
    "## 7. Network Hyperparameters\n",
    "\n",
    "Configure training settings, architecture parameters, and regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "0a183G6zQDW_"
   },
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "LEARNING_RATE = 1e-3\n",
    "EPOCHS = 500\n",
    "PATIENCE = 50\n",
    "\n",
    "# Architecture\n",
    "HIDDEN_LAYERS = 2        # Hidden layers\n",
    "HIDDEN_SIZE = 42  # Neurons per layer -> prev hidden size = 128\n",
    "\n",
    "# Regularisation\n",
    "DROPOUT_RATE = 0.30      # Dropout probability\n",
    "\n",
    "# For now disable weight decay\n",
    "L1_LAMBDA = 1e-6        # L1 penalty\n",
    "L2_LAMBDA = 1e-3          # L2 penalty\n",
    "\n",
    "\n",
    "# Set up loss function and optimizer\n",
    "\n",
    "# TO GIVE DIFFERENT WEIGHTS TO THE LOSS DEPENDING ON THE INVERSE OF EACH LABEL TOTAL NUMBER:\n",
    "# Set up loss function and optimizer\n",
    "# from sklearn.utils.class_weight import compute_class_weight\n",
    "# classes = np.unique(y_train)\n",
    "# class_weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_train)\n",
    "# class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "# criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "# print(f\"Class weights (order = {classes}): {class_weights.cpu().numpy()}\")\n",
    "\n",
    "\n",
    "# TO GIVE FIXED WEIGHTS TO THE LOSS FUNCTION DEPENDING ON THE LABELS DISTRIBUTION:\n",
    "# weights = torch.tensor([1.0, 1.2, 1.4]).to(device)\n",
    "# criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# TO NOT CHANGE THE WEIGHTS, BUT WITH LABEL SMOOTHING\n",
    "# alpha = torch.tensor([0.5, 1.5, 2.0])\n",
    "# criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "\n",
    "\n",
    "# TO WEIGHT MORE THE \"MORE DIFFICULT\" CASES AND THE LESS FREQUENT LABELS:\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=2.0):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.ce = nn.CrossEntropyLoss(weight=alpha, reduction='none')\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = self.ce(inputs, targets)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = ((1 - pt) ** self.gamma) * ce_loss\n",
    "        return focal_loss.mean()\n",
    "alpha = None\n",
    "# alpha = torch.tensor([0.7, 1.3, 1.7], dtype=torch.float32, device=device)  # None if we don't want to alterate the weights of each label losses (FocalLoss already do it)\n",
    "criterion = FocalLoss(alpha=alpha, gamma=2.3)  # gamma = 0 it's like Crossentropy(), gamma < 1 it's like in between Crossentropy and FocalLoss,\n",
    "                                               # gamma = 1 it's a good compromise, gamma = 1.5 or gamma = 2 to weight so much the less present labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "7-RAXl_BQDXA"
   },
   "outputs": [],
   "source": [
    "# Initialize best model tracking variables\n",
    "best_model = None\n",
    "best_performance = float('-inf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ouaaN67Ho8R3"
   },
   "source": [
    "<a id=\"model-architecture\"></a>\n",
    "## 8. Model Architecture\n",
    "\n",
    "Custom RNN/LSTM/GRU classifier with configurable bidirectionality and dropout."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enhanced LSTM Model with Embedding Layers\n",
    "\n",
    "This enhanced model uses embedding layers for the categorical `pain_survey` features before feeding them to the LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedRecurrentClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Enhanced RNN classifier with embedding layers for categorical features.\n",
    "    Combines embeddings with continuous features before passing to LSTM.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            continuous_input_size,    # Number of continuous features (joints)\n",
    "            categorical_features,     # Dict: {feature_name: num_unique_values}\n",
    "            embedding_dims,          # Dict: {feature_name: embedding_dimension}\n",
    "            hidden_size,\n",
    "            num_layers,\n",
    "            num_classes,\n",
    "            rnn_type='LSTM',\n",
    "            bidirectional=True,\n",
    "            dropout_rate=0.2\n",
    "            ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.rnn_type = rnn_type\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bidirectional = bidirectional\n",
    "        self.categorical_features = list(categorical_features.keys())\n",
    "        self.continuous_input_size = continuous_input_size\n",
    "        \n",
    "        # Create embedding layers for each categorical feature\n",
    "        self.embeddings = nn.ModuleDict()\n",
    "        total_embedding_size = 0\n",
    "        \n",
    "        for feature, num_values in categorical_features.items():\n",
    "            embed_dim = embedding_dims[feature]\n",
    "            # Add 1 to num_values to handle potential out-of-range values\n",
    "            self.embeddings[feature] = nn.Embedding(num_values + 1, embed_dim)\n",
    "            total_embedding_size += embed_dim\n",
    "            print(f\"Created embedding for {feature}: {num_values} -> {embed_dim}\")\n",
    "        \n",
    "        # Total input size to RNN = continuous features + all embedding outputs\n",
    "        rnn_input_size = continuous_input_size + total_embedding_size\n",
    "        print(f\"Total RNN input size: {continuous_input_size} (continuous) + {total_embedding_size} (embeddings) = {rnn_input_size}\")\n",
    "        \n",
    "        # Map string name to PyTorch RNN class\n",
    "        rnn_map = {\n",
    "            'RNN': nn.RNN,\n",
    "            'LSTM': nn.LSTM,\n",
    "            'GRU': nn.GRU\n",
    "        }\n",
    "        \n",
    "        if rnn_type not in rnn_map:\n",
    "            raise ValueError(\"rnn_type must be 'RNN', 'LSTM', or 'GRU'\")\n",
    "        \n",
    "        rnn_module = rnn_map[rnn_type]\n",
    "        \n",
    "        # Dropout is only applied between layers (if num_layers > 1)\n",
    "        dropout_val = dropout_rate if num_layers > 1 else 0\n",
    "        \n",
    "        # Create the recurrent layer\n",
    "        self.rnn = rnn_module(\n",
    "            input_size=rnn_input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional,\n",
    "            dropout=dropout_val\n",
    "        )\n",
    "        \n",
    "        # Calculate input size for the final classifier\n",
    "        if self.bidirectional:\n",
    "            classifier_input_size = hidden_size * 2\n",
    "        else:\n",
    "            classifier_input_size = hidden_size\n",
    "        \n",
    "        # Final classification layer\n",
    "        self.classifier = nn.Linear(classifier_input_size, num_classes)\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "    \n",
    "    def forward(self, x, categorical_indices):\n",
    "        \"\"\"\n",
    "        x: (batch_size, seq_length, continuous_features)\n",
    "        categorical_indices: Dict of {feature_name: (batch_size, seq_length)} tensors\n",
    "        \"\"\"\n",
    "        batch_size, seq_length, _ = x.shape\n",
    "        \n",
    "        # Process embeddings for each categorical feature\n",
    "        embedded_features = []\n",
    "        \n",
    "        for feature_name in self.categorical_features:\n",
    "            if feature_name in categorical_indices:\n",
    "                # Get the categorical indices for this feature\n",
    "                cat_indices = categorical_indices[feature_name]  # (batch_size, seq_length)\n",
    "                \n",
    "                # Apply embedding\n",
    "                embedded = self.embeddings[feature_name](cat_indices)  # (batch_size, seq_length, embed_dim)\n",
    "                embedded_features.append(embedded)\n",
    "        \n",
    "        # Concatenate all embeddings with continuous features\n",
    "        if embedded_features:\n",
    "            all_embeddings = torch.cat(embedded_features, dim=-1)  # (batch_size, seq_length, total_embed_dim)\n",
    "            combined_input = torch.cat([x, all_embeddings], dim=-1)  # (batch_size, seq_length, total_input_size)\n",
    "        else:\n",
    "            combined_input = x\n",
    "        \n",
    "        # Pass through RNN\n",
    "        rnn_out, hidden = self.rnn(combined_input)\n",
    "        \n",
    "        # Use the last hidden state for classification\n",
    "        if self.rnn_type in ['LSTM', 'GRU']:\n",
    "            if isinstance(hidden, tuple):  # LSTM returns (h_n, c_n)\n",
    "                last_hidden = hidden[0]\n",
    "            else:  # GRU returns h_n\n",
    "                last_hidden = hidden\n",
    "        else:  # RNN\n",
    "            last_hidden = hidden\n",
    "        \n",
    "        # Get the last hidden state from the last layer\n",
    "        if self.bidirectional:\n",
    "            # Concatenate forward and backward directions\n",
    "            last_hidden = torch.cat([last_hidden[-2], last_hidden[-1]], dim=1)\n",
    "        else:\n",
    "            last_hidden = last_hidden[-1]\n",
    "        \n",
    "        # Apply dropout and classification\n",
    "        output = self.dropout(last_hidden)\n",
    "        output = self.classifier(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_sequences_enhanced(df, window=200, stride=200, categorical_features=['pain_survey_1', 'pain_survey_2', 'pain_survey_3', 'pain_survey_4']):\n",
    "    \"\"\"\n",
    "    Enhanced version of build_sequences that separates categorical and continuous features.\n",
    "    \n",
    "    Returns:\n",
    "    - continuous_data: numpy array of continuous features\n",
    "    - categorical_data: dict of categorical features\n",
    "    - labels: numpy array of labels\n",
    "    \"\"\"\n",
    "    # Sanity check to ensure the window is divisible by the stride\n",
    "    assert window % stride == 0\n",
    "    \n",
    "    # Separate categorical and continuous features\n",
    "    exclude_cols = ['sample_index', 'label', 'time'] + categorical_features\n",
    "    continuous_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "    \n",
    "    print(f\"Categorical features: {categorical_features}\")\n",
    "    print(f\"Continuous features: {continuous_cols}\")\n",
    "    print(f\"Number of continuous features: {len(continuous_cols)}\")\n",
    "    \n",
    "    # Initialize lists to store sequences and their corresponding labels\n",
    "    continuous_dataset = []\n",
    "    categorical_datasets = {feature: [] for feature in categorical_features}\n",
    "    labels = []\n",
    "    \n",
    "    # Iterate over unique IDs in the DataFrame\n",
    "    for id in df['sample_index'].unique():\n",
    "        # Extract data for the current sample index\n",
    "        pirate_data = df[df['sample_index'] == id]\n",
    "        \n",
    "        # Extract continuous features\n",
    "        continuous_temp = pirate_data[continuous_cols].values\n",
    "        \n",
    "        # Extract categorical features\n",
    "        categorical_temps = {}\n",
    "        for feature in categorical_features:\n",
    "            categorical_temps[feature] = pirate_data[feature].values\n",
    "        \n",
    "        # Retrieve the label for the current pirate\n",
    "        label = pirate_data['label'].values[0]\n",
    "        \n",
    "        # Calculate padding length to ensure full windows\n",
    "        padding_len = window - len(continuous_temp) % window\n",
    "        \n",
    "        # Create zero padding for continuous features\n",
    "        continuous_padding = np.zeros((padding_len, len(continuous_cols)), dtype='float32')\n",
    "        continuous_temp = np.concatenate((continuous_temp, continuous_padding))\n",
    "        \n",
    "        # Create padding for categorical features (use 0 as padding)\n",
    "        for feature in categorical_features:\n",
    "            categorical_padding = np.zeros(padding_len, dtype='int64')\n",
    "            categorical_temps[feature] = np.concatenate((categorical_temps[feature], categorical_padding))\n",
    "        \n",
    "        # Build feature windows and associate them with labels\n",
    "        idx = 0\n",
    "        while idx + window <= len(continuous_temp):\n",
    "            # Continuous features\n",
    "            continuous_dataset.append(continuous_temp[idx:idx + window])\n",
    "            \n",
    "            # Categorical features\n",
    "            for feature in categorical_features:\n",
    "                categorical_datasets[feature].append(categorical_temps[feature][idx:idx + window])\n",
    "            \n",
    "            labels.append(label)\n",
    "            idx += stride\n",
    "    \n",
    "    # Convert lists to numpy arrays for further processing\n",
    "    continuous_dataset = np.array(continuous_dataset, dtype='float32')\n",
    "    for feature in categorical_features:\n",
    "        categorical_datasets[feature] = np.array(categorical_datasets[feature], dtype='int64')\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    print(f\"Continuous data shape: {continuous_dataset.shape}\")\n",
    "    for feature in categorical_features:\n",
    "        print(f\"{feature} shape: {categorical_datasets[feature].shape}\")\n",
    "    print(f\"Labels shape: {labels.shape}\")\n",
    "    \n",
    "    return continuous_dataset, categorical_datasets, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Building Enhanced Sequences ===\n",
      "Categorical features: ['pain_survey_1', 'pain_survey_2', 'pain_survey_3', 'pain_survey_4']\n",
      "Continuous features: ['joint_00', 'joint_01', 'joint_02', 'joint_03', 'joint_04', 'joint_05', 'joint_06', 'joint_07', 'joint_08', 'joint_09', 'joint_10', 'joint_11', 'joint_12', 'joint_26', 'joint_27', 'joint_28', 'joint_29']\n",
      "Number of continuous features: 17\n",
      "Continuous data shape: (2988, 100, 17)\n",
      "pain_survey_1 shape: (2988, 100)\n",
      "pain_survey_2 shape: (2988, 100)\n",
      "pain_survey_3 shape: (2988, 100)\n",
      "pain_survey_4 shape: (2988, 100)\n",
      "Labels shape: (2988,)\n",
      "Categorical features: ['pain_survey_1', 'pain_survey_2', 'pain_survey_3', 'pain_survey_4']\n",
      "Continuous features: ['joint_00', 'joint_01', 'joint_02', 'joint_03', 'joint_04', 'joint_05', 'joint_06', 'joint_07', 'joint_08', 'joint_09', 'joint_10', 'joint_11', 'joint_12', 'joint_26', 'joint_27', 'joint_28', 'joint_29']\n",
      "Number of continuous features: 17\n",
      "Continuous data shape: (960, 100, 17)\n",
      "pain_survey_1 shape: (960, 100)\n",
      "pain_survey_2 shape: (960, 100)\n",
      "pain_survey_3 shape: (960, 100)\n",
      "pain_survey_4 shape: (960, 100)\n",
      "Labels shape: (960,)\n",
      "Categorical features: ['pain_survey_1', 'pain_survey_2', 'pain_survey_3', 'pain_survey_4']\n",
      "Continuous features: ['joint_00', 'joint_01', 'joint_02', 'joint_03', 'joint_04', 'joint_05', 'joint_06', 'joint_07', 'joint_08', 'joint_09', 'joint_10', 'joint_11', 'joint_12', 'joint_26', 'joint_27', 'joint_28', 'joint_29']\n",
      "Number of continuous features: 17\n",
      "Continuous data shape: (18, 100, 17)\n",
      "pain_survey_1 shape: (18, 100)\n",
      "pain_survey_2 shape: (18, 100)\n",
      "pain_survey_3 shape: (18, 100)\n",
      "pain_survey_4 shape: (18, 100)\n",
      "Labels shape: (18,)\n",
      "\n",
      "=== Enhanced Dataset Shapes ===\n",
      "Training: continuous (2988, 100, 17), labels (2988,)\n",
      "Validation: continuous (960, 100, 17), labels (960,)\n",
      "Test: continuous (18, 100, 17), labels (18,)\n",
      "\n",
      "Categorical feature configuration: {'pain_survey_1': 3, 'pain_survey_2': 3, 'pain_survey_3': 3, 'pain_survey_4': 3}\n",
      "Continuous feature size: 17\n"
     ]
    }
   ],
   "source": [
    "# Generate enhanced sequences for training, validation, and test sets\n",
    "print(\"=== Building Enhanced Sequences ===\")\n",
    "\n",
    "# Build sequences with separated categorical and continuous features\n",
    "X_train_cont, X_train_cat, y_train_enh = build_sequences_enhanced(df_train, WINDOW_SIZE, STRIDE)\n",
    "X_val_cont, X_val_cat, y_val_enh = build_sequences_enhanced(df_val, WINDOW_SIZE, STRIDE)\n",
    "X_test_cont, X_test_cat, y_test_enh = build_sequences_enhanced(df_test, WINDOW_SIZE, STRIDE)\n",
    "\n",
    "print(\"\\n=== Enhanced Dataset Shapes ===\")\n",
    "print(f\"Training: continuous {X_train_cont.shape}, labels {y_train_enh.shape}\")\n",
    "print(f\"Validation: continuous {X_val_cont.shape}, labels {y_val_enh.shape}\")  \n",
    "print(f\"Test: continuous {X_test_cont.shape}, labels {y_test_enh.shape}\")\n",
    "\n",
    "# Define categorical feature parameters for the model\n",
    "categorical_feature_config = {\n",
    "    'pain_survey_1': 3,  # values: 0, 1, 2\n",
    "    'pain_survey_2': 3,  # values: 0, 1, 2\n",
    "    'pain_survey_3': 3,  # values: 0, 1, 2\n",
    "    'pain_survey_4': 3   # values: 0, 1, 2\n",
    "}\n",
    "\n",
    "print(f\"\\nCategorical feature configuration: {categorical_feature_config}\")\n",
    "print(f\"Continuous feature size: {X_train_cont.shape[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Creating Enhanced Datasets ===\n",
      "Enhanced dataset sizes:\n",
      "Train: 2988 samples\n",
      "Val: 960 samples\n",
      "Test: 18 samples\n"
     ]
    }
   ],
   "source": [
    "# # Create custom dataset class for enhanced model\n",
    "class EnhancedDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, continuous_data, categorical_data, labels):\n",
    "        \"\"\"\n",
    "        Dataset for enhanced model with embeddings\n",
    "        \n",
    "        Args:\n",
    "            continuous_data: numpy array of shape (N, seq_len, continuous_features)\n",
    "            categorical_data: dict of {feature_name: numpy array of shape (N, seq_len)}\n",
    "            labels: numpy array of shape (N,)\n",
    "        \"\"\"\n",
    "        self.continuous_data = torch.FloatTensor(continuous_data)\n",
    "        self.categorical_data = {}\n",
    "        for feature, data in categorical_data.items():\n",
    "            self.categorical_data[feature] = torch.LongTensor(data)\n",
    "        self.labels = torch.LongTensor(labels)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        continuous = self.continuous_data[idx]\n",
    "        categorical = {feature: data[idx] for feature, data in self.categorical_data.items()}\n",
    "        label = self.labels[idx]\n",
    "        return continuous, categorical, label\n",
    "\n",
    "# Create enhanced datasets\n",
    "print(\"=== Creating Enhanced Datasets ===\")\n",
    "\n",
    "train_enhanced_ds = EnhancedDataset(X_train_cont, X_train_cat, y_train_enh)\n",
    "val_enhanced_ds = EnhancedDataset(X_val_cont, X_val_cat, y_val_enh)\n",
    "test_enhanced_ds = EnhancedDataset(X_test_cont, X_test_cat, y_test_enh)\n",
    "\n",
    "print(f\"Enhanced dataset sizes:\")\n",
    "print(f\"Train: {len(train_enhanced_ds)} samples\")\n",
    "print(f\"Val: {len(val_enhanced_ds)} samples\") \n",
    "print(f\"Test: {len(test_enhanced_ds)} samples\")\n",
    "\n",
    "# Create enhanced data loaders\n",
    "def collate_enhanced(batch):\n",
    "    \"\"\"Custom collate function for enhanced dataset\"\"\"\n",
    "    continuous_batch = torch.stack([item[0] for item in batch])\n",
    "    \n",
    "    categorical_batch = {}\n",
    "    if batch:  # Check if batch is not empty\n",
    "        # Get feature names from first item\n",
    "        feature_names = batch[0][1].keys()\n",
    "        for feature in feature_names:\n",
    "            categorical_batch[feature] = torch.stack([item[1][feature] for item in batch])\n",
    "    \n",
    "    labels_batch = torch.stack([item[2] for item in batch])\n",
    "    \n",
    "    return continuous_batch, categorical_batch, labels_batch\n",
    "\n",
    "# # Create data loaders with custom collate function\n",
    "# train_enhanced_loader = DataLoader(\n",
    "#     train_enhanced_ds, \n",
    "#     batch_size=BATCH_SIZE, \n",
    "#     shuffle=True, \n",
    "#     collate_fn=collate_enhanced,\n",
    "#     num_workers=2,\n",
    "#     pin_memory=True\n",
    "# )\n",
    "\n",
    "# val_enhanced_loader = DataLoader(\n",
    "#     val_enhanced_ds, \n",
    "#     batch_size=BATCH_SIZE, \n",
    "#     shuffle=False, \n",
    "#     collate_fn=collate_enhanced,\n",
    "#     num_workers=2,\n",
    "#     pin_memory=True\n",
    "# )\n",
    "\n",
    "# test_enhanced_loader = DataLoader(\n",
    "#     test_enhanced_ds, \n",
    "#     batch_size=BATCH_SIZE, \n",
    "#     shuffle=False, \n",
    "#     collate_fn=collate_enhanced,\n",
    "#     num_workers=2,\n",
    "#     pin_memory=True\n",
    "# )\n",
    "\n",
    "# print(f\"Enhanced data loaders created successfully!\")\n",
    "\n",
    "# # Test the enhanced data loader\n",
    "# print(f\"\\\\n=== Testing Enhanced Data Loader ===\")\n",
    "# for continuous_batch, categorical_batch, labels_batch in train_enhanced_loader:\n",
    "#     print(f\"Batch continuous shape: {continuous_batch.shape}\")\n",
    "#     print(f\"Batch labels shape: {labels_batch.shape}\")\n",
    "#     for feature, data in categorical_batch.items():\n",
    "#         print(f\"Batch {feature} shape: {data.shape}\")\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Creating Enhanced Datasets (CORRECTED) ===\n",
      "‚úÖ Enhanced data loaders created successfully with custom collate function!\n",
      "\\n=== Testing Corrected Enhanced Data Loader ===\n",
      "‚úÖ Batch continuous shape: torch.Size([480, 100, 17])\n",
      "‚úÖ Batch labels shape: torch.Size([480])\n",
      "‚úÖ Batch pain_survey_1 shape: torch.Size([480, 100])\n",
      "‚úÖ Batch pain_survey_2 shape: torch.Size([480, 100])\n",
      "‚úÖ Batch pain_survey_3 shape: torch.Size([480, 100])\n",
      "‚úÖ Batch pain_survey_4 shape: torch.Size([480, 100])\n",
      "\\n‚úÖ Enhanced data loaders are now CORRECTLY configured!\n"
     ]
    }
   ],
   "source": [
    "# Create enhanced datasets (CORRECTED VERSION)\n",
    "print(\"=== Creating Enhanced Datasets (CORRECTED) ===\")\n",
    "# ‚úÖ SOLUTION: Create enhanced data loaders with custom collate function\n",
    "\n",
    "def make_enhanced_loader(ds, batch_size, shuffle, drop_last, sampler=None):\n",
    "    \"\"\"\n",
    "    Enhanced version of make_loader with custom collate function for enhanced datasets\n",
    "    \"\"\"\n",
    "    def collate_enhanced(batch):\n",
    "        \"\"\"Custom collate function for enhanced dataset\"\"\"\n",
    "        continuous_batch = torch.stack([item[0] for item in batch])\n",
    "        \n",
    "        categorical_batch = {}\n",
    "        if batch:  # Check if batch is not empty\n",
    "            # Get feature names from first item\n",
    "            feature_names = batch[0][1].keys()\n",
    "            for feature in feature_names:\n",
    "                categorical_batch[feature] = torch.stack([item[1][feature] for item in batch])\n",
    "        \n",
    "        labels_batch = torch.stack([item[2] for item in batch])\n",
    "        \n",
    "        return continuous_batch, categorical_batch, labels_batch\n",
    "    \n",
    "    # Use single thread to avoid pickle issues with local functions\n",
    "    num_workers = 0  # Avoid multiprocessing issues with local collate function\n",
    "    \n",
    "    return DataLoader(\n",
    "        ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle if sampler is None else False,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True if torch.cuda.is_available() else False,\n",
    "        collate_fn=collate_enhanced, \n",
    "        sampler=sampler\n",
    "    )\n",
    "\n",
    "# Create enhanced data loaders with CORRECT configuration\n",
    "train_enhanced_loader = make_enhanced_loader(\n",
    "                        train_enhanced_ds,\n",
    "                        batch_size=BATCH_SIZE,\n",
    "                        shuffle=True,\n",
    "                        drop_last=False,\n",
    "                        sampler=None\n",
    "                    )\n",
    "\n",
    "val_enhanced_loader = make_enhanced_loader(\n",
    "                        val_enhanced_ds,\n",
    "                        batch_size=BATCH_SIZE,\n",
    "                        shuffle=False, \n",
    "                        drop_last=False,\n",
    "                        sampler=None\n",
    "                    )\n",
    "\n",
    "test_enhanced_loader = make_enhanced_loader(\n",
    "                        test_enhanced_ds,\n",
    "                        batch_size=BATCH_SIZE,\n",
    "                        shuffle=False, \n",
    "                        drop_last=False,\n",
    "                        sampler=None\n",
    "                    )\n",
    "\n",
    "print(f\"‚úÖ Enhanced data loaders created successfully with custom collate function!\")\n",
    "\n",
    "# Test the corrected enhanced data loader\n",
    "print(f\"\\\\n=== Testing Corrected Enhanced Data Loader ===\")\n",
    "for continuous_batch, categorical_batch, labels_batch in train_enhanced_loader:\n",
    "    print(f\"‚úÖ Batch continuous shape: {continuous_batch.shape}\")\n",
    "    print(f\"‚úÖ Batch labels shape: {labels_batch.shape}\")\n",
    "    for feature, data in categorical_batch.items():\n",
    "        print(f\"‚úÖ Batch {feature} shape: {data.shape}\")\n",
    "    break\n",
    "\n",
    "print(f\"\\\\n‚úÖ Enhanced data loaders are now CORRECTLY configured!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Enhanced Model\n",
    "\n",
    "Now you can train the enhanced model using the same training loop structure as your original model, but with the new data loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Creating Enhanced Model for Training Demo ===\n",
      "Created embedding for pain_survey_1: 3 -> 2\n",
      "Created embedding for pain_survey_2: 3 -> 2\n",
      "Created embedding for pain_survey_3: 3 -> 2\n",
      "Created embedding for pain_survey_4: 3 -> 2\n",
      "Total RNN input size: 17 (continuous) + 8 (embeddings) = 25\n",
      "Demo model parameters: 66,479\n",
      "üìä Available scheduler options:\n",
      "  ‚Ä¢ scheduler_f1: Focuses on F1 score improvement\n",
      "  ‚Ä¢ scheduler_loss: Focuses on validation loss reduction\n",
      "  ‚Ä¢ scheduler_combined: Uses combined F1 and loss metric\n",
      "üéØ Currently using: ReduceLROnPlateau with mode='max'\n",
      "Ready for training!\n",
      "Starting enhanced model training with Early Stopping...\n",
      "üîß Regularization: L1=1.00e-06, L2=1.00e-03\n",
      "üõë Early Stopping: Patience=50 epochs, Min improvement=0.0010\n",
      "üìä Scheduler Metric: combined\n",
      "üìà Using ReduceLROnPlateau scheduler - Mode: max, Factor: 0.8, Patience: 25\n",
      "‚úÖ NEW BEST F1: 0.6768 at epoch 1 (improvement: +0.0010)\n",
      "üîΩ NEW BEST VAL LOSS: 0.3102 at epoch 1 (improvement: -0.0010)\n",
      "Epoch   1/500 | Train Loss: 0.9201 | Val Loss: 0.3102 | Train F1: 0.5961 | Val F1: 0.6768 | LR: 1.00e-03\n",
      "         Best F1: 0.6768 (Ep 1) | Best Loss: 0.3102 (Ep 1) | No F1 Improve: 0/50 | No Loss Improve: 0/50\n",
      "üîΩ NEW BEST VAL LOSS: 0.2557 at epoch 2 (improvement: -0.0010)\n",
      "Epoch   2/500 | Train Loss: 0.7666 | Val Loss: 0.2557 | Train F1: 0.6742 | Val F1: 0.6768 | LR: 1.00e-03\n",
      "         Best F1: 0.6768 (Ep 1) | Best Loss: 0.2557 (Ep 2) | No F1 Improve: 1/50 | No Loss Improve: 0/50\n",
      "Epoch   3/500 | Train Loss: 0.7053 | Val Loss: 0.2566 | Train F1: 0.6742 | Val F1: 0.6768 | LR: 1.00e-03\n",
      "         Best F1: 0.6768 (Ep 1) | Best Loss: 0.2557 (Ep 2) | No F1 Improve: 2/50 | No Loss Improve: 1/50\n",
      "üîΩ NEW BEST VAL LOSS: 0.2496 at epoch 4 (improvement: -0.0010)\n",
      "Epoch   4/500 | Train Loss: 0.6565 | Val Loss: 0.2496 | Train F1: 0.6742 | Val F1: 0.6768 | LR: 1.00e-03\n",
      "         Best F1: 0.6768 (Ep 1) | Best Loss: 0.2496 (Ep 4) | No F1 Improve: 3/50 | No Loss Improve: 0/50\n",
      "Epoch   5/500 | Train Loss: 0.6072 | Val Loss: 0.2498 | Train F1: 0.6742 | Val F1: 0.6768 | LR: 1.00e-03\n",
      "         Best F1: 0.6768 (Ep 1) | Best Loss: 0.2496 (Ep 4) | No F1 Improve: 4/50 | No Loss Improve: 1/50\n",
      "üîΩ NEW BEST VAL LOSS: 0.2446 at epoch 6 (improvement: -0.0010)\n",
      "Epoch   6/500 | Train Loss: 0.5681 | Val Loss: 0.2446 | Train F1: 0.6742 | Val F1: 0.6768 | LR: 1.00e-03\n",
      "         Best F1: 0.6768 (Ep 1) | Best Loss: 0.2446 (Ep 6) | No F1 Improve: 5/50 | No Loss Improve: 0/50\n",
      "üîΩ NEW BEST VAL LOSS: 0.2420 at epoch 7 (improvement: -0.0010)\n",
      "Epoch   7/500 | Train Loss: 0.5341 | Val Loss: 0.2420 | Train F1: 0.6742 | Val F1: 0.6768 | LR: 1.00e-03\n",
      "         Best F1: 0.6768 (Ep 1) | Best Loss: 0.2420 (Ep 7) | No F1 Improve: 6/50 | No Loss Improve: 0/50\n",
      "üîΩ NEW BEST VAL LOSS: 0.2378 at epoch 8 (improvement: -0.0010)\n",
      "Epoch   8/500 | Train Loss: 0.4986 | Val Loss: 0.2378 | Train F1: 0.6743 | Val F1: 0.6768 | LR: 1.00e-03\n",
      "         Best F1: 0.6768 (Ep 1) | Best Loss: 0.2378 (Ep 8) | No F1 Improve: 7/50 | No Loss Improve: 0/50\n",
      "‚úÖ NEW BEST F1: 0.6956 at epoch 9 (improvement: +0.0010)\n",
      "üîΩ NEW BEST VAL LOSS: 0.2328 at epoch 9 (improvement: -0.0010)\n",
      "Epoch   9/500 | Train Loss: 0.4768 | Val Loss: 0.2328 | Train F1: 0.6830 | Val F1: 0.6956 | LR: 1.00e-03\n",
      "         Best F1: 0.6956 (Ep 9) | Best Loss: 0.2328 (Ep 9) | No F1 Improve: 0/50 | No Loss Improve: 0/50\n",
      "‚úÖ NEW BEST F1: 0.7144 at epoch 10 (improvement: +0.0010)\n",
      "üîΩ NEW BEST VAL LOSS: 0.2263 at epoch 10 (improvement: -0.0010)\n",
      "Epoch  10/500 | Train Loss: 0.4427 | Val Loss: 0.2263 | Train F1: 0.7122 | Val F1: 0.7144 | LR: 1.00e-03\n",
      "         Best F1: 0.7144 (Ep 10) | Best Loss: 0.2263 (Ep 10) | No F1 Improve: 0/50 | No Loss Improve: 0/50\n",
      "----------------------------------------------------------------------------------------------------\n",
      "‚úÖ NEW BEST F1: 0.7408 at epoch 11 (improvement: +0.0010)\n",
      "üîΩ NEW BEST VAL LOSS: 0.2201 at epoch 11 (improvement: -0.0010)\n",
      "Epoch  11/500 | Train Loss: 0.4217 | Val Loss: 0.2201 | Train F1: 0.7314 | Val F1: 0.7408 | LR: 1.00e-03\n",
      "         Best F1: 0.7408 (Ep 11) | Best Loss: 0.2201 (Ep 11) | No F1 Improve: 0/50 | No Loss Improve: 0/50\n",
      "‚úÖ NEW BEST F1: 0.7470 at epoch 12 (improvement: +0.0010)\n",
      "üîΩ NEW BEST VAL LOSS: 0.2114 at epoch 12 (improvement: -0.0010)\n",
      "Epoch  12/500 | Train Loss: 0.3949 | Val Loss: 0.2114 | Train F1: 0.7400 | Val F1: 0.7470 | LR: 1.00e-03\n",
      "         Best F1: 0.7470 (Ep 12) | Best Loss: 0.2114 (Ep 12) | No F1 Improve: 0/50 | No Loss Improve: 0/50\n",
      "‚úÖ NEW BEST F1: 0.7560 at epoch 13 (improvement: +0.0010)\n",
      "Epoch  13/500 | Train Loss: 0.3689 | Val Loss: 0.2115 | Train F1: 0.7526 | Val F1: 0.7560 | LR: 1.00e-03\n",
      "         Best F1: 0.7560 (Ep 13) | Best Loss: 0.2114 (Ep 12) | No F1 Improve: 0/50 | No Loss Improve: 1/50\n",
      "Epoch  14/500 | Train Loss: 0.3645 | Val Loss: 0.2115 | Train F1: 0.7498 | Val F1: 0.7491 | LR: 1.00e-03\n",
      "         Best F1: 0.7560 (Ep 13) | Best Loss: 0.2114 (Ep 12) | No F1 Improve: 1/50 | No Loss Improve: 2/50\n",
      "Epoch  15/500 | Train Loss: 0.3413 | Val Loss: 0.2110 | Train F1: 0.7553 | Val F1: 0.7497 | LR: 1.00e-03\n",
      "         Best F1: 0.7560 (Ep 13) | Best Loss: 0.2114 (Ep 12) | No F1 Improve: 2/50 | No Loss Improve: 3/50\n",
      "‚úÖ NEW BEST F1: 0.7571 at epoch 16 (improvement: +0.0010)\n",
      "üîΩ NEW BEST VAL LOSS: 0.2086 at epoch 16 (improvement: -0.0010)\n",
      "Epoch  16/500 | Train Loss: 0.3326 | Val Loss: 0.2086 | Train F1: 0.7525 | Val F1: 0.7571 | LR: 1.00e-03\n",
      "         Best F1: 0.7571 (Ep 16) | Best Loss: 0.2086 (Ep 16) | No F1 Improve: 0/50 | No Loss Improve: 0/50\n",
      "Epoch  17/500 | Train Loss: 0.3194 | Val Loss: 0.2111 | Train F1: 0.7544 | Val F1: 0.7532 | LR: 1.00e-03\n",
      "         Best F1: 0.7571 (Ep 16) | Best Loss: 0.2086 (Ep 16) | No F1 Improve: 1/50 | No Loss Improve: 1/50\n",
      "Epoch  18/500 | Train Loss: 0.3094 | Val Loss: 0.2102 | Train F1: 0.7494 | Val F1: 0.7579 | LR: 1.00e-03\n",
      "         Best F1: 0.7571 (Ep 16) | Best Loss: 0.2086 (Ep 16) | No F1 Improve: 2/50 | No Loss Improve: 2/50\n",
      "Epoch  19/500 | Train Loss: 0.3074 | Val Loss: 0.2130 | Train F1: 0.7520 | Val F1: 0.7492 | LR: 1.00e-03\n",
      "         Best F1: 0.7571 (Ep 16) | Best Loss: 0.2086 (Ep 16) | No F1 Improve: 3/50 | No Loss Improve: 3/50\n",
      "Epoch  20/500 | Train Loss: 0.2929 | Val Loss: 0.2134 | Train F1: 0.7491 | Val F1: 0.7388 | LR: 1.00e-03\n",
      "         Best F1: 0.7571 (Ep 16) | Best Loss: 0.2086 (Ep 16) | No F1 Improve: 4/50 | No Loss Improve: 4/50\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch  21/500 | Train Loss: 0.2897 | Val Loss: 0.2157 | Train F1: 0.7554 | Val F1: 0.7478 | LR: 1.00e-03\n",
      "         Best F1: 0.7571 (Ep 16) | Best Loss: 0.2086 (Ep 16) | No F1 Improve: 5/50 | No Loss Improve: 5/50\n",
      "Epoch  22/500 | Train Loss: 0.2868 | Val Loss: 0.2124 | Train F1: 0.7569 | Val F1: 0.7373 | LR: 1.00e-03\n",
      "         Best F1: 0.7571 (Ep 16) | Best Loss: 0.2086 (Ep 16) | No F1 Improve: 6/50 | No Loss Improve: 6/50\n",
      "Epoch  23/500 | Train Loss: 0.2733 | Val Loss: 0.2175 | Train F1: 0.7559 | Val F1: 0.7288 | LR: 1.00e-03\n",
      "         Best F1: 0.7571 (Ep 16) | Best Loss: 0.2086 (Ep 16) | No F1 Improve: 7/50 | No Loss Improve: 7/50\n",
      "Epoch  24/500 | Train Loss: 0.2783 | Val Loss: 0.2165 | Train F1: 0.7616 | Val F1: 0.7310 | LR: 1.00e-03\n",
      "         Best F1: 0.7571 (Ep 16) | Best Loss: 0.2086 (Ep 16) | No F1 Improve: 8/50 | No Loss Improve: 8/50\n",
      "Epoch  25/500 | Train Loss: 0.2833 | Val Loss: 0.2161 | Train F1: 0.7365 | Val F1: 0.7314 | LR: 1.00e-03\n",
      "         Best F1: 0.7571 (Ep 16) | Best Loss: 0.2086 (Ep 16) | No F1 Improve: 9/50 | No Loss Improve: 9/50\n",
      "Epoch  26/500 | Train Loss: 0.2764 | Val Loss: 0.2152 | Train F1: 0.7529 | Val F1: 0.7333 | LR: 1.00e-03\n",
      "         Best F1: 0.7571 (Ep 16) | Best Loss: 0.2086 (Ep 16) | No F1 Improve: 10/50 | No Loss Improve: 10/50\n",
      "  üîΩ Learning rate reduced from 1.00e-03 to 8.00e-04 (based on combined)\n",
      "Epoch  27/500 | Train Loss: 0.2655 | Val Loss: 0.2118 | Train F1: 0.7641 | Val F1: 0.7430 | LR: 8.00e-04\n",
      "         Best F1: 0.7571 (Ep 16) | Best Loss: 0.2086 (Ep 16) | No F1 Improve: 11/50 | No Loss Improve: 11/50\n",
      "Epoch  28/500 | Train Loss: 0.2677 | Val Loss: 0.2113 | Train F1: 0.7549 | Val F1: 0.7395 | LR: 8.00e-04\n",
      "         Best F1: 0.7571 (Ep 16) | Best Loss: 0.2086 (Ep 16) | No F1 Improve: 12/50 | No Loss Improve: 12/50\n",
      "Epoch  29/500 | Train Loss: 0.2631 | Val Loss: 0.2132 | Train F1: 0.7580 | Val F1: 0.7430 | LR: 8.00e-04\n",
      "         Best F1: 0.7571 (Ep 16) | Best Loss: 0.2086 (Ep 16) | No F1 Improve: 13/50 | No Loss Improve: 13/50\n",
      "Epoch  30/500 | Train Loss: 0.2515 | Val Loss: 0.2122 | Train F1: 0.7614 | Val F1: 0.7475 | LR: 8.00e-04\n",
      "         Best F1: 0.7571 (Ep 16) | Best Loss: 0.2086 (Ep 16) | No F1 Improve: 14/50 | No Loss Improve: 14/50\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch  31/500 | Train Loss: 0.2573 | Val Loss: 0.2114 | Train F1: 0.7674 | Val F1: 0.7393 | LR: 8.00e-04\n",
      "         Best F1: 0.7571 (Ep 16) | Best Loss: 0.2086 (Ep 16) | No F1 Improve: 15/50 | No Loss Improve: 15/50\n",
      "Epoch  32/500 | Train Loss: 0.2551 | Val Loss: 0.2120 | Train F1: 0.7673 | Val F1: 0.7449 | LR: 8.00e-04\n",
      "         Best F1: 0.7571 (Ep 16) | Best Loss: 0.2086 (Ep 16) | No F1 Improve: 16/50 | No Loss Improve: 16/50\n",
      "Epoch  33/500 | Train Loss: 0.2523 | Val Loss: 0.2127 | Train F1: 0.7694 | Val F1: 0.7404 | LR: 8.00e-04\n",
      "         Best F1: 0.7571 (Ep 16) | Best Loss: 0.2086 (Ep 16) | No F1 Improve: 17/50 | No Loss Improve: 17/50\n",
      "Epoch  34/500 | Train Loss: 0.2493 | Val Loss: 0.2128 | Train F1: 0.7700 | Val F1: 0.7394 | LR: 8.00e-04\n",
      "         Best F1: 0.7571 (Ep 16) | Best Loss: 0.2086 (Ep 16) | No F1 Improve: 18/50 | No Loss Improve: 18/50\n",
      "Epoch  35/500 | Train Loss: 0.2428 | Val Loss: 0.2126 | Train F1: 0.7697 | Val F1: 0.7503 | LR: 8.00e-04\n",
      "         Best F1: 0.7571 (Ep 16) | Best Loss: 0.2086 (Ep 16) | No F1 Improve: 19/50 | No Loss Improve: 19/50\n",
      "Epoch  36/500 | Train Loss: 0.2453 | Val Loss: 0.2119 | Train F1: 0.7692 | Val F1: 0.7524 | LR: 8.00e-04\n",
      "         Best F1: 0.7571 (Ep 16) | Best Loss: 0.2086 (Ep 16) | No F1 Improve: 20/50 | No Loss Improve: 20/50\n",
      "Epoch  37/500 | Train Loss: 0.2397 | Val Loss: 0.2168 | Train F1: 0.7732 | Val F1: 0.7361 | LR: 8.00e-04\n",
      "         Best F1: 0.7571 (Ep 16) | Best Loss: 0.2086 (Ep 16) | No F1 Improve: 21/50 | No Loss Improve: 21/50\n",
      "Epoch  38/500 | Train Loss: 0.2426 | Val Loss: 0.2107 | Train F1: 0.7631 | Val F1: 0.7534 | LR: 8.00e-04\n",
      "         Best F1: 0.7571 (Ep 16) | Best Loss: 0.2086 (Ep 16) | No F1 Improve: 22/50 | No Loss Improve: 22/50\n",
      "Epoch  39/500 | Train Loss: 0.2392 | Val Loss: 0.2127 | Train F1: 0.7774 | Val F1: 0.7527 | LR: 8.00e-04\n",
      "         Best F1: 0.7571 (Ep 16) | Best Loss: 0.2086 (Ep 16) | No F1 Improve: 23/50 | No Loss Improve: 23/50\n",
      "Epoch  40/500 | Train Loss: 0.2311 | Val Loss: 0.2121 | Train F1: 0.7712 | Val F1: 0.7475 | LR: 8.00e-04\n",
      "         Best F1: 0.7571 (Ep 16) | Best Loss: 0.2086 (Ep 16) | No F1 Improve: 24/50 | No Loss Improve: 24/50\n",
      "----------------------------------------------------------------------------------------------------\n",
      "üîΩ NEW BEST VAL LOSS: 0.2056 at epoch 41 (improvement: -0.0010)\n",
      "Epoch  41/500 | Train Loss: 0.2406 | Val Loss: 0.2056 | Train F1: 0.7700 | Val F1: 0.7555 | LR: 8.00e-04\n",
      "         Best F1: 0.7571 (Ep 16) | Best Loss: 0.2056 (Ep 41) | No F1 Improve: 25/50 | No Loss Improve: 0/50\n",
      "üîΩ NEW BEST VAL LOSS: 0.2018 at epoch 42 (improvement: -0.0010)\n",
      "Epoch  42/500 | Train Loss: 0.2383 | Val Loss: 0.2018 | Train F1: 0.7820 | Val F1: 0.7531 | LR: 8.00e-04\n",
      "         Best F1: 0.7571 (Ep 16) | Best Loss: 0.2018 (Ep 42) | No F1 Improve: 26/50 | No Loss Improve: 0/50\n",
      "Epoch  43/500 | Train Loss: 0.2306 | Val Loss: 0.2025 | Train F1: 0.7797 | Val F1: 0.7549 | LR: 8.00e-04\n",
      "         Best F1: 0.7571 (Ep 16) | Best Loss: 0.2018 (Ep 42) | No F1 Improve: 27/50 | No Loss Improve: 1/50\n",
      "‚úÖ NEW BEST F1: 0.7600 at epoch 44 (improvement: +0.0010)\n",
      "Epoch  44/500 | Train Loss: 0.2334 | Val Loss: 0.2082 | Train F1: 0.7811 | Val F1: 0.7600 | LR: 8.00e-04\n",
      "         Best F1: 0.7600 (Ep 44) | Best Loss: 0.2018 (Ep 42) | No F1 Improve: 0/50 | No Loss Improve: 2/50\n",
      "‚úÖ NEW BEST F1: 0.7643 at epoch 45 (improvement: +0.0010)\n",
      "Epoch  45/500 | Train Loss: 0.2390 | Val Loss: 0.2049 | Train F1: 0.7841 | Val F1: 0.7643 | LR: 8.00e-04\n",
      "         Best F1: 0.7643 (Ep 45) | Best Loss: 0.2018 (Ep 42) | No F1 Improve: 0/50 | No Loss Improve: 3/50\n",
      "üîΩ NEW BEST VAL LOSS: 0.1959 at epoch 46 (improvement: -0.0010)\n",
      "Epoch  46/500 | Train Loss: 0.2283 | Val Loss: 0.1959 | Train F1: 0.7780 | Val F1: 0.7583 | LR: 8.00e-04\n",
      "         Best F1: 0.7643 (Ep 45) | Best Loss: 0.1959 (Ep 46) | No F1 Improve: 1/50 | No Loss Improve: 0/50\n",
      "Epoch  47/500 | Train Loss: 0.2237 | Val Loss: 0.2045 | Train F1: 0.7824 | Val F1: 0.7545 | LR: 8.00e-04\n",
      "         Best F1: 0.7643 (Ep 45) | Best Loss: 0.1959 (Ep 46) | No F1 Improve: 2/50 | No Loss Improve: 1/50\n",
      "Epoch  48/500 | Train Loss: 0.2321 | Val Loss: 0.2112 | Train F1: 0.7872 | Val F1: 0.7575 | LR: 8.00e-04\n",
      "         Best F1: 0.7643 (Ep 45) | Best Loss: 0.1959 (Ep 46) | No F1 Improve: 3/50 | No Loss Improve: 2/50\n",
      "Epoch  49/500 | Train Loss: 0.2250 | Val Loss: 0.2074 | Train F1: 0.7896 | Val F1: 0.7577 | LR: 8.00e-04\n",
      "         Best F1: 0.7643 (Ep 45) | Best Loss: 0.1959 (Ep 46) | No F1 Improve: 4/50 | No Loss Improve: 3/50\n",
      "‚úÖ NEW BEST F1: 0.7702 at epoch 50 (improvement: +0.0010)\n",
      "Epoch  50/500 | Train Loss: 0.2210 | Val Loss: 0.2008 | Train F1: 0.7885 | Val F1: 0.7702 | LR: 8.00e-04\n",
      "         Best F1: 0.7702 (Ep 50) | Best Loss: 0.1959 (Ep 46) | No F1 Improve: 0/50 | No Loss Improve: 4/50\n",
      "----------------------------------------------------------------------------------------------------\n",
      "‚úÖ NEW BEST F1: 0.7745 at epoch 51 (improvement: +0.0010)\n",
      "Epoch  51/500 | Train Loss: 0.2226 | Val Loss: 0.1980 | Train F1: 0.7933 | Val F1: 0.7745 | LR: 8.00e-04\n",
      "         Best F1: 0.7745 (Ep 51) | Best Loss: 0.1959 (Ep 46) | No F1 Improve: 0/50 | No Loss Improve: 5/50\n",
      "üîΩ NEW BEST VAL LOSS: 0.1922 at epoch 52 (improvement: -0.0010)\n",
      "Epoch  52/500 | Train Loss: 0.2190 | Val Loss: 0.1922 | Train F1: 0.7969 | Val F1: 0.7730 | LR: 8.00e-04\n",
      "         Best F1: 0.7745 (Ep 51) | Best Loss: 0.1922 (Ep 52) | No F1 Improve: 1/50 | No Loss Improve: 0/50\n",
      "‚úÖ NEW BEST F1: 0.7780 at epoch 53 (improvement: +0.0010)\n",
      "  üîΩ Learning rate reduced from 8.00e-04 to 6.40e-04 (based on combined)\n",
      "Epoch  53/500 | Train Loss: 0.2193 | Val Loss: 0.1942 | Train F1: 0.8008 | Val F1: 0.7780 | LR: 6.40e-04\n",
      "         Best F1: 0.7780 (Ep 53) | Best Loss: 0.1922 (Ep 52) | No F1 Improve: 0/50 | No Loss Improve: 1/50\n",
      "Epoch  54/500 | Train Loss: 0.2117 | Val Loss: 0.1969 | Train F1: 0.8036 | Val F1: 0.7768 | LR: 6.40e-04\n",
      "         Best F1: 0.7780 (Ep 53) | Best Loss: 0.1922 (Ep 52) | No F1 Improve: 1/50 | No Loss Improve: 2/50\n",
      "‚úÖ NEW BEST F1: 0.7809 at epoch 55 (improvement: +0.0010)\n",
      "Epoch  55/500 | Train Loss: 0.2147 | Val Loss: 0.1936 | Train F1: 0.7987 | Val F1: 0.7809 | LR: 6.40e-04\n",
      "         Best F1: 0.7809 (Ep 55) | Best Loss: 0.1922 (Ep 52) | No F1 Improve: 0/50 | No Loss Improve: 3/50\n",
      "üîΩ NEW BEST VAL LOSS: 0.1883 at epoch 56 (improvement: -0.0010)\n",
      "Epoch  56/500 | Train Loss: 0.2105 | Val Loss: 0.1883 | Train F1: 0.8063 | Val F1: 0.7799 | LR: 6.40e-04\n",
      "         Best F1: 0.7809 (Ep 55) | Best Loss: 0.1883 (Ep 56) | No F1 Improve: 1/50 | No Loss Improve: 0/50\n",
      "Epoch  57/500 | Train Loss: 0.2122 | Val Loss: 0.1905 | Train F1: 0.7996 | Val F1: 0.7762 | LR: 6.40e-04\n",
      "         Best F1: 0.7809 (Ep 55) | Best Loss: 0.1883 (Ep 56) | No F1 Improve: 2/50 | No Loss Improve: 1/50\n",
      "‚úÖ NEW BEST F1: 0.7833 at epoch 58 (improvement: +0.0010)\n",
      "üîΩ NEW BEST VAL LOSS: 0.1851 at epoch 58 (improvement: -0.0010)\n",
      "Epoch  58/500 | Train Loss: 0.2072 | Val Loss: 0.1851 | Train F1: 0.8005 | Val F1: 0.7833 | LR: 6.40e-04\n",
      "         Best F1: 0.7833 (Ep 58) | Best Loss: 0.1851 (Ep 58) | No F1 Improve: 0/50 | No Loss Improve: 0/50\n",
      "Epoch  59/500 | Train Loss: 0.2037 | Val Loss: 0.1917 | Train F1: 0.8130 | Val F1: 0.7827 | LR: 6.40e-04\n",
      "         Best F1: 0.7833 (Ep 58) | Best Loss: 0.1851 (Ep 58) | No F1 Improve: 1/50 | No Loss Improve: 1/50\n",
      "Epoch  60/500 | Train Loss: 0.1970 | Val Loss: 0.2002 | Train F1: 0.8113 | Val F1: 0.7632 | LR: 6.40e-04\n",
      "         Best F1: 0.7833 (Ep 58) | Best Loss: 0.1851 (Ep 58) | No F1 Improve: 2/50 | No Loss Improve: 2/50\n",
      "----------------------------------------------------------------------------------------------------\n",
      "üîΩ NEW BEST VAL LOSS: 0.1834 at epoch 61 (improvement: -0.0010)\n",
      "Epoch  61/500 | Train Loss: 0.2165 | Val Loss: 0.1834 | Train F1: 0.7763 | Val F1: 0.7752 | LR: 6.40e-04\n",
      "         Best F1: 0.7833 (Ep 58) | Best Loss: 0.1834 (Ep 61) | No F1 Improve: 3/50 | No Loss Improve: 0/50\n",
      "Epoch  62/500 | Train Loss: 0.2104 | Val Loss: 0.1947 | Train F1: 0.7996 | Val F1: 0.7814 | LR: 6.40e-04\n",
      "         Best F1: 0.7833 (Ep 58) | Best Loss: 0.1834 (Ep 61) | No F1 Improve: 4/50 | No Loss Improve: 1/50\n",
      "Epoch  63/500 | Train Loss: 0.2064 | Val Loss: 0.1878 | Train F1: 0.8125 | Val F1: 0.7806 | LR: 6.40e-04\n",
      "         Best F1: 0.7833 (Ep 58) | Best Loss: 0.1834 (Ep 61) | No F1 Improve: 5/50 | No Loss Improve: 2/50\n",
      "Epoch  64/500 | Train Loss: 0.2010 | Val Loss: 0.1834 | Train F1: 0.8113 | Val F1: 0.7790 | LR: 6.40e-04\n",
      "         Best F1: 0.7833 (Ep 58) | Best Loss: 0.1834 (Ep 61) | No F1 Improve: 6/50 | No Loss Improve: 3/50\n",
      "Epoch  65/500 | Train Loss: 0.1945 | Val Loss: 0.1907 | Train F1: 0.8135 | Val F1: 0.7737 | LR: 6.40e-04\n",
      "         Best F1: 0.7833 (Ep 58) | Best Loss: 0.1834 (Ep 61) | No F1 Improve: 7/50 | No Loss Improve: 4/50\n",
      "Epoch  66/500 | Train Loss: 0.1984 | Val Loss: 0.1964 | Train F1: 0.8182 | Val F1: 0.7642 | LR: 6.40e-04\n",
      "         Best F1: 0.7833 (Ep 58) | Best Loss: 0.1834 (Ep 61) | No F1 Improve: 8/50 | No Loss Improve: 5/50\n",
      "üîΩ NEW BEST VAL LOSS: 0.1822 at epoch 67 (improvement: -0.0010)\n",
      "Epoch  67/500 | Train Loss: 0.1978 | Val Loss: 0.1822 | Train F1: 0.8169 | Val F1: 0.7723 | LR: 6.40e-04\n",
      "         Best F1: 0.7833 (Ep 58) | Best Loss: 0.1822 (Ep 67) | No F1 Improve: 9/50 | No Loss Improve: 0/50\n",
      "Epoch  68/500 | Train Loss: 0.2032 | Val Loss: 0.1928 | Train F1: 0.8185 | Val F1: 0.7677 | LR: 6.40e-04\n",
      "         Best F1: 0.7833 (Ep 58) | Best Loss: 0.1822 (Ep 67) | No F1 Improve: 10/50 | No Loss Improve: 1/50\n",
      "üîΩ NEW BEST VAL LOSS: 0.1777 at epoch 69 (improvement: -0.0010)\n",
      "Epoch  69/500 | Train Loss: 0.1977 | Val Loss: 0.1777 | Train F1: 0.8095 | Val F1: 0.7723 | LR: 6.40e-04\n",
      "         Best F1: 0.7833 (Ep 58) | Best Loss: 0.1777 (Ep 69) | No F1 Improve: 11/50 | No Loss Improve: 0/50\n",
      "Epoch  70/500 | Train Loss: 0.1979 | Val Loss: 0.1854 | Train F1: 0.8128 | Val F1: 0.7709 | LR: 6.40e-04\n",
      "         Best F1: 0.7833 (Ep 58) | Best Loss: 0.1777 (Ep 69) | No F1 Improve: 12/50 | No Loss Improve: 1/50\n",
      "----------------------------------------------------------------------------------------------------\n",
      "üîΩ NEW BEST VAL LOSS: 0.1730 at epoch 71 (improvement: -0.0010)\n",
      "Epoch  71/500 | Train Loss: 0.1977 | Val Loss: 0.1730 | Train F1: 0.8202 | Val F1: 0.7762 | LR: 6.40e-04\n",
      "         Best F1: 0.7833 (Ep 58) | Best Loss: 0.1730 (Ep 71) | No F1 Improve: 13/50 | No Loss Improve: 0/50\n",
      "üîΩ NEW BEST VAL LOSS: 0.1718 at epoch 72 (improvement: -0.0010)\n",
      "Epoch  72/500 | Train Loss: 0.1966 | Val Loss: 0.1718 | Train F1: 0.8129 | Val F1: 0.7768 | LR: 6.40e-04\n",
      "         Best F1: 0.7833 (Ep 58) | Best Loss: 0.1718 (Ep 72) | No F1 Improve: 14/50 | No Loss Improve: 0/50\n",
      "Epoch  73/500 | Train Loss: 0.1968 | Val Loss: 0.1920 | Train F1: 0.8181 | Val F1: 0.7622 | LR: 6.40e-04\n",
      "         Best F1: 0.7833 (Ep 58) | Best Loss: 0.1718 (Ep 72) | No F1 Improve: 15/50 | No Loss Improve: 1/50\n",
      "üîΩ NEW BEST VAL LOSS: 0.1703 at epoch 74 (improvement: -0.0010)\n",
      "Epoch  74/500 | Train Loss: 0.1995 | Val Loss: 0.1703 | Train F1: 0.8033 | Val F1: 0.7676 | LR: 6.40e-04\n",
      "         Best F1: 0.7833 (Ep 58) | Best Loss: 0.1703 (Ep 74) | No F1 Improve: 16/50 | No Loss Improve: 0/50\n",
      "Epoch  75/500 | Train Loss: 0.1924 | Val Loss: 0.1722 | Train F1: 0.8071 | Val F1: 0.7796 | LR: 6.40e-04\n",
      "         Best F1: 0.7833 (Ep 58) | Best Loss: 0.1703 (Ep 74) | No F1 Improve: 17/50 | No Loss Improve: 1/50\n",
      "üîΩ NEW BEST VAL LOSS: 0.1688 at epoch 76 (improvement: -0.0010)\n",
      "Epoch  76/500 | Train Loss: 0.1898 | Val Loss: 0.1688 | Train F1: 0.8192 | Val F1: 0.7693 | LR: 6.40e-04\n",
      "         Best F1: 0.7833 (Ep 58) | Best Loss: 0.1688 (Ep 76) | No F1 Improve: 18/50 | No Loss Improve: 0/50\n",
      "Epoch  77/500 | Train Loss: 0.1853 | Val Loss: 0.1687 | Train F1: 0.8237 | Val F1: 0.7728 | LR: 6.40e-04\n",
      "         Best F1: 0.7833 (Ep 58) | Best Loss: 0.1688 (Ep 76) | No F1 Improve: 19/50 | No Loss Improve: 1/50\n",
      "‚úÖ NEW BEST F1: 0.7844 at epoch 78 (improvement: +0.0010)\n",
      "Epoch  78/500 | Train Loss: 0.1807 | Val Loss: 0.1743 | Train F1: 0.8228 | Val F1: 0.7844 | LR: 6.40e-04\n",
      "         Best F1: 0.7844 (Ep 78) | Best Loss: 0.1688 (Ep 76) | No F1 Improve: 0/50 | No Loss Improve: 2/50\n",
      "‚úÖ NEW BEST F1: 0.7937 at epoch 79 (improvement: +0.0010)\n",
      "üîΩ NEW BEST VAL LOSS: 0.1666 at epoch 79 (improvement: -0.0010)\n",
      "  üîΩ Learning rate reduced from 6.40e-04 to 5.12e-04 (based on combined)\n",
      "Epoch  79/500 | Train Loss: 0.1812 | Val Loss: 0.1666 | Train F1: 0.8298 | Val F1: 0.7937 | LR: 5.12e-04\n",
      "         Best F1: 0.7937 (Ep 79) | Best Loss: 0.1666 (Ep 79) | No F1 Improve: 0/50 | No Loss Improve: 0/50\n",
      "‚úÖ NEW BEST F1: 0.8038 at epoch 80 (improvement: +0.0010)\n",
      "üîΩ NEW BEST VAL LOSS: 0.1610 at epoch 80 (improvement: -0.0010)\n",
      "Epoch  80/500 | Train Loss: 0.1716 | Val Loss: 0.1610 | Train F1: 0.8290 | Val F1: 0.8038 | LR: 5.12e-04\n",
      "         Best F1: 0.8038 (Ep 80) | Best Loss: 0.1610 (Ep 80) | No F1 Improve: 0/50 | No Loss Improve: 0/50\n",
      "----------------------------------------------------------------------------------------------------\n",
      "‚úÖ NEW BEST F1: 0.8055 at epoch 81 (improvement: +0.0010)\n",
      "Epoch  81/500 | Train Loss: 0.1778 | Val Loss: 0.1681 | Train F1: 0.8320 | Val F1: 0.8055 | LR: 5.12e-04\n",
      "         Best F1: 0.8055 (Ep 81) | Best Loss: 0.1610 (Ep 80) | No F1 Improve: 0/50 | No Loss Improve: 1/50\n",
      "Epoch  82/500 | Train Loss: 0.1756 | Val Loss: 0.1638 | Train F1: 0.8286 | Val F1: 0.8006 | LR: 5.12e-04\n",
      "         Best F1: 0.8055 (Ep 81) | Best Loss: 0.1610 (Ep 80) | No F1 Improve: 1/50 | No Loss Improve: 2/50\n",
      "Epoch  83/500 | Train Loss: 0.1708 | Val Loss: 0.1670 | Train F1: 0.8339 | Val F1: 0.8043 | LR: 5.12e-04\n",
      "         Best F1: 0.8055 (Ep 81) | Best Loss: 0.1610 (Ep 80) | No F1 Improve: 2/50 | No Loss Improve: 3/50\n",
      "Epoch  84/500 | Train Loss: 0.1735 | Val Loss: 0.1769 | Train F1: 0.8320 | Val F1: 0.7976 | LR: 5.12e-04\n",
      "         Best F1: 0.8055 (Ep 81) | Best Loss: 0.1610 (Ep 80) | No F1 Improve: 3/50 | No Loss Improve: 4/50\n",
      "‚úÖ NEW BEST F1: 0.8157 at epoch 85 (improvement: +0.0010)\n",
      "Epoch  85/500 | Train Loss: 0.1672 | Val Loss: 0.1667 | Train F1: 0.8476 | Val F1: 0.8157 | LR: 5.12e-04\n",
      "         Best F1: 0.8157 (Ep 85) | Best Loss: 0.1610 (Ep 80) | No F1 Improve: 0/50 | No Loss Improve: 5/50\n",
      "‚úÖ NEW BEST F1: 0.8318 at epoch 86 (improvement: +0.0010)\n",
      "üîΩ NEW BEST VAL LOSS: 0.1432 at epoch 86 (improvement: -0.0010)\n",
      "Epoch  86/500 | Train Loss: 0.1667 | Val Loss: 0.1432 | Train F1: 0.8476 | Val F1: 0.8318 | LR: 5.12e-04\n",
      "         Best F1: 0.8318 (Ep 86) | Best Loss: 0.1432 (Ep 86) | No F1 Improve: 0/50 | No Loss Improve: 0/50\n",
      "Epoch  87/500 | Train Loss: 0.1668 | Val Loss: 0.1667 | Train F1: 0.8471 | Val F1: 0.8212 | LR: 5.12e-04\n",
      "         Best F1: 0.8318 (Ep 86) | Best Loss: 0.1432 (Ep 86) | No F1 Improve: 1/50 | No Loss Improve: 1/50\n",
      "Epoch  88/500 | Train Loss: 0.1606 | Val Loss: 0.1747 | Train F1: 0.8567 | Val F1: 0.8062 | LR: 5.12e-04\n",
      "         Best F1: 0.8318 (Ep 86) | Best Loss: 0.1432 (Ep 86) | No F1 Improve: 2/50 | No Loss Improve: 2/50\n",
      "Epoch  89/500 | Train Loss: 0.1586 | Val Loss: 0.1505 | Train F1: 0.8438 | Val F1: 0.8308 | LR: 5.12e-04\n",
      "         Best F1: 0.8318 (Ep 86) | Best Loss: 0.1432 (Ep 86) | No F1 Improve: 3/50 | No Loss Improve: 3/50\n",
      "Epoch  90/500 | Train Loss: 0.1513 | Val Loss: 0.1682 | Train F1: 0.8617 | Val F1: 0.8227 | LR: 5.12e-04\n",
      "         Best F1: 0.8318 (Ep 86) | Best Loss: 0.1432 (Ep 86) | No F1 Improve: 4/50 | No Loss Improve: 4/50\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch  91/500 | Train Loss: 0.1644 | Val Loss: 0.1807 | Train F1: 0.8570 | Val F1: 0.8201 | LR: 5.12e-04\n",
      "         Best F1: 0.8318 (Ep 86) | Best Loss: 0.1432 (Ep 86) | No F1 Improve: 5/50 | No Loss Improve: 5/50\n",
      "Epoch  92/500 | Train Loss: 0.1542 | Val Loss: 0.1670 | Train F1: 0.8657 | Val F1: 0.8186 | LR: 5.12e-04\n",
      "         Best F1: 0.8318 (Ep 86) | Best Loss: 0.1432 (Ep 86) | No F1 Improve: 6/50 | No Loss Improve: 6/50\n",
      "Epoch  93/500 | Train Loss: 0.1526 | Val Loss: 0.1571 | Train F1: 0.8576 | Val F1: 0.8217 | LR: 5.12e-04\n",
      "         Best F1: 0.8318 (Ep 86) | Best Loss: 0.1432 (Ep 86) | No F1 Improve: 7/50 | No Loss Improve: 7/50\n",
      "Epoch  94/500 | Train Loss: 0.1501 | Val Loss: 0.1605 | Train F1: 0.8620 | Val F1: 0.8241 | LR: 5.12e-04\n",
      "         Best F1: 0.8318 (Ep 86) | Best Loss: 0.1432 (Ep 86) | No F1 Improve: 8/50 | No Loss Improve: 8/50\n",
      "‚úÖ NEW BEST F1: 0.8372 at epoch 95 (improvement: +0.0010)\n",
      "üîΩ NEW BEST VAL LOSS: 0.1407 at epoch 95 (improvement: -0.0010)\n",
      "Epoch  95/500 | Train Loss: 0.1467 | Val Loss: 0.1407 | Train F1: 0.8625 | Val F1: 0.8372 | LR: 5.12e-04\n",
      "         Best F1: 0.8372 (Ep 95) | Best Loss: 0.1407 (Ep 95) | No F1 Improve: 0/50 | No Loss Improve: 0/50\n",
      "Epoch  96/500 | Train Loss: 0.1578 | Val Loss: 0.1584 | Train F1: 0.8608 | Val F1: 0.8301 | LR: 5.12e-04\n",
      "         Best F1: 0.8372 (Ep 95) | Best Loss: 0.1407 (Ep 95) | No F1 Improve: 1/50 | No Loss Improve: 1/50\n",
      "Epoch  97/500 | Train Loss: 0.1505 | Val Loss: 0.1457 | Train F1: 0.8645 | Val F1: 0.8324 | LR: 5.12e-04\n",
      "         Best F1: 0.8372 (Ep 95) | Best Loss: 0.1407 (Ep 95) | No F1 Improve: 2/50 | No Loss Improve: 2/50\n",
      "‚úÖ NEW BEST F1: 0.8394 at epoch 98 (improvement: +0.0010)\n",
      "Epoch  98/500 | Train Loss: 0.1503 | Val Loss: 0.1662 | Train F1: 0.8641 | Val F1: 0.8394 | LR: 5.12e-04\n",
      "         Best F1: 0.8394 (Ep 98) | Best Loss: 0.1407 (Ep 95) | No F1 Improve: 0/50 | No Loss Improve: 3/50\n",
      "Epoch  99/500 | Train Loss: 0.1435 | Val Loss: 0.1557 | Train F1: 0.8686 | Val F1: 0.8286 | LR: 5.12e-04\n",
      "         Best F1: 0.8394 (Ep 98) | Best Loss: 0.1407 (Ep 95) | No F1 Improve: 1/50 | No Loss Improve: 4/50\n",
      "Epoch 100/500 | Train Loss: 0.1463 | Val Loss: 0.1537 | Train F1: 0.8672 | Val F1: 0.8373 | LR: 5.12e-04\n",
      "         Best F1: 0.8394 (Ep 98) | Best Loss: 0.1407 (Ep 95) | No F1 Improve: 2/50 | No Loss Improve: 5/50\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 101/500 | Train Loss: 0.1533 | Val Loss: 0.1492 | Train F1: 0.8534 | Val F1: 0.8304 | LR: 5.12e-04\n",
      "         Best F1: 0.8394 (Ep 98) | Best Loss: 0.1407 (Ep 95) | No F1 Improve: 3/50 | No Loss Improve: 6/50\n",
      "Epoch 102/500 | Train Loss: 0.1486 | Val Loss: 0.1819 | Train F1: 0.8602 | Val F1: 0.8146 | LR: 5.12e-04\n",
      "         Best F1: 0.8394 (Ep 98) | Best Loss: 0.1407 (Ep 95) | No F1 Improve: 4/50 | No Loss Improve: 7/50\n",
      "Epoch 103/500 | Train Loss: 0.1528 | Val Loss: 0.1447 | Train F1: 0.8545 | Val F1: 0.8272 | LR: 5.12e-04\n",
      "         Best F1: 0.8394 (Ep 98) | Best Loss: 0.1407 (Ep 95) | No F1 Improve: 5/50 | No Loss Improve: 8/50\n",
      "Epoch 104/500 | Train Loss: 0.1443 | Val Loss: 0.1521 | Train F1: 0.8610 | Val F1: 0.8243 | LR: 5.12e-04\n",
      "         Best F1: 0.8394 (Ep 98) | Best Loss: 0.1407 (Ep 95) | No F1 Improve: 6/50 | No Loss Improve: 9/50\n",
      "  üîΩ Learning rate reduced from 5.12e-04 to 4.10e-04 (based on combined)\n",
      "Epoch 105/500 | Train Loss: 0.1446 | Val Loss: 0.1464 | Train F1: 0.8625 | Val F1: 0.8390 | LR: 4.10e-04\n",
      "         Best F1: 0.8394 (Ep 98) | Best Loss: 0.1407 (Ep 95) | No F1 Improve: 7/50 | No Loss Improve: 10/50\n",
      "Epoch 106/500 | Train Loss: 0.1369 | Val Loss: 0.1525 | Train F1: 0.8687 | Val F1: 0.8396 | LR: 4.10e-04\n",
      "         Best F1: 0.8394 (Ep 98) | Best Loss: 0.1407 (Ep 95) | No F1 Improve: 8/50 | No Loss Improve: 11/50\n",
      "Epoch 107/500 | Train Loss: 0.1408 | Val Loss: 0.1513 | Train F1: 0.8690 | Val F1: 0.8341 | LR: 4.10e-04\n",
      "         Best F1: 0.8394 (Ep 98) | Best Loss: 0.1407 (Ep 95) | No F1 Improve: 9/50 | No Loss Improve: 12/50\n",
      "Epoch 108/500 | Train Loss: 0.1407 | Val Loss: 0.1630 | Train F1: 0.8673 | Val F1: 0.8195 | LR: 4.10e-04\n",
      "         Best F1: 0.8394 (Ep 98) | Best Loss: 0.1407 (Ep 95) | No F1 Improve: 10/50 | No Loss Improve: 13/50\n",
      "Epoch 109/500 | Train Loss: 0.1439 | Val Loss: 0.1487 | Train F1: 0.8545 | Val F1: 0.8327 | LR: 4.10e-04\n",
      "         Best F1: 0.8394 (Ep 98) | Best Loss: 0.1407 (Ep 95) | No F1 Improve: 11/50 | No Loss Improve: 14/50\n",
      "Epoch 110/500 | Train Loss: 0.1388 | Val Loss: 0.1501 | Train F1: 0.8593 | Val F1: 0.8294 | LR: 4.10e-04\n",
      "         Best F1: 0.8394 (Ep 98) | Best Loss: 0.1407 (Ep 95) | No F1 Improve: 12/50 | No Loss Improve: 15/50\n",
      "----------------------------------------------------------------------------------------------------\n",
      "‚úÖ NEW BEST F1: 0.8436 at epoch 111 (improvement: +0.0010)\n",
      "Epoch 111/500 | Train Loss: 0.1340 | Val Loss: 0.1520 | Train F1: 0.8683 | Val F1: 0.8436 | LR: 4.10e-04\n",
      "         Best F1: 0.8436 (Ep 111) | Best Loss: 0.1407 (Ep 95) | No F1 Improve: 0/50 | No Loss Improve: 16/50\n",
      "‚úÖ NEW BEST F1: 0.8476 at epoch 112 (improvement: +0.0010)\n",
      "Epoch 112/500 | Train Loss: 0.1373 | Val Loss: 0.1505 | Train F1: 0.8754 | Val F1: 0.8476 | LR: 4.10e-04\n",
      "         Best F1: 0.8476 (Ep 112) | Best Loss: 0.1407 (Ep 95) | No F1 Improve: 0/50 | No Loss Improve: 17/50\n",
      "Epoch 113/500 | Train Loss: 0.1388 | Val Loss: 0.1454 | Train F1: 0.8767 | Val F1: 0.8435 | LR: 4.10e-04\n",
      "         Best F1: 0.8476 (Ep 112) | Best Loss: 0.1407 (Ep 95) | No F1 Improve: 1/50 | No Loss Improve: 18/50\n",
      "Epoch 114/500 | Train Loss: 0.1314 | Val Loss: 0.1497 | Train F1: 0.8709 | Val F1: 0.8436 | LR: 4.10e-04\n",
      "         Best F1: 0.8476 (Ep 112) | Best Loss: 0.1407 (Ep 95) | No F1 Improve: 2/50 | No Loss Improve: 19/50\n",
      "Epoch 115/500 | Train Loss: 0.1386 | Val Loss: 0.1495 | Train F1: 0.8737 | Val F1: 0.8479 | LR: 4.10e-04\n",
      "         Best F1: 0.8476 (Ep 112) | Best Loss: 0.1407 (Ep 95) | No F1 Improve: 3/50 | No Loss Improve: 20/50\n",
      "‚úÖ NEW BEST F1: 0.8586 at epoch 116 (improvement: +0.0010)\n",
      "Epoch 116/500 | Train Loss: 0.1416 | Val Loss: 0.1439 | Train F1: 0.8842 | Val F1: 0.8586 | LR: 4.10e-04\n",
      "         Best F1: 0.8586 (Ep 116) | Best Loss: 0.1407 (Ep 95) | No F1 Improve: 0/50 | No Loss Improve: 21/50\n",
      "Epoch 117/500 | Train Loss: 0.1357 | Val Loss: 0.1497 | Train F1: 0.8808 | Val F1: 0.8475 | LR: 4.10e-04\n",
      "         Best F1: 0.8586 (Ep 116) | Best Loss: 0.1407 (Ep 95) | No F1 Improve: 1/50 | No Loss Improve: 22/50\n",
      "‚úÖ NEW BEST F1: 0.8680 at epoch 118 (improvement: +0.0010)\n",
      "Epoch 118/500 | Train Loss: 0.1314 | Val Loss: 0.1481 | Train F1: 0.8950 | Val F1: 0.8680 | LR: 4.10e-04\n",
      "         Best F1: 0.8680 (Ep 118) | Best Loss: 0.1407 (Ep 95) | No F1 Improve: 0/50 | No Loss Improve: 23/50\n",
      "‚úÖ NEW BEST F1: 0.8705 at epoch 119 (improvement: +0.0010)\n",
      "Epoch 119/500 | Train Loss: 0.1380 | Val Loss: 0.1472 | Train F1: 0.9018 | Val F1: 0.8705 | LR: 4.10e-04\n",
      "         Best F1: 0.8705 (Ep 119) | Best Loss: 0.1407 (Ep 95) | No F1 Improve: 0/50 | No Loss Improve: 24/50\n",
      "Epoch 120/500 | Train Loss: 0.1331 | Val Loss: 0.1576 | Train F1: 0.8953 | Val F1: 0.8509 | LR: 4.10e-04\n",
      "         Best F1: 0.8705 (Ep 119) | Best Loss: 0.1407 (Ep 95) | No F1 Improve: 1/50 | No Loss Improve: 25/50\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 121/500 | Train Loss: 0.1426 | Val Loss: 0.1477 | Train F1: 0.8961 | Val F1: 0.8538 | LR: 4.10e-04\n",
      "         Best F1: 0.8705 (Ep 119) | Best Loss: 0.1407 (Ep 95) | No F1 Improve: 2/50 | No Loss Improve: 26/50\n",
      "üîΩ NEW BEST VAL LOSS: 0.1385 at epoch 122 (improvement: -0.0010)\n",
      "Epoch 122/500 | Train Loss: 0.1441 | Val Loss: 0.1385 | Train F1: 0.8855 | Val F1: 0.8521 | LR: 4.10e-04\n",
      "         Best F1: 0.8705 (Ep 119) | Best Loss: 0.1385 (Ep 122) | No F1 Improve: 3/50 | No Loss Improve: 0/50\n",
      "Epoch 123/500 | Train Loss: 0.1358 | Val Loss: 0.1642 | Train F1: 0.8992 | Val F1: 0.8439 | LR: 4.10e-04\n",
      "         Best F1: 0.8705 (Ep 119) | Best Loss: 0.1385 (Ep 122) | No F1 Improve: 4/50 | No Loss Improve: 1/50\n",
      "Epoch 124/500 | Train Loss: 0.1360 | Val Loss: 0.1479 | Train F1: 0.8998 | Val F1: 0.8572 | LR: 4.10e-04\n",
      "         Best F1: 0.8705 (Ep 119) | Best Loss: 0.1385 (Ep 122) | No F1 Improve: 5/50 | No Loss Improve: 2/50\n",
      "Epoch 125/500 | Train Loss: 0.1323 | Val Loss: 0.1424 | Train F1: 0.9005 | Val F1: 0.8565 | LR: 4.10e-04\n",
      "         Best F1: 0.8705 (Ep 119) | Best Loss: 0.1385 (Ep 122) | No F1 Improve: 6/50 | No Loss Improve: 3/50\n",
      "üîΩ NEW BEST VAL LOSS: 0.1359 at epoch 126 (improvement: -0.0010)\n",
      "Epoch 126/500 | Train Loss: 0.1347 | Val Loss: 0.1359 | Train F1: 0.8991 | Val F1: 0.8633 | LR: 4.10e-04\n",
      "         Best F1: 0.8705 (Ep 119) | Best Loss: 0.1359 (Ep 126) | No F1 Improve: 7/50 | No Loss Improve: 0/50\n",
      "Epoch 127/500 | Train Loss: 0.1379 | Val Loss: 0.1575 | Train F1: 0.8931 | Val F1: 0.8536 | LR: 4.10e-04\n",
      "         Best F1: 0.8705 (Ep 119) | Best Loss: 0.1359 (Ep 126) | No F1 Improve: 8/50 | No Loss Improve: 1/50\n",
      "Epoch 128/500 | Train Loss: 0.1375 | Val Loss: 0.1497 | Train F1: 0.9008 | Val F1: 0.8504 | LR: 4.10e-04\n",
      "         Best F1: 0.8705 (Ep 119) | Best Loss: 0.1359 (Ep 126) | No F1 Improve: 9/50 | No Loss Improve: 2/50\n",
      "Epoch 129/500 | Train Loss: 0.1326 | Val Loss: 0.1355 | Train F1: 0.8904 | Val F1: 0.8578 | LR: 4.10e-04\n",
      "         Best F1: 0.8705 (Ep 119) | Best Loss: 0.1359 (Ep 126) | No F1 Improve: 10/50 | No Loss Improve: 3/50\n",
      "Epoch 130/500 | Train Loss: 0.1328 | Val Loss: 0.1520 | Train F1: 0.9047 | Val F1: 0.8569 | LR: 4.10e-04\n",
      "         Best F1: 0.8705 (Ep 119) | Best Loss: 0.1359 (Ep 126) | No F1 Improve: 11/50 | No Loss Improve: 4/50\n",
      "----------------------------------------------------------------------------------------------------\n",
      "  üîΩ Learning rate reduced from 4.10e-04 to 3.28e-04 (based on combined)\n",
      "Epoch 131/500 | Train Loss: 0.1392 | Val Loss: 0.1465 | Train F1: 0.9007 | Val F1: 0.8613 | LR: 3.28e-04\n",
      "         Best F1: 0.8705 (Ep 119) | Best Loss: 0.1359 (Ep 126) | No F1 Improve: 12/50 | No Loss Improve: 5/50\n",
      "Epoch 132/500 | Train Loss: 0.1291 | Val Loss: 0.1359 | Train F1: 0.9051 | Val F1: 0.8604 | LR: 3.28e-04\n",
      "         Best F1: 0.8705 (Ep 119) | Best Loss: 0.1359 (Ep 126) | No F1 Improve: 13/50 | No Loss Improve: 6/50\n",
      "Epoch 133/500 | Train Loss: 0.1363 | Val Loss: 0.1536 | Train F1: 0.9077 | Val F1: 0.8482 | LR: 3.28e-04\n",
      "         Best F1: 0.8705 (Ep 119) | Best Loss: 0.1359 (Ep 126) | No F1 Improve: 14/50 | No Loss Improve: 7/50\n",
      "Epoch 134/500 | Train Loss: 0.1304 | Val Loss: 0.1371 | Train F1: 0.9040 | Val F1: 0.8453 | LR: 3.28e-04\n",
      "         Best F1: 0.8705 (Ep 119) | Best Loss: 0.1359 (Ep 126) | No F1 Improve: 15/50 | No Loss Improve: 8/50\n",
      "Epoch 135/500 | Train Loss: 0.1291 | Val Loss: 0.1424 | Train F1: 0.8987 | Val F1: 0.8574 | LR: 3.28e-04\n",
      "         Best F1: 0.8705 (Ep 119) | Best Loss: 0.1359 (Ep 126) | No F1 Improve: 16/50 | No Loss Improve: 9/50\n",
      "Epoch 136/500 | Train Loss: 0.1317 | Val Loss: 0.1387 | Train F1: 0.9077 | Val F1: 0.8622 | LR: 3.28e-04\n",
      "         Best F1: 0.8705 (Ep 119) | Best Loss: 0.1359 (Ep 126) | No F1 Improve: 17/50 | No Loss Improve: 10/50\n",
      "Epoch 137/500 | Train Loss: 0.1249 | Val Loss: 0.1402 | Train F1: 0.9113 | Val F1: 0.8602 | LR: 3.28e-04\n",
      "         Best F1: 0.8705 (Ep 119) | Best Loss: 0.1359 (Ep 126) | No F1 Improve: 18/50 | No Loss Improve: 11/50\n",
      "Epoch 138/500 | Train Loss: 0.1263 | Val Loss: 0.1420 | Train F1: 0.9099 | Val F1: 0.8617 | LR: 3.28e-04\n",
      "         Best F1: 0.8705 (Ep 119) | Best Loss: 0.1359 (Ep 126) | No F1 Improve: 19/50 | No Loss Improve: 12/50\n",
      "Epoch 139/500 | Train Loss: 0.1291 | Val Loss: 0.1422 | Train F1: 0.9088 | Val F1: 0.8663 | LR: 3.28e-04\n",
      "         Best F1: 0.8705 (Ep 119) | Best Loss: 0.1359 (Ep 126) | No F1 Improve: 20/50 | No Loss Improve: 13/50\n",
      "‚úÖ NEW BEST F1: 0.8775 at epoch 140 (improvement: +0.0010)\n",
      "Epoch 140/500 | Train Loss: 0.1303 | Val Loss: 0.1395 | Train F1: 0.9010 | Val F1: 0.8775 | LR: 3.28e-04\n",
      "         Best F1: 0.8775 (Ep 140) | Best Loss: 0.1359 (Ep 126) | No F1 Improve: 0/50 | No Loss Improve: 14/50\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 141/500 | Train Loss: 0.1251 | Val Loss: 0.1472 | Train F1: 0.9023 | Val F1: 0.8503 | LR: 3.28e-04\n",
      "         Best F1: 0.8775 (Ep 140) | Best Loss: 0.1359 (Ep 126) | No F1 Improve: 1/50 | No Loss Improve: 15/50\n",
      "Epoch 142/500 | Train Loss: 0.1232 | Val Loss: 0.1379 | Train F1: 0.8966 | Val F1: 0.8506 | LR: 3.28e-04\n",
      "         Best F1: 0.8775 (Ep 140) | Best Loss: 0.1359 (Ep 126) | No F1 Improve: 2/50 | No Loss Improve: 16/50\n",
      "Epoch 143/500 | Train Loss: 0.1237 | Val Loss: 0.1404 | Train F1: 0.9053 | Val F1: 0.8570 | LR: 3.28e-04\n",
      "         Best F1: 0.8775 (Ep 140) | Best Loss: 0.1359 (Ep 126) | No F1 Improve: 3/50 | No Loss Improve: 17/50\n",
      "Epoch 144/500 | Train Loss: 0.1245 | Val Loss: 0.1382 | Train F1: 0.9128 | Val F1: 0.8573 | LR: 3.28e-04\n",
      "         Best F1: 0.8775 (Ep 140) | Best Loss: 0.1359 (Ep 126) | No F1 Improve: 4/50 | No Loss Improve: 18/50\n",
      "Epoch 145/500 | Train Loss: 0.1333 | Val Loss: 0.1718 | Train F1: 0.8982 | Val F1: 0.8453 | LR: 3.28e-04\n",
      "         Best F1: 0.8775 (Ep 140) | Best Loss: 0.1359 (Ep 126) | No F1 Improve: 5/50 | No Loss Improve: 19/50\n",
      "üîΩ NEW BEST VAL LOSS: 0.1307 at epoch 146 (improvement: -0.0010)\n",
      "Epoch 146/500 | Train Loss: 0.1365 | Val Loss: 0.1307 | Train F1: 0.8934 | Val F1: 0.8465 | LR: 3.28e-04\n",
      "         Best F1: 0.8775 (Ep 140) | Best Loss: 0.1307 (Ep 146) | No F1 Improve: 6/50 | No Loss Improve: 0/50\n",
      "Epoch 147/500 | Train Loss: 0.1383 | Val Loss: 0.1476 | Train F1: 0.8950 | Val F1: 0.8442 | LR: 3.28e-04\n",
      "         Best F1: 0.8775 (Ep 140) | Best Loss: 0.1307 (Ep 146) | No F1 Improve: 7/50 | No Loss Improve: 1/50\n",
      "Epoch 148/500 | Train Loss: 0.1335 | Val Loss: 0.1426 | Train F1: 0.9084 | Val F1: 0.8524 | LR: 3.28e-04\n",
      "         Best F1: 0.8775 (Ep 140) | Best Loss: 0.1307 (Ep 146) | No F1 Improve: 8/50 | No Loss Improve: 2/50\n",
      "Epoch 149/500 | Train Loss: 0.1276 | Val Loss: 0.1329 | Train F1: 0.9018 | Val F1: 0.8644 | LR: 3.28e-04\n",
      "         Best F1: 0.8775 (Ep 140) | Best Loss: 0.1307 (Ep 146) | No F1 Improve: 9/50 | No Loss Improve: 3/50\n",
      "Epoch 150/500 | Train Loss: 0.1276 | Val Loss: 0.1438 | Train F1: 0.9122 | Val F1: 0.8502 | LR: 3.28e-04\n",
      "         Best F1: 0.8775 (Ep 140) | Best Loss: 0.1307 (Ep 146) | No F1 Improve: 10/50 | No Loss Improve: 4/50\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 151/500 | Train Loss: 0.1283 | Val Loss: 0.1354 | Train F1: 0.9019 | Val F1: 0.8559 | LR: 3.28e-04\n",
      "         Best F1: 0.8775 (Ep 140) | Best Loss: 0.1307 (Ep 146) | No F1 Improve: 11/50 | No Loss Improve: 5/50\n",
      "üîΩ NEW BEST VAL LOSS: 0.1289 at epoch 152 (improvement: -0.0010)\n",
      "Epoch 152/500 | Train Loss: 0.1236 | Val Loss: 0.1289 | Train F1: 0.9111 | Val F1: 0.8626 | LR: 3.28e-04\n",
      "         Best F1: 0.8775 (Ep 140) | Best Loss: 0.1289 (Ep 152) | No F1 Improve: 12/50 | No Loss Improve: 0/50\n",
      "Epoch 153/500 | Train Loss: 0.1253 | Val Loss: 0.1386 | Train F1: 0.9133 | Val F1: 0.8577 | LR: 3.28e-04\n",
      "         Best F1: 0.8775 (Ep 140) | Best Loss: 0.1289 (Ep 152) | No F1 Improve: 13/50 | No Loss Improve: 1/50\n",
      "Epoch 154/500 | Train Loss: 0.1184 | Val Loss: 0.1369 | Train F1: 0.9122 | Val F1: 0.8553 | LR: 3.28e-04\n",
      "         Best F1: 0.8775 (Ep 140) | Best Loss: 0.1289 (Ep 152) | No F1 Improve: 14/50 | No Loss Improve: 2/50\n",
      "Epoch 155/500 | Train Loss: 0.1192 | Val Loss: 0.1306 | Train F1: 0.9102 | Val F1: 0.8624 | LR: 3.28e-04\n",
      "         Best F1: 0.8775 (Ep 140) | Best Loss: 0.1289 (Ep 152) | No F1 Improve: 15/50 | No Loss Improve: 3/50\n",
      "Epoch 156/500 | Train Loss: 0.1226 | Val Loss: 0.1494 | Train F1: 0.9115 | Val F1: 0.8513 | LR: 3.28e-04\n",
      "         Best F1: 0.8775 (Ep 140) | Best Loss: 0.1289 (Ep 152) | No F1 Improve: 16/50 | No Loss Improve: 4/50\n",
      "  üîΩ Learning rate reduced from 3.28e-04 to 2.62e-04 (based on combined)\n",
      "Epoch 157/500 | Train Loss: 0.1414 | Val Loss: 0.1423 | Train F1: 0.8945 | Val F1: 0.8535 | LR: 2.62e-04\n",
      "         Best F1: 0.8775 (Ep 140) | Best Loss: 0.1289 (Ep 152) | No F1 Improve: 17/50 | No Loss Improve: 5/50\n",
      "Epoch 158/500 | Train Loss: 0.1343 | Val Loss: 0.1286 | Train F1: 0.9018 | Val F1: 0.8651 | LR: 2.62e-04\n",
      "         Best F1: 0.8775 (Ep 140) | Best Loss: 0.1289 (Ep 152) | No F1 Improve: 18/50 | No Loss Improve: 6/50\n",
      "Epoch 159/500 | Train Loss: 0.1236 | Val Loss: 0.1435 | Train F1: 0.9088 | Val F1: 0.8511 | LR: 2.62e-04\n",
      "         Best F1: 0.8775 (Ep 140) | Best Loss: 0.1289 (Ep 152) | No F1 Improve: 19/50 | No Loss Improve: 7/50\n",
      "Epoch 160/500 | Train Loss: 0.1213 | Val Loss: 0.1394 | Train F1: 0.9078 | Val F1: 0.8561 | LR: 2.62e-04\n",
      "         Best F1: 0.8775 (Ep 140) | Best Loss: 0.1289 (Ep 152) | No F1 Improve: 20/50 | No Loss Improve: 8/50\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 161/500 | Train Loss: 0.1183 | Val Loss: 0.1359 | Train F1: 0.9137 | Val F1: 0.8701 | LR: 2.62e-04\n",
      "         Best F1: 0.8775 (Ep 140) | Best Loss: 0.1289 (Ep 152) | No F1 Improve: 21/50 | No Loss Improve: 9/50\n",
      "Epoch 162/500 | Train Loss: 0.1217 | Val Loss: 0.1464 | Train F1: 0.9117 | Val F1: 0.8509 | LR: 2.62e-04\n",
      "         Best F1: 0.8775 (Ep 140) | Best Loss: 0.1289 (Ep 152) | No F1 Improve: 22/50 | No Loss Improve: 10/50\n",
      "Epoch 163/500 | Train Loss: 0.1232 | Val Loss: 0.1379 | Train F1: 0.9161 | Val F1: 0.8563 | LR: 2.62e-04\n",
      "         Best F1: 0.8775 (Ep 140) | Best Loss: 0.1289 (Ep 152) | No F1 Improve: 23/50 | No Loss Improve: 11/50\n",
      "Epoch 164/500 | Train Loss: 0.1202 | Val Loss: 0.1362 | Train F1: 0.9165 | Val F1: 0.8582 | LR: 2.62e-04\n",
      "         Best F1: 0.8775 (Ep 140) | Best Loss: 0.1289 (Ep 152) | No F1 Improve: 24/50 | No Loss Improve: 12/50\n",
      "Epoch 165/500 | Train Loss: 0.1132 | Val Loss: 0.1401 | Train F1: 0.9159 | Val F1: 0.8490 | LR: 2.62e-04\n",
      "         Best F1: 0.8775 (Ep 140) | Best Loss: 0.1289 (Ep 152) | No F1 Improve: 25/50 | No Loss Improve: 13/50\n",
      "Epoch 166/500 | Train Loss: 0.1172 | Val Loss: 0.1369 | Train F1: 0.9115 | Val F1: 0.8516 | LR: 2.62e-04\n",
      "         Best F1: 0.8775 (Ep 140) | Best Loss: 0.1289 (Ep 152) | No F1 Improve: 26/50 | No Loss Improve: 14/50\n",
      "Epoch 167/500 | Train Loss: 0.1242 | Val Loss: 0.1372 | Train F1: 0.9141 | Val F1: 0.8613 | LR: 2.62e-04\n",
      "         Best F1: 0.8775 (Ep 140) | Best Loss: 0.1289 (Ep 152) | No F1 Improve: 27/50 | No Loss Improve: 15/50\n",
      "Epoch 168/500 | Train Loss: 0.1169 | Val Loss: 0.1389 | Train F1: 0.9159 | Val F1: 0.8533 | LR: 2.62e-04\n",
      "         Best F1: 0.8775 (Ep 140) | Best Loss: 0.1289 (Ep 152) | No F1 Improve: 28/50 | No Loss Improve: 16/50\n",
      "Epoch 169/500 | Train Loss: 0.1209 | Val Loss: 0.1343 | Train F1: 0.9119 | Val F1: 0.8562 | LR: 2.62e-04\n",
      "         Best F1: 0.8775 (Ep 140) | Best Loss: 0.1289 (Ep 152) | No F1 Improve: 29/50 | No Loss Improve: 17/50\n",
      "Epoch 170/500 | Train Loss: 0.1177 | Val Loss: 0.1313 | Train F1: 0.9138 | Val F1: 0.8642 | LR: 2.62e-04\n",
      "         Best F1: 0.8775 (Ep 140) | Best Loss: 0.1289 (Ep 152) | No F1 Improve: 30/50 | No Loss Improve: 18/50\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 171/500 | Train Loss: 0.1228 | Val Loss: 0.1285 | Train F1: 0.9177 | Val F1: 0.8622 | LR: 2.62e-04\n",
      "         Best F1: 0.8775 (Ep 140) | Best Loss: 0.1289 (Ep 152) | No F1 Improve: 31/50 | No Loss Improve: 19/50\n",
      "Epoch 172/500 | Train Loss: 0.1158 | Val Loss: 0.1372 | Train F1: 0.9166 | Val F1: 0.8619 | LR: 2.62e-04\n",
      "         Best F1: 0.8775 (Ep 140) | Best Loss: 0.1289 (Ep 152) | No F1 Improve: 32/50 | No Loss Improve: 20/50\n",
      "Epoch 173/500 | Train Loss: 0.1186 | Val Loss: 0.1330 | Train F1: 0.9155 | Val F1: 0.8678 | LR: 2.62e-04\n",
      "         Best F1: 0.8775 (Ep 140) | Best Loss: 0.1289 (Ep 152) | No F1 Improve: 33/50 | No Loss Improve: 21/50\n",
      "Epoch 174/500 | Train Loss: 0.1242 | Val Loss: 0.1488 | Train F1: 0.9142 | Val F1: 0.8484 | LR: 2.62e-04\n",
      "         Best F1: 0.8775 (Ep 140) | Best Loss: 0.1289 (Ep 152) | No F1 Improve: 34/50 | No Loss Improve: 22/50\n",
      "Epoch 175/500 | Train Loss: 0.1240 | Val Loss: 0.1352 | Train F1: 0.9133 | Val F1: 0.8431 | LR: 2.62e-04\n",
      "         Best F1: 0.8775 (Ep 140) | Best Loss: 0.1289 (Ep 152) | No F1 Improve: 35/50 | No Loss Improve: 23/50\n",
      "Epoch 176/500 | Train Loss: 0.1245 | Val Loss: 0.1343 | Train F1: 0.9037 | Val F1: 0.8606 | LR: 2.62e-04\n",
      "         Best F1: 0.8775 (Ep 140) | Best Loss: 0.1289 (Ep 152) | No F1 Improve: 36/50 | No Loss Improve: 24/50\n",
      "Epoch 177/500 | Train Loss: 0.1203 | Val Loss: 0.1409 | Train F1: 0.9158 | Val F1: 0.8624 | LR: 2.62e-04\n",
      "         Best F1: 0.8775 (Ep 140) | Best Loss: 0.1289 (Ep 152) | No F1 Improve: 37/50 | No Loss Improve: 25/50\n",
      "Epoch 178/500 | Train Loss: 0.1199 | Val Loss: 0.1349 | Train F1: 0.9116 | Val F1: 0.8568 | LR: 2.62e-04\n",
      "         Best F1: 0.8775 (Ep 140) | Best Loss: 0.1289 (Ep 152) | No F1 Improve: 38/50 | No Loss Improve: 26/50\n",
      "Epoch 179/500 | Train Loss: 0.1184 | Val Loss: 0.1436 | Train F1: 0.9173 | Val F1: 0.8508 | LR: 2.62e-04\n",
      "         Best F1: 0.8775 (Ep 140) | Best Loss: 0.1289 (Ep 152) | No F1 Improve: 39/50 | No Loss Improve: 27/50\n",
      "Epoch 180/500 | Train Loss: 0.1195 | Val Loss: 0.1303 | Train F1: 0.9113 | Val F1: 0.8709 | LR: 2.62e-04\n",
      "         Best F1: 0.8775 (Ep 140) | Best Loss: 0.1289 (Ep 152) | No F1 Improve: 40/50 | No Loss Improve: 28/50\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 181/500 | Train Loss: 0.1206 | Val Loss: 0.1460 | Train F1: 0.9167 | Val F1: 0.8636 | LR: 2.62e-04\n",
      "         Best F1: 0.8775 (Ep 140) | Best Loss: 0.1289 (Ep 152) | No F1 Improve: 41/50 | No Loss Improve: 29/50\n",
      "Epoch 182/500 | Train Loss: 0.1214 | Val Loss: 0.1305 | Train F1: 0.9128 | Val F1: 0.8680 | LR: 2.62e-04\n",
      "         Best F1: 0.8775 (Ep 140) | Best Loss: 0.1289 (Ep 152) | No F1 Improve: 42/50 | No Loss Improve: 30/50\n",
      "  üîΩ Learning rate reduced from 2.62e-04 to 2.10e-04 (based on combined)\n",
      "Epoch 183/500 | Train Loss: 0.1264 | Val Loss: 0.1431 | Train F1: 0.8955 | Val F1: 0.8421 | LR: 2.10e-04\n",
      "         Best F1: 0.8775 (Ep 140) | Best Loss: 0.1289 (Ep 152) | No F1 Improve: 43/50 | No Loss Improve: 31/50\n",
      "Epoch 184/500 | Train Loss: 0.1247 | Val Loss: 0.1330 | Train F1: 0.8908 | Val F1: 0.8507 | LR: 2.10e-04\n",
      "         Best F1: 0.8775 (Ep 140) | Best Loss: 0.1289 (Ep 152) | No F1 Improve: 44/50 | No Loss Improve: 32/50\n",
      "Epoch 185/500 | Train Loss: 0.1253 | Val Loss: 0.1390 | Train F1: 0.9070 | Val F1: 0.8552 | LR: 2.10e-04\n",
      "         Best F1: 0.8775 (Ep 140) | Best Loss: 0.1289 (Ep 152) | No F1 Improve: 45/50 | No Loss Improve: 33/50\n",
      "Epoch 186/500 | Train Loss: 0.1227 | Val Loss: 0.1321 | Train F1: 0.9210 | Val F1: 0.8701 | LR: 2.10e-04\n",
      "         Best F1: 0.8775 (Ep 140) | Best Loss: 0.1289 (Ep 152) | No F1 Improve: 46/50 | No Loss Improve: 34/50\n",
      "Epoch 187/500 | Train Loss: 0.1174 | Val Loss: 0.1408 | Train F1: 0.9113 | Val F1: 0.8633 | LR: 2.10e-04\n",
      "         Best F1: 0.8775 (Ep 140) | Best Loss: 0.1289 (Ep 152) | No F1 Improve: 47/50 | No Loss Improve: 35/50\n",
      "Epoch 188/500 | Train Loss: 0.1204 | Val Loss: 0.1331 | Train F1: 0.9097 | Val F1: 0.8650 | LR: 2.10e-04\n",
      "         Best F1: 0.8775 (Ep 140) | Best Loss: 0.1289 (Ep 152) | No F1 Improve: 48/50 | No Loss Improve: 36/50\n",
      "Epoch 189/500 | Train Loss: 0.1277 | Val Loss: 0.1444 | Train F1: 0.9158 | Val F1: 0.8569 | LR: 2.10e-04\n",
      "         Best F1: 0.8775 (Ep 140) | Best Loss: 0.1289 (Ep 152) | No F1 Improve: 49/50 | No Loss Improve: 37/50\n",
      "Epoch 190/500 | Train Loss: 0.1235 | Val Loss: 0.1296 | Train F1: 0.9187 | Val F1: 0.8718 | LR: 2.10e-04\n",
      "         Best F1: 0.8775 (Ep 140) | Best Loss: 0.1289 (Ep 152) | No F1 Improve: 50/50 | No Loss Improve: 38/50\n",
      "üõë EARLY STOPPING triggered at epoch 190 (F1 plateau)\n",
      "üéØ Best validation F1: 0.8775 at epoch 140\n",
      "üéØ Best validation Loss: 0.1289 at epoch 152\n",
      "üîÑ Model restored to best F1 state (epoch 140, F1: 0.8775, Val Loss: 0.1395)\n",
      "‚úÖ Training completed!\n",
      "üìä Final Results:\n",
      "   ‚Ä¢ Best validation F1: 0.8775 (Epoch 140)\n",
      "   ‚Ä¢ Best validation Loss: 0.1289 (Epoch 152)\n",
      "   ‚Ä¢ Total epochs trained: 190\n",
      "   ‚Ä¢ Early stopping: Yes\n",
      "   ‚Ä¢ Final val loss at best F1: 0.1395\n",
      "üéØ Regularization applied - L1: 1.00e-06, L2: 1.00e-03\n",
      "\\nüéâ Enhanced Training with Validation Loss Integration Complete!\n",
      "üìä Training Summary:\n",
      "   ‚Ä¢ Best F1 Score: 0.8775\n",
      "   ‚Ä¢ Best Validation Loss: 0.1285\n",
      "   ‚Ä¢ Best Epoch (F1): 140\n",
      "   ‚Ä¢ Best Epoch (Loss): 171\n",
      "   ‚Ä¢ Total Epochs: 190 (stopped early to prevent overfitting)\n",
      "   ‚Ä¢ F1 at best loss epoch: 0.8622\n",
      "   ‚Ä¢ Loss at best F1 epoch: 0.1395\n",
      "   ‚Ä¢ Expected Test Performance: 0.7% - 0.8% (more realistic range)\n",
      "\\nüõ°Ô∏è Overfitting Prevention Applied:\n",
      "‚úÖ Early stopping with patience=50 epochs\n",
      "‚úÖ Learning rate scheduling (F1 or loss-based)\n",
      "‚úÖ L1/L2 regularization\n",
      "‚úÖ Dropout layers\n",
      "‚úÖ Model state restored to best checkpoint\n",
      "‚úÖ Dual metric monitoring (F1 + validation loss)\n",
      "\\nüìã Next Steps for Better Generalization:\n",
      "1. ‚úÖ Use this early-stopped model for submission\n",
      "2. üîÑ Consider cross-validation for more robust validation\n",
      "3. üéõÔ∏è Try ensemble methods (train multiple models)\n",
      "4. üìä Analyze prediction confidence scores\n",
      "5. üîç Review data for potential train/test distribution shifts\n",
      "6. üÜö Compare F1-based vs Loss-based early stopping\n"
     ]
    }
   ],
   "source": [
    "# Enhanced model training setup with ReduceLROnPlateau scheduler and Early Stopping\n",
    "def train_enhanced_model(model, train_loader, val_loader, criterion, optimizer, scheduler=None, epochs=5, \n",
    "                        l1_lambda=None, l2_lambda=None, patience=30, min_delta=0.001, scheduler_metric='f1'):\n",
    "    \"\"\"\n",
    "    Training function for the enhanced model with embeddings\n",
    "    Includes support for ReduceLROnPlateau scheduler, L1/L2 regularization, and Early Stopping\n",
    "    \n",
    "    Args:\n",
    "        model: Enhanced model with embeddings\n",
    "        train_loader: Training data loader\n",
    "        val_loader: Validation data loader  \n",
    "        criterion: Loss function\n",
    "        optimizer: Optimizer\n",
    "        scheduler: Optional ReduceLROnPlateau scheduler\n",
    "        epochs: Maximum number of training epochs\n",
    "        l1_lambda: L1 regularization coefficient (if None, uses L1_LAMBDA global)\n",
    "        l2_lambda: L2 regularization coefficient (if None, uses L2_LAMBDA global)\n",
    "        patience: Early stopping patience (epochs to wait without improvement)\n",
    "        min_delta: Minimum change to qualify as improvement\n",
    "        scheduler_metric: Metric to use for scheduler ('f1', 'val_loss', or 'combined')\n",
    "        \n",
    "    Returns:\n",
    "        train_losses, val_losses, val_f1_scores, train_f1_scores, best_epoch: Training history and best epoch\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_f1_scores = []\n",
    "    train_f1_scores = []  # Track training F1 scores for plotting\n",
    "    \n",
    "    # Early stopping variables - track both F1 and validation loss\n",
    "    best_val_f1 = 0\n",
    "    best_val_loss = float('inf')\n",
    "    best_epoch = 0\n",
    "    best_epoch_loss = 0\n",
    "    epochs_without_improvement = 0\n",
    "    epochs_without_loss_improvement = 0\n",
    "    best_model_state = None\n",
    "    \n",
    "    # Set regularization coefficients\n",
    "    if l1_lambda is None:\n",
    "        l1_lambda = L1_LAMBDA if 'L1_LAMBDA' in globals() else 0\n",
    "    if l2_lambda is None:\n",
    "        l2_lambda = L2_LAMBDA if 'L2_LAMBDA' in globals() else 0\n",
    "    \n",
    "    print(\"Starting enhanced model training with Early Stopping...\")\n",
    "    print(f\"üîß Regularization: L1={l1_lambda:.2e}, L2={l2_lambda:.2e}\")\n",
    "    print(f\"üõë Early Stopping: Patience={patience} epochs, Min improvement={min_delta:.4f}\")\n",
    "    print(f\"üìä Scheduler Metric: {scheduler_metric}\")\n",
    "    if scheduler:\n",
    "        print(f\"üìà Using ReduceLROnPlateau scheduler - Mode: {scheduler.mode}, Factor: {scheduler.factor}, Patience: {scheduler.patience}\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        epoch_train_loss = 0.0\n",
    "        num_batches = 0\n",
    "        # Track training predictions for F1 calculation\n",
    "        train_predictions = []\n",
    "        train_true_labels = []\n",
    "        \n",
    "        for continuous_batch, categorical_batch, labels_batch in train_loader:\n",
    "            # Move to device\n",
    "            continuous_batch = continuous_batch.to(device)\n",
    "            categorical_batch = {k: v.to(device) for k, v in categorical_batch.items()}\n",
    "            labels_batch = labels_batch.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(continuous_batch, categorical_batch)\n",
    "            loss = criterion(outputs, labels_batch)\n",
    "            \n",
    "            # Add L1 and L2 regularization penalties\n",
    "            if l1_lambda > 0 or l2_lambda > 0:\n",
    "                l1_norm = 0\n",
    "                l2_norm = 0\n",
    "                \n",
    "                for param in model.parameters():\n",
    "                    if l1_lambda > 0:\n",
    "                        l1_norm += torch.norm(param, 1)\n",
    "                    if l2_lambda > 0:\n",
    "                        l2_norm += torch.norm(param, 2) ** 2\n",
    "                \n",
    "                # Add regularization terms to loss\n",
    "                regularization_loss = l1_lambda * l1_norm + l2_lambda * l2_norm\n",
    "                loss = loss + regularization_loss\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Collect predictions for F1 calculation\n",
    "            with torch.no_grad():\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                train_predictions.extend(predicted.cpu().numpy())\n",
    "                train_true_labels.extend(labels_batch.cpu().numpy())\n",
    "            \n",
    "            epoch_train_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            \n",
    "            # Print progress occasionally (less frequently for cleaner output)\n",
    "            if num_batches % 20 == 0:  \n",
    "                print(f\"  Epoch {epoch+1}/{epochs}, Batch {num_batches}, Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        avg_train_loss = epoch_train_loss / num_batches\n",
    "            \n",
    "        # Calculate training F1 score\n",
    "        train_f1 = f1_score(train_true_labels, train_predictions, average='weighted')\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_predictions = []\n",
    "        val_true_labels = []\n",
    "        epoch_val_loss = 0.0\n",
    "        val_batches = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for continuous_batch, categorical_batch, labels_batch in val_loader:\n",
    "                continuous_batch = continuous_batch.to(device)\n",
    "                categorical_batch = {k: v.to(device) for k, v in categorical_batch.items()}\n",
    "                labels_batch = labels_batch.to(device)\n",
    "                continuous_batch = continuous_batch.to(device)\n",
    "                outputs = model(continuous_batch, categorical_batch)\n",
    "                loss = criterion(outputs, labels_batch)\n",
    "                epoch_val_loss += loss.item()\n",
    "                val_batches += 1\n",
    "                \n",
    "                # Get predictions\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_predictions.extend(predicted.cpu().numpy())\n",
    "                val_true_labels.extend(labels_batch.cpu().numpy())\n",
    "        avg_val_loss = epoch_val_loss / val_batches\n",
    "        val_f1 = f1_score(val_true_labels, val_predictions, average='weighted')\n",
    "        \n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        val_f1_scores.append(val_f1)\n",
    "        train_f1_scores.append(train_f1)\n",
    "        \n",
    "        # Enhanced Early stopping check - track both F1 and validation loss\n",
    "        f1_improved = val_f1 > best_val_f1 + min_delta\n",
    "        loss_improved = avg_val_loss < best_val_loss - min_delta\n",
    "        \n",
    "        if f1_improved:\n",
    "            best_val_f1 = val_f1\n",
    "            best_epoch = epoch + 1\n",
    "            epochs_without_improvement = 0\n",
    "            # Save best model state based on F1\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            print(f\"‚úÖ NEW BEST F1: {val_f1:.4f} at epoch {best_epoch} (improvement: +{val_f1-(best_val_f1-min_delta):.4f})\")\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            \n",
    "        if loss_improved:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_epoch_loss = epoch + 1\n",
    "            epochs_without_loss_improvement = 0\n",
    "            print(f\"üîΩ NEW BEST VAL LOSS: {avg_val_loss:.4f} at epoch {best_epoch_loss} (improvement: -{(best_val_loss+min_delta)-avg_val_loss:.4f})\")\n",
    "        else:\n",
    "            epochs_without_loss_improvement += 1\n",
    "        \n",
    "        # Update learning rate scheduler if provided\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        if scheduler:\n",
    "            # Choose metric based on scheduler_metric parameter\n",
    "            if scheduler_metric == 'f1':\n",
    "                if scheduler.mode == 'max':\n",
    "                    scheduler.step(val_f1)  # F1 score - higher is better\n",
    "                else:\n",
    "                    # If mode is 'min' but metric is f1, use negative f1\n",
    "                    scheduler.step(-val_f1)\n",
    "            elif scheduler_metric == 'val_loss':\n",
    "                if scheduler.mode == 'min':\n",
    "                    scheduler.step(avg_val_loss)  # Loss - lower is better\n",
    "                else:\n",
    "                    # If mode is 'max' but metric is val_loss, use negative loss\n",
    "                    scheduler.step(-avg_val_loss)\n",
    "            elif scheduler_metric == 'combined':\n",
    "                # Combined metric: normalized F1 - normalized val_loss\n",
    "                # Normalize F1 (0-1) and val_loss (inverse, lower is better)\n",
    "                if len(val_f1_scores) > 1:\n",
    "                    combined_metric = val_f1 - (avg_val_loss / max(val_losses))\n",
    "                    if scheduler.mode == 'max':\n",
    "                        scheduler.step(combined_metric)\n",
    "                    else:\n",
    "                        scheduler.step(-combined_metric)\n",
    "                else:\n",
    "                    # First epoch, use F1\n",
    "                    scheduler.step(val_f1 if scheduler.mode == 'max' else -val_f1)\n",
    "            \n",
    "            new_lr = optimizer.param_groups[0]['lr']\n",
    "            if new_lr != current_lr:\n",
    "                print(f\"  üîΩ Learning rate reduced from {current_lr:.2e} to {new_lr:.2e} (based on {scheduler_metric})\")\n",
    "        \n",
    "        print(f\"Epoch {epoch+1:3d}/{epochs} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | \"\n",
    "              f\"Train F1: {train_f1:.4f} | Val F1: {val_f1:.4f} | LR: {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "        print(f\"         Best F1: {best_val_f1:.4f} (Ep {best_epoch}) | Best Loss: {best_val_loss:.4f} (Ep {best_epoch_loss}) | \"\n",
    "              f\"No F1 Improve: {epochs_without_improvement}/{patience} | No Loss Improve: {epochs_without_loss_improvement}/{patience}\")\n",
    "        \n",
    "        # Early stopping trigger (based on F1 by default, can be modified)\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f\"üõë EARLY STOPPING triggered at epoch {epoch+1} (F1 plateau)\")\n",
    "            print(f\"üéØ Best validation F1: {best_val_f1:.4f} at epoch {best_epoch}\")\n",
    "            print(f\"üéØ Best validation Loss: {best_val_loss:.4f} at epoch {best_epoch_loss}\")\n",
    "            break\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:  # Separator every 10 epochs\n",
    "            print(\"-\" * 100)\n",
    "    \n",
    "    # Restore best model state\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "        print(f\"üîÑ Model restored to best F1 state (epoch {best_epoch}, F1: {best_val_f1:.4f}, Val Loss: {val_losses[best_epoch-1]:.4f})\")\n",
    "    \n",
    "    print(f\"‚úÖ Training completed!\")\n",
    "    print(f\"üìä Final Results:\")\n",
    "    print(f\"   ‚Ä¢ Best validation F1: {best_val_f1:.4f} (Epoch {best_epoch})\")\n",
    "    print(f\"   ‚Ä¢ Best validation Loss: {best_val_loss:.4f} (Epoch {best_epoch_loss})\")\n",
    "    print(f\"   ‚Ä¢ Total epochs trained: {len(val_f1_scores)}\")\n",
    "    print(f\"   ‚Ä¢ Early stopping: {'Yes' if epochs_without_improvement >= patience else 'No'}\")\n",
    "    print(f\"   ‚Ä¢ Final val loss at best F1: {val_losses[best_epoch-1] if best_epoch <= len(val_losses) else 'N/A':.4f}\")\n",
    "    if l1_lambda > 0 or l2_lambda > 0:\n",
    "        print(f\"üéØ Regularization applied - L1: {l1_lambda:.2e}, L2: {l2_lambda:.2e}\")\n",
    "    return train_losses, val_losses, val_f1_scores, train_f1_scores, best_epoch\n",
    "\n",
    "# Create a smaller enhanced model for quick demonstration\n",
    "print(\"=== Creating Enhanced Model for Training Demo ===\")\n",
    "\n",
    "demo_enhanced_model = EnhancedRecurrentClassifier(\n",
    "    continuous_input_size=17,\n",
    "    categorical_features=categorical_feature_config,\n",
    "    embedding_dims=embedding_dims,\n",
    "    hidden_size=HIDDEN_SIZE,  # Smaller for faster training\n",
    "    num_layers=2,\n",
    "    num_classes=3,\n",
    "    rnn_type='LSTM',\n",
    "    bidirectional=True,\n",
    "    dropout_rate=0.2\n",
    ").to(device)\n",
    "\n",
    "# TO WEIGHT MORE THE \"MORE DIFFICULT\" CASES AND THE LESS FREQUENT LABELS:\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=2.0):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.ce = nn.CrossEntropyLoss(weight=alpha, reduction='none')\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = self.ce(inputs, targets)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = ((1 - pt) ** self.gamma) * ce_loss\n",
    "        return focal_loss.mean()\n",
    "        \n",
    "alpha = None\n",
    "# alpha = torch.tensor([0.7, 1.3, 1.7], dtype=torch.float32, device=device)  # None if we don't want to alterate the weights of each label losses (FocalLoss already do it)\n",
    "criterion = FocalLoss(alpha=alpha, gamma=2.3)  # gamma = 0 it's like Crossentropy(), gamma < 1 it's like in between Crossentropy and FocalLoss,\n",
    "                                               # gamma = 1 it's a good compromise, gamma = 1.5 or gamma = 2 to weight so much the less present labels\n",
    "criterion = FocalLoss(alpha=alpha, gamma=2.3)  # gamma = 0 it's like Crossentropy(), gamma < 1 it's like in between Crossentropy and FocalLoss,\n",
    "\n",
    "# Setup training components\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "# Setup training components\n",
    "\n",
    "# Define optimizer with L2 regularization\n",
    "optimizer = torch.optim.AdamW(demo_enhanced_model.parameters(), lr=LEARNING_RATE, weight_decay=L2_LAMBDA)\n",
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Example configurations for different scheduler setups:\n",
    "\n",
    "# Option 1: Focus on F1 score (current setup)\n",
    "scheduler_f1 = ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='max',          # max for F1 score (higher is better)\n",
    "    factor=0.8,          # multiply lr by 0.8 when plateau\n",
    "    patience=25,         # wait 25 epochs without improvement\n",
    "    min_lr=1e-6          # minimum limit for learning rate\n",
    ")\n",
    "\n",
    "# Option 2: Focus on validation loss\n",
    "scheduler_loss = ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min',          # min for validation loss (lower is better)\n",
    "    factor=0.7,          # more aggressive reduction for loss-based\n",
    "    patience=20,         # slightly less patience for loss\n",
    "    min_lr=1e-6\n",
    ")\n",
    "\n",
    "# Option 3: Combined approach - will use the combined metric\n",
    "scheduler_combined = ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='max',          # max for combined metric\n",
    "    factor=0.75,         # moderate reduction\n",
    "    patience=22,         # balanced patience\n",
    "    min_lr=1e-6\n",
    ")\n",
    "\n",
    "# Choose which scheduler to use\n",
    "scheduler = scheduler_f1  # Change this to scheduler_loss or scheduler_combined as needed\n",
    "\n",
    "print(f\"Demo model parameters: {sum(p.numel() for p in demo_enhanced_model.parameters()):,}\")\n",
    "print(\"üìä Available scheduler options:\")\n",
    "print(\"  ‚Ä¢ scheduler_f1: Focuses on F1 score improvement\")\n",
    "print(\"  ‚Ä¢ scheduler_loss: Focuses on validation loss reduction\") \n",
    "print(\"  ‚Ä¢ scheduler_combined: Uses combined F1 and loss metric\")\n",
    "print(f\"üéØ Currently using: {scheduler.__class__.__name__} with mode='{scheduler.mode}'\")\n",
    "print(\"Ready for training!\")\n",
    "\n",
    "# Optional: Run a quick training demo (uncomment to run)\n",
    "demo_train_losses, demo_val_losses, demo_val_f1s, demo_train_f1s, demo_best_epoch = train_enhanced_model(\n",
    "    model=demo_enhanced_model, \n",
    "    train_loader=train_enhanced_loader, \n",
    "    val_loader=val_enhanced_loader, \n",
    "    criterion=criterion, \n",
    "    optimizer=optimizer, \n",
    "    scheduler=scheduler,  # Include the ReduceLROnPlateau scheduler\n",
    "    epochs=EPOCHS,\n",
    "    l1_lambda=L1_LAMBDA,  # L1 regularization\n",
    "    l2_lambda=L2_LAMBDA,  # L2 regularization (in addition to optimizer weight_decay)\n",
    "    patience=50,          # Early stopping patience - wait 50 epochs without improvement\n",
    "    min_delta=0.001,      # Minimum F1 improvement to count as progress\n",
    "    scheduler_metric='combined'  # Can be 'f1', 'val_loss', or 'combined'\n",
    ")\n",
    "print(f\"\\\\nüéâ Enhanced Training with Validation Loss Integration Complete!\")\n",
    "print(f\"üìä Training Summary:\")\n",
    "print(f\"   ‚Ä¢ Best F1 Score: {max(demo_val_f1s):.4f}\")\n",
    "print(f\"   ‚Ä¢ Best Validation Loss: {min(demo_val_losses):.4f}\")\n",
    "print(f\"   ‚Ä¢ Best Epoch (F1): {demo_best_epoch}\")\n",
    "print(f\"   ‚Ä¢ Best Epoch (Loss): {demo_val_losses.index(min(demo_val_losses)) + 1}\")\n",
    "print(f\"   ‚Ä¢ Total Epochs: {len(demo_val_f1s)} (stopped early to prevent overfitting)\")\n",
    "print(f\"   ‚Ä¢ F1 at best loss epoch: {demo_val_f1s[demo_val_losses.index(min(demo_val_losses))]:.4f}\")\n",
    "print(f\"   ‚Ä¢ Loss at best F1 epoch: {demo_val_losses[demo_best_epoch-1]:.4f}\")\n",
    "print(f\"   ‚Ä¢ Expected Test Performance: {max(demo_val_f1s)*0.85:.1f}% - {max(demo_val_f1s)*0.95:.1f}% (more realistic range)\")\n",
    "\n",
    "print(\"\\\\nüõ°Ô∏è Overfitting Prevention Applied:\")\n",
    "print(\"‚úÖ Early stopping with patience=50 epochs\")\n",
    "print(\"‚úÖ Learning rate scheduling (F1 or loss-based)\")\n",
    "print(\"‚úÖ L1/L2 regularization\")\n",
    "print(\"‚úÖ Dropout layers\")\n",
    "print(\"‚úÖ Model state restored to best checkpoint\")\n",
    "print(\"‚úÖ Dual metric monitoring (F1 + validation loss)\")\n",
    "\n",
    "print(\"\\\\nüìã Next Steps for Better Generalization:\")\n",
    "print(\"1. ‚úÖ Use this early-stopped model for submission\")\n",
    "print(\"2. üîÑ Consider cross-validation for more robust validation\")\n",
    "print(\"3. üéõÔ∏è Try ensemble methods (train multiple models)\")\n",
    "print(\"4. üìä Analyze prediction confidence scores\")\n",
    "print(\"5. üîç Review data for potential train/test distribution shifts\")\n",
    "print(\"6. üÜö Compare F1-based vs Loss-based early stopping\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting enhanced model results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Enhanced Model Training Results Visualization ===\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Enhanced Model Training Summary:\n",
      "   ‚Ä¢ Total Epochs: 190\n",
      "   ‚Ä¢ Best Validation F1: 0.8775 (Epoch 140)\n",
      "   ‚Ä¢ Best Validation Loss: 0.1285 (Epoch 171)\n",
      "   ‚Ä¢ Final Validation F1: 0.8718\n",
      "   ‚Ä¢ Final Validation Loss: 0.1296\n"
     ]
    }
   ],
   "source": [
    "# @title Plot Enhanced Model Training History\n",
    "print(\"=== Enhanced Model Training Results Visualization ===\")\n",
    "\n",
    "# Check if enhanced model training results are available\n",
    "if 'demo_train_losses' in globals() and 'demo_val_losses' in globals() and 'demo_val_f1s' in globals():\n",
    "    # Create a figure with three subplots (three columns)\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(nrows=1, ncols=3, figsize=(20, 5))\n",
    "\n",
    "    # Plot 1: Training F1 Score Only\n",
    "    epochs_range = range(1, len(demo_train_losses) + 1)\n",
    "    \n",
    "    # Check if training F1 scores are available\n",
    "    if 'demo_train_f1s' in globals():\n",
    "        ax1.plot(epochs_range, demo_train_f1s, label='Training F1', alpha=0.8, color='#2ca02c', linewidth=2, marker='o', markersize=3)\n",
    "        ax1.set_title('Enhanced Model - Training F1 Score', fontsize=14, fontweight='bold')\n",
    "        ax1.set_xlabel('Epoch')\n",
    "        ax1.set_ylabel('Training F1 Score')\n",
    "        ax1.legend()\n",
    "        ax1.grid(alpha=0.3)\n",
    "        \n",
    "        # Add text with training F1 statistics\n",
    "        final_train_f1 = demo_train_f1s[-1]\n",
    "        max_train_f1 = max(demo_train_f1s)\n",
    "        max_train_f1_epoch = demo_train_f1s.index(max_train_f1) + 1\n",
    "        ax1.text(0.02, 0.98, f'Best Train F1: {max_train_f1:.4f} (Epoch {max_train_f1_epoch})\\nFinal: {final_train_f1:.4f}', \n",
    "                 transform=ax1.transAxes, verticalalignment='top', \n",
    "                 bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.7))\n",
    "    else:\n",
    "        # Fallback to training loss if F1 scores not available\n",
    "        ax1.plot(epochs_range, demo_train_losses, label='Training loss', alpha=0.8, color='#2ca02c', linewidth=2, marker='o', markersize=3)\n",
    "        ax1.set_title('Enhanced Model - Training Loss', fontsize=14, fontweight='bold')\n",
    "        ax1.set_xlabel('Epoch')\n",
    "        ax1.set_ylabel('Training Loss')\n",
    "        ax1.legend()\n",
    "        ax1.grid(alpha=0.3)\n",
    "        \n",
    "        # Add text with final training loss\n",
    "        final_train_loss = demo_train_losses[-1]\n",
    "        min_train_loss = min(demo_train_losses)\n",
    "        min_train_loss_epoch = demo_train_losses.index(min_train_loss) + 1\n",
    "        ax1.text(0.02, 0.98, f'Min Train Loss: {min_train_loss:.4f} (Epoch {min_train_loss_epoch})\\nFinal: {final_train_loss:.4f}', \n",
    "                 transform=ax1.transAxes, verticalalignment='top', \n",
    "                 bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.7))\n",
    "        ax1.text(0.02, 0.85, 'Note: Training F1 scores not available\\nShowing training loss instead', \n",
    "                 transform=ax1.transAxes, verticalalignment='top', \n",
    "                 bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.5))\n",
    "\n",
    "    # Plot 2: Training vs Validation Loss Comparison\n",
    "    ax2.plot(epochs_range, demo_train_losses, label='Training loss', alpha=0.7, color='#2ca02c', linewidth=2)\n",
    "    ax2.plot(epochs_range, demo_val_losses, label='Validation loss', alpha=0.9, color='#d62728', linewidth=2)\n",
    "    ax2.set_title('Enhanced Model - Loss Comparison', fontsize=14, fontweight='bold')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.legend()\n",
    "    ax2.grid(alpha=0.3)\n",
    "    \n",
    "    # Add text with best validation loss\n",
    "    best_val_loss = min(demo_val_losses)\n",
    "    best_val_loss_epoch = demo_val_losses.index(best_val_loss) + 1\n",
    "    ax2.text(0.02, 0.98, f'Best Val Loss: {best_val_loss:.4f} (Epoch {best_val_loss_epoch})', \n",
    "             transform=ax2.transAxes, verticalalignment='top', \n",
    "             bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.7))\n",
    "\n",
    "    # Plot 3: Validation F1 Score\n",
    "    ax3.plot(epochs_range, demo_val_f1s, label='Validation F1', alpha=0.9, color='#1f77b4', linewidth=2, marker='o', markersize=4)\n",
    "    ax3.set_title('Enhanced Model - F1 Score Over Time', fontsize=14, fontweight='bold')\n",
    "    ax3.set_xlabel('Epoch')\n",
    "    ax3.set_ylabel('F1 Score')\n",
    "    ax3.legend()\n",
    "    ax3.grid(alpha=0.3)\n",
    "    \n",
    "    # Add text with best F1 score\n",
    "    best_f1 = max(demo_val_f1s)\n",
    "    best_f1_epoch = demo_val_f1s.index(best_f1) + 1\n",
    "    ax3.text(0.02, 0.98, f'Best Val F1: {best_f1:.4f} (Epoch {best_f1_epoch})', \n",
    "             transform=ax3.transAxes, verticalalignment='top', \n",
    "             bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.7))\n",
    "\n",
    "    # Adjust the layout and display the plot\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(right=0.85)\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(f\"\\nüìä Enhanced Model Training Summary:\")\n",
    "    print(f\"   ‚Ä¢ Total Epochs: {len(demo_train_losses)}\")\n",
    "    print(f\"   ‚Ä¢ Best Validation F1: {best_f1:.4f} (Epoch {best_f1_epoch})\")\n",
    "    print(f\"   ‚Ä¢ Best Validation Loss: {best_val_loss:.4f} (Epoch {best_val_loss_epoch})\")\n",
    "    print(f\"   ‚Ä¢ Final Validation F1: {demo_val_f1s[-1]:.4f}\")\n",
    "    print(f\"   ‚Ä¢ Final Validation Loss: {demo_val_losses[-1]:.4f}\")\n",
    "    \n",
    "    # Compare with original model results (if available)\n",
    "    if 'best_val_f1' in globals():\n",
    "        improvement = best_f1 - best_val_f1\n",
    "        print(f\"\\nüî• Comparison with Original Model:\")\n",
    "        print(f\"   ‚Ä¢ Original Best F1: {best_val_f1:.4f}\")\n",
    "        print(f\"   ‚Ä¢ Enhanced Best F1: {best_f1:.4f}\")\n",
    "        print(f\"   ‚Ä¢ Improvement: {improvement:+.4f} ({improvement/best_val_f1*100:+.2f}%)\")\n",
    "        \n",
    "        if improvement > 0:\n",
    "            print(f\"   ‚úÖ Enhanced model performs BETTER!\")\n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è Original model still performs better. Consider hyperparameter tuning.\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Enhanced model training results not found.\")\n",
    "    print(\"Please run the enhanced model training first:\")\n",
    "    print(\"   1. Execute the enhanced model training cell\")\n",
    "    print(\"   2. Then run this plotting cell\")\n",
    "    print(\"\\nAvailable variables:\")\n",
    "    available_vars = [var for var in ['demo_train_losses', 'demo_val_losses', 'demo_val_f1s', 'demo_train_f1s'] if var in globals()]\n",
    "    print(f\"   ‚Ä¢ Found: {available_vars}\")\n",
    "    missing_vars = [var for var in ['demo_train_losses', 'demo_val_losses', 'demo_val_f1s', 'demo_train_f1s'] if var not in globals()]\n",
    "    print(f\"   ‚Ä¢ Missing: {missing_vars}\")\n",
    "    \n",
    "    # Special note about training F1 scores\n",
    "    if 'demo_train_f1s' not in globals():\n",
    "        print(f\"   ‚Ä¢ Note: Training F1 scores not available - will show training loss instead in first plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Enhanced Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Enhanced Model Saved Successfully!\n",
      "üìÅ File: models/enhanced_lstm_embeddings_F10.91_20251113_134602.pt\n",
      "üìä Performance: Best F1 = 0.9133 (Epoch 166)\n",
      "üîß Model: EnhancedRecurrentClassifier\n",
      "üìà Parameters: 66,479\n",
      "‚úÖ Verified: File exists (0.26 MB)\n",
      "\n",
      "üí° To load this model later, use:\n",
      "   checkpoint = torch.load('models/enhanced_lstm_embeddings_F10.91_20251113_134602.pt')\n",
      "   model.load_state_dict(checkpoint['model_state_dict'])\n"
     ]
    }
   ],
   "source": [
    "# Simple model saving code\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Create models directory if it doesn't exist\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "# Generate timestamp for unique naming\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Calculate performance metrics\n",
    "if 'demo_val_f1s' in globals() and len(demo_val_f1s) > 0:\n",
    "    best_f1 = max(demo_val_f1s)\n",
    "    best_epoch = demo_val_f1s.index(best_f1) + 1\n",
    "    final_f1 = demo_val_f1s[-1]\n",
    "else:\n",
    "    best_f1 = final_f1 = best_epoch = 0\n",
    "\n",
    "# Create filename with performance info\n",
    "model_name = f\"enhanced_lstm_embeddings_F1{best_f1:.2f}_{timestamp}\"\n",
    "model_path = f\"models/{model_name}.pt\"\n",
    "\n",
    "# Save the model\n",
    "torch.save({\n",
    "    'model_state_dict': demo_enhanced_model.state_dict(),\n",
    "    'model_class': 'EnhancedRecurrentClassifier',\n",
    "    'timestamp': timestamp,\n",
    "    'training_info': {\n",
    "        'best_val_f1': best_f1,\n",
    "        'best_epoch': best_epoch,\n",
    "        'final_val_f1': final_f1,\n",
    "        'total_epochs': len(demo_val_f1s) if 'demo_val_f1s' in globals() else 0,\n",
    "        'model_config': {\n",
    "            'hidden_size': HIDDEN_SIZE,\n",
    "            'num_layers': 2,\n",
    "            'rnn_type': 'LSTM',\n",
    "            'bidirectional': True,\n",
    "            'dropout_rate': 0.2,\n",
    "            'continuous_input_size': 17,\n",
    "            'categorical_features': ['pain_survey_1', 'pain_survey_2', 'pain_survey_3', 'pain_survey_4'],\n",
    "            'embedding_dims': {'pain_survey_1': 2, 'pain_survey_2': 2, 'pain_survey_3': 2, 'pain_survey_4': 2}\n",
    "        },\n",
    "        'regularization': {\n",
    "            'l1_lambda': L1_LAMBDA,\n",
    "            'l2_lambda': L2_LAMBDA,\n",
    "            'weight_decay': L2_LAMBDA\n",
    "        }\n",
    "    }\n",
    "}, model_path)\n",
    "\n",
    "print(\"üíæ Enhanced Model Saved Successfully!\")\n",
    "print(f\"üìÅ File: {model_path}\")\n",
    "print(f\"üìä Performance: Best F1 = {best_f1:.4f} (Epoch {best_epoch})\")\n",
    "print(f\"üîß Model: {demo_enhanced_model.__class__.__name__}\")\n",
    "print(f\"üìà Parameters: {sum(p.numel() for p in demo_enhanced_model.parameters()):,}\")\n",
    "\n",
    "# Verification - check if file was created\n",
    "if os.path.exists(model_path):\n",
    "    file_size = os.path.getsize(model_path) / (1024 * 1024)  # Convert to MB\n",
    "    print(f\"‚úÖ Verified: File exists ({file_size:.2f} MB)\")\n",
    "else:\n",
    "    print(\"‚ùå Error: File was not created!\")\n",
    "\n",
    "print(f\"\\nüí° To load this model later, use:\")\n",
    "print(f\"   checkpoint = torch.load('{model_path}')\")\n",
    "print(f\"   model.load_state_dict(checkpoint['model_state_dict'])\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HbaDexCwnJLG"
   },
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "qerAnh47nLLx"
   },
   "outputs": [],
   "source": [
    "# @title Plot Hitory\n",
    "# Create figure with two subplots sharing x axis\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(18, 5), sharex=True)\n",
    "\n",
    "# Color palette for K splits\n",
    "colors = plt.cm.get_cmap('tab10', K)\n",
    "\n",
    "# Plot validation loss for each split\n",
    "for split in range(K):\n",
    "    axes[0].plot(losses[f'split_{split}'][:-PATIENCE_KFOLD], label=f'Split {split+1}',\n",
    "                 color=colors(split), alpha=0.6)\n",
    "axes[0].set_title('Validation Loss per Split')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Plot validation F1 score for each split\n",
    "for split in range(K):\n",
    "    axes[1].plot(metrics[f'split_{split}'][:-PATIENCE_KFOLD], label=f'Split {split+1}',\n",
    "                 color=colors(split), alpha=0.6)\n",
    "axes[1].set_title('Validation F1 Score per Split')\n",
    "axes[1].set_ylabel('F1 Score')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "# Add shared legend on the right\n",
    "handles, labels = axes[0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='center left', bbox_to_anchor=(1, 0.5), fontsize='small')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(right=0.975)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zn0xjziNo8R5"
   },
   "source": [
    "## 14 Competition Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_Wi1u77Gtxfe",
    "outputId": "2b6505cf-09ba-4763-89c7-fdd2664abf26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building ENHANCED sequences for actual test dataset with WINDOW_SIZE=100, STRIDE=20\n",
      "Available columns: ['sample_index', 'time', 'pain_survey_1', 'pain_survey_2', 'pain_survey_3', 'pain_survey_4', 'joint_00', 'joint_01', 'joint_02', 'joint_03', 'joint_04', 'joint_05', 'joint_06', 'joint_07', 'joint_08', 'joint_09', 'joint_10', 'joint_11', 'joint_12', 'joint_26', 'joint_27', 'joint_28', 'joint_29']\n",
      "Categorical features: ['pain_survey_1', 'pain_survey_2', 'pain_survey_3', 'pain_survey_4']\n",
      "Continuous features: ['joint_00', 'joint_01', 'joint_02', 'joint_03', 'joint_04', 'joint_05', 'joint_06', 'joint_07', 'joint_08', 'joint_09', 'joint_10', 'joint_11', 'joint_12', 'joint_26', 'joint_27', 'joint_28', 'joint_29'] (count: 17)\n",
      "Enhanced test sequences shapes:\n",
      "  Continuous: (5296, 100, 17)\n",
      "  pain_survey_1: (5296, 100)\n",
      "  pain_survey_2: (5296, 100)\n",
      "  pain_survey_3: (5296, 100)\n",
      "  pain_survey_4: (5296, 100)\n",
      "  Sample indices: 5296\n",
      "\n",
      "‚úÖ Enhanced test dataset created successfully!\n",
      "üìä Dataset size: 5296 samples\n",
      "üîÑ Number of batches: 12\n",
      "üìê Continuous input size: 17\n",
      "üè∑Ô∏è Categorical features: ['pain_survey_1', 'pain_survey_2', 'pain_survey_3', 'pain_survey_4']\n",
      "\n",
      "üß™ Testing Enhanced DataLoader...\n",
      "‚úÖ Continuous batch shape: torch.Size([480, 100, 17])\n",
      "‚úÖ Labels batch shape: torch.Size([480])\n",
      "‚úÖ pain_survey_1 batch shape: torch.Size([480, 100])\n",
      "‚úÖ pain_survey_2 batch shape: torch.Size([480, 100])\n",
      "‚úÖ pain_survey_3 batch shape: torch.Size([480, 100])\n",
      "‚úÖ pain_survey_4 batch shape: torch.Size([480, 100])\n",
      "\n",
      "üéØ Enhanced test data is ready for the Enhanced model!\n",
      "Enhanced test sequences shapes:\n",
      "  Continuous: (5296, 100, 17)\n",
      "  pain_survey_1: (5296, 100)\n",
      "  pain_survey_2: (5296, 100)\n",
      "  pain_survey_3: (5296, 100)\n",
      "  pain_survey_4: (5296, 100)\n",
      "  Sample indices: 5296\n",
      "\n",
      "‚úÖ Enhanced test dataset created successfully!\n",
      "üìä Dataset size: 5296 samples\n",
      "üîÑ Number of batches: 12\n",
      "üìê Continuous input size: 17\n",
      "üè∑Ô∏è Categorical features: ['pain_survey_1', 'pain_survey_2', 'pain_survey_3', 'pain_survey_4']\n",
      "\n",
      "üß™ Testing Enhanced DataLoader...\n",
      "‚úÖ Continuous batch shape: torch.Size([480, 100, 17])\n",
      "‚úÖ Labels batch shape: torch.Size([480])\n",
      "‚úÖ pain_survey_1 batch shape: torch.Size([480, 100])\n",
      "‚úÖ pain_survey_2 batch shape: torch.Size([480, 100])\n",
      "‚úÖ pain_survey_3 batch shape: torch.Size([480, 100])\n",
      "‚úÖ pain_survey_4 batch shape: torch.Size([480, 100])\n",
      "\n",
      "üéØ Enhanced test data is ready for the Enhanced model!\n"
     ]
    }
   ],
   "source": [
    "# Build sequences from the actual test data for Enhanced Model\n",
    "print(f\"Building ENHANCED sequences for actual test dataset with WINDOW_SIZE={WINDOW_SIZE}, STRIDE={STRIDE}\")\n",
    "\n",
    "# Identify feature types from X_test_final_df columns\n",
    "all_columns = X_test_final_df.columns.tolist()\n",
    "print(f\"Available columns: {all_columns}\")\n",
    "\n",
    "# Define feature separation\n",
    "categorical_features = ['pain_survey_1', 'pain_survey_2', 'pain_survey_3', 'pain_survey_4']\n",
    "joint_features = [col for col in all_columns if col.startswith('joint_')]\n",
    "continuous_features = joint_features  # Continuous features are the joint features\n",
    "exclude_cols = ['sample_index'] + ([] if 'time' not in all_columns else ['time'])  # Exclude non-feature columns\n",
    "\n",
    "print(f\"Categorical features: {categorical_features}\")\n",
    "print(f\"Continuous features: {continuous_features} (count: {len(continuous_features)})\")\n",
    "\n",
    "# Create enhanced sequences function for test data\n",
    "def build_sequences_test_enhanced(df, window=WINDOW_SIZE, stride=STRIDE):\n",
    "    \"\"\"\n",
    "    Build sequences for test data separating continuous and categorical features\n",
    "    \"\"\"\n",
    "    continuous_dataset = []\n",
    "    categorical_datasets = {feature: [] for feature in categorical_features}\n",
    "    sample_indices = []\n",
    "    \n",
    "    # Get unique sample IDs\n",
    "    for sample_id in df['sample_index'].unique():\n",
    "        # Extract rows for this sample\n",
    "        sample_data = df[df['sample_index'] == sample_id].copy()\n",
    "        \n",
    "        # If sample has fewer rows than WINDOW_SIZE, pad with zeros\n",
    "        if len(sample_data) < window:\n",
    "            # Create padding dataframe\n",
    "            padding_rows = window - len(sample_data)\n",
    "            padding = pd.DataFrame(0, index=range(padding_rows), columns=sample_data.columns)\n",
    "            sample_data = pd.concat([sample_data, padding], ignore_index=True)\n",
    "        \n",
    "        # Extract continuous features (joints only)\n",
    "        continuous_data = sample_data[continuous_features].values\n",
    "        \n",
    "        # Build continuous sequences\n",
    "        continuous_seqs = []\n",
    "        for i in range(0, len(continuous_data) - window + 1, stride):\n",
    "            continuous_seqs.append(continuous_data[i:i + window])\n",
    "        \n",
    "        # If no sequences generated, take the last window\n",
    "        if len(continuous_seqs) == 0:\n",
    "            continuous_seqs = [continuous_data[-window:]]\n",
    "        \n",
    "        # Build categorical sequences\n",
    "        categorical_seqs = {feature: [] for feature in categorical_features}\n",
    "        for feature in categorical_features:\n",
    "            if feature in sample_data.columns:\n",
    "                cat_data = sample_data[feature].values\n",
    "                for i in range(0, len(cat_data) - window + 1, stride):\n",
    "                    categorical_seqs[feature].append(cat_data[i:i + window])\n",
    "                # If no sequences generated, take the last window\n",
    "                if len(categorical_seqs[feature]) == 0:\n",
    "                    categorical_seqs[feature] = [cat_data[-window:]]\n",
    "            else:\n",
    "                # If feature doesn't exist, create zero sequences\n",
    "                for _ in continuous_seqs:\n",
    "                    categorical_seqs[feature].append(np.zeros(window))\n",
    "        \n",
    "        # Store sequences (take first sequence for each sample)\n",
    "        continuous_dataset.extend(continuous_seqs)\n",
    "        for feature in categorical_features:\n",
    "            categorical_datasets[feature].extend(categorical_seqs[feature])\n",
    "        sample_indices.extend([sample_id] * len(continuous_seqs))\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    continuous_dataset = np.array(continuous_dataset, dtype='float32')\n",
    "    for feature in categorical_features:\n",
    "        categorical_datasets[feature] = np.array(categorical_datasets[feature], dtype='int64')\n",
    "    \n",
    "    return continuous_dataset, categorical_datasets, sample_indices\n",
    "\n",
    "# Build enhanced sequences\n",
    "X_test_continuous, X_test_categorical, test_sample_indices = build_sequences_test_enhanced(X_test_final_df)\n",
    "\n",
    "# Handle NaN values\n",
    "if np.isnan(X_test_continuous).any():\n",
    "    X_test_continuous = np.nan_to_num(X_test_continuous)\n",
    "    print(\"NaN values found and replaced with 0 in continuous test sequences.\")\n",
    "\n",
    "for feature in categorical_features:\n",
    "    if np.isnan(X_test_categorical[feature]).any():\n",
    "        X_test_categorical[feature] = np.nan_to_num(X_test_categorical[feature]).astype('int64')\n",
    "        print(f\"NaN values found and replaced with 0 in {feature} test sequences.\")\n",
    "\n",
    "print(f\"Enhanced test sequences shapes:\")\n",
    "print(f\"  Continuous: {X_test_continuous.shape}\")\n",
    "for feature in categorical_features:\n",
    "    print(f\"  {feature}: {X_test_categorical[feature].shape}\")\n",
    "print(f\"  Sample indices: {len(test_sample_indices)}\")\n",
    "\n",
    "# Create dummy labels for test data (required by EnhancedDataset but not used)\n",
    "dummy_labels = np.zeros(len(test_sample_indices), dtype='int64')\n",
    "\n",
    "# Create Enhanced dataset\n",
    "test_enhanced_final_ds = EnhancedDataset(X_test_continuous, X_test_categorical, dummy_labels)\n",
    "\n",
    "# Create Enhanced DataLoader\n",
    "test_enhanced_final_loader = make_enhanced_loader(\n",
    "    test_enhanced_final_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    sampler=None\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Enhanced test dataset created successfully!\")\n",
    "print(f\"üìä Dataset size: {len(test_enhanced_final_ds)} samples\")\n",
    "print(f\"üîÑ Number of batches: {len(test_enhanced_final_loader)}\")\n",
    "print(f\"üìê Continuous input size: {X_test_continuous.shape[-1]}\")\n",
    "print(f\"üè∑Ô∏è Categorical features: {list(X_test_categorical.keys())}\")\n",
    "\n",
    "# Test the enhanced loader\n",
    "print(f\"\\nüß™ Testing Enhanced DataLoader...\")\n",
    "for continuous_batch, categorical_batch, labels_batch in test_enhanced_final_loader:\n",
    "    print(f\"‚úÖ Continuous batch shape: {continuous_batch.shape}\")\n",
    "    print(f\"‚úÖ Labels batch shape: {labels_batch.shape}\")\n",
    "    for feature, data in categorical_batch.items():\n",
    "        print(f\"‚úÖ {feature} batch shape: {data.shape}\")\n",
    "    break\n",
    "\n",
    "print(f\"\\nüéØ Enhanced test data is ready for the Enhanced model!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "id": "Zhd7Pt3JwILk",
    "outputId": "860f454c-21e4-4547-90ba-7204cd564ffd"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_index</th>\n",
       "      <th>time</th>\n",
       "      <th>pain_survey_1</th>\n",
       "      <th>pain_survey_2</th>\n",
       "      <th>pain_survey_3</th>\n",
       "      <th>pain_survey_4</th>\n",
       "      <th>joint_00</th>\n",
       "      <th>joint_01</th>\n",
       "      <th>joint_02</th>\n",
       "      <th>joint_03</th>\n",
       "      <th>...</th>\n",
       "      <th>joint_07</th>\n",
       "      <th>joint_08</th>\n",
       "      <th>joint_09</th>\n",
       "      <th>joint_10</th>\n",
       "      <th>joint_11</th>\n",
       "      <th>joint_12</th>\n",
       "      <th>joint_26</th>\n",
       "      <th>joint_27</th>\n",
       "      <th>joint_28</th>\n",
       "      <th>joint_29</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.561563</td>\n",
       "      <td>0.553352</td>\n",
       "      <td>0.419037</td>\n",
       "      <td>0.270175</td>\n",
       "      <td>...</td>\n",
       "      <td>0.654303</td>\n",
       "      <td>0.737832</td>\n",
       "      <td>0.742275</td>\n",
       "      <td>0.100076</td>\n",
       "      <td>0.146564</td>\n",
       "      <td>0.745300</td>\n",
       "      <td>0.014909</td>\n",
       "      <td>0.045098</td>\n",
       "      <td>0.012882</td>\n",
       "      <td>0.010178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.599088</td>\n",
       "      <td>0.532067</td>\n",
       "      <td>0.461325</td>\n",
       "      <td>0.327922</td>\n",
       "      <td>...</td>\n",
       "      <td>0.684443</td>\n",
       "      <td>0.772454</td>\n",
       "      <td>0.710705</td>\n",
       "      <td>0.103457</td>\n",
       "      <td>0.174403</td>\n",
       "      <td>0.594262</td>\n",
       "      <td>0.053679</td>\n",
       "      <td>0.055375</td>\n",
       "      <td>0.013892</td>\n",
       "      <td>0.029085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.638365</td>\n",
       "      <td>0.583960</td>\n",
       "      <td>0.445804</td>\n",
       "      <td>0.308796</td>\n",
       "      <td>...</td>\n",
       "      <td>0.676488</td>\n",
       "      <td>0.799646</td>\n",
       "      <td>0.722061</td>\n",
       "      <td>0.143175</td>\n",
       "      <td>0.159973</td>\n",
       "      <td>0.652024</td>\n",
       "      <td>0.042305</td>\n",
       "      <td>0.039620</td>\n",
       "      <td>0.016286</td>\n",
       "      <td>0.040638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.554938</td>\n",
       "      <td>0.488719</td>\n",
       "      <td>0.443494</td>\n",
       "      <td>0.355023</td>\n",
       "      <td>...</td>\n",
       "      <td>0.650563</td>\n",
       "      <td>0.738087</td>\n",
       "      <td>0.709363</td>\n",
       "      <td>0.141007</td>\n",
       "      <td>0.167449</td>\n",
       "      <td>0.709558</td>\n",
       "      <td>0.037477</td>\n",
       "      <td>0.031101</td>\n",
       "      <td>0.008568</td>\n",
       "      <td>0.018730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.537192</td>\n",
       "      <td>0.528780</td>\n",
       "      <td>0.413159</td>\n",
       "      <td>0.363199</td>\n",
       "      <td>...</td>\n",
       "      <td>0.653239</td>\n",
       "      <td>0.703021</td>\n",
       "      <td>0.681513</td>\n",
       "      <td>0.140234</td>\n",
       "      <td>0.186249</td>\n",
       "      <td>0.590142</td>\n",
       "      <td>0.015210</td>\n",
       "      <td>0.019426</td>\n",
       "      <td>0.008189</td>\n",
       "      <td>0.013444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211835</th>\n",
       "      <td>1323.0</td>\n",
       "      <td>155.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.746791</td>\n",
       "      <td>0.715951</td>\n",
       "      <td>0.787217</td>\n",
       "      <td>0.908019</td>\n",
       "      <td>...</td>\n",
       "      <td>0.746858</td>\n",
       "      <td>0.646380</td>\n",
       "      <td>0.606524</td>\n",
       "      <td>0.361932</td>\n",
       "      <td>0.515863</td>\n",
       "      <td>0.762767</td>\n",
       "      <td>0.009774</td>\n",
       "      <td>0.031095</td>\n",
       "      <td>0.006292</td>\n",
       "      <td>0.070267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211836</th>\n",
       "      <td>1323.0</td>\n",
       "      <td>156.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.712191</td>\n",
       "      <td>0.805042</td>\n",
       "      <td>0.749162</td>\n",
       "      <td>0.829449</td>\n",
       "      <td>...</td>\n",
       "      <td>0.741321</td>\n",
       "      <td>0.576658</td>\n",
       "      <td>0.647361</td>\n",
       "      <td>0.389554</td>\n",
       "      <td>0.471334</td>\n",
       "      <td>0.723960</td>\n",
       "      <td>0.027009</td>\n",
       "      <td>0.049113</td>\n",
       "      <td>0.029173</td>\n",
       "      <td>0.068884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211837</th>\n",
       "      <td>1323.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.778327</td>\n",
       "      <td>0.727727</td>\n",
       "      <td>0.775497</td>\n",
       "      <td>0.841656</td>\n",
       "      <td>...</td>\n",
       "      <td>0.762795</td>\n",
       "      <td>0.678090</td>\n",
       "      <td>0.628388</td>\n",
       "      <td>0.353318</td>\n",
       "      <td>0.453431</td>\n",
       "      <td>0.739291</td>\n",
       "      <td>0.025910</td>\n",
       "      <td>0.042127</td>\n",
       "      <td>0.011971</td>\n",
       "      <td>0.079291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211838</th>\n",
       "      <td>1323.0</td>\n",
       "      <td>158.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.740327</td>\n",
       "      <td>0.768988</td>\n",
       "      <td>0.749230</td>\n",
       "      <td>0.854667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.694102</td>\n",
       "      <td>0.654149</td>\n",
       "      <td>0.611106</td>\n",
       "      <td>0.378361</td>\n",
       "      <td>0.497827</td>\n",
       "      <td>0.714944</td>\n",
       "      <td>0.052790</td>\n",
       "      <td>0.019468</td>\n",
       "      <td>0.015994</td>\n",
       "      <td>0.106760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211839</th>\n",
       "      <td>1323.0</td>\n",
       "      <td>159.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.708521</td>\n",
       "      <td>0.771210</td>\n",
       "      <td>0.785865</td>\n",
       "      <td>0.833756</td>\n",
       "      <td>...</td>\n",
       "      <td>0.710532</td>\n",
       "      <td>0.658793</td>\n",
       "      <td>0.608914</td>\n",
       "      <td>0.360460</td>\n",
       "      <td>0.440642</td>\n",
       "      <td>0.810988</td>\n",
       "      <td>0.033291</td>\n",
       "      <td>0.024055</td>\n",
       "      <td>0.047641</td>\n",
       "      <td>0.099013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>211840 rows √ó 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        sample_index   time  pain_survey_1  pain_survey_2  pain_survey_3  \\\n",
       "0                0.0    0.0            1.0            1.0            1.0   \n",
       "1                0.0    1.0            1.0            1.0            1.0   \n",
       "2                0.0    2.0            1.0            1.0            1.0   \n",
       "3                0.0    3.0            0.5            1.0            1.0   \n",
       "4                0.0    4.0            1.0            1.0            1.0   \n",
       "...              ...    ...            ...            ...            ...   \n",
       "211835        1323.0  155.0            1.0            1.0            0.5   \n",
       "211836        1323.0  156.0            0.5            1.0            1.0   \n",
       "211837        1323.0  157.0            0.5            1.0            1.0   \n",
       "211838        1323.0  158.0            1.0            1.0            0.5   \n",
       "211839        1323.0  159.0            0.5            0.0            1.0   \n",
       "\n",
       "        pain_survey_4  joint_00  joint_01  joint_02  joint_03  ...  joint_07  \\\n",
       "0                 1.0  0.561563  0.553352  0.419037  0.270175  ...  0.654303   \n",
       "1                 1.0  0.599088  0.532067  0.461325  0.327922  ...  0.684443   \n",
       "2                 1.0  0.638365  0.583960  0.445804  0.308796  ...  0.676488   \n",
       "3                 1.0  0.554938  0.488719  0.443494  0.355023  ...  0.650563   \n",
       "4                 0.0  0.537192  0.528780  0.413159  0.363199  ...  0.653239   \n",
       "...               ...       ...       ...       ...       ...  ...       ...   \n",
       "211835            1.0  0.746791  0.715951  0.787217  0.908019  ...  0.746858   \n",
       "211836            1.0  0.712191  0.805042  0.749162  0.829449  ...  0.741321   \n",
       "211837            1.0  0.778327  0.727727  0.775497  0.841656  ...  0.762795   \n",
       "211838            1.0  0.740327  0.768988  0.749230  0.854667  ...  0.694102   \n",
       "211839            0.0  0.708521  0.771210  0.785865  0.833756  ...  0.710532   \n",
       "\n",
       "        joint_08  joint_09  joint_10  joint_11  joint_12  joint_26  joint_27  \\\n",
       "0       0.737832  0.742275  0.100076  0.146564  0.745300  0.014909  0.045098   \n",
       "1       0.772454  0.710705  0.103457  0.174403  0.594262  0.053679  0.055375   \n",
       "2       0.799646  0.722061  0.143175  0.159973  0.652024  0.042305  0.039620   \n",
       "3       0.738087  0.709363  0.141007  0.167449  0.709558  0.037477  0.031101   \n",
       "4       0.703021  0.681513  0.140234  0.186249  0.590142  0.015210  0.019426   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "211835  0.646380  0.606524  0.361932  0.515863  0.762767  0.009774  0.031095   \n",
       "211836  0.576658  0.647361  0.389554  0.471334  0.723960  0.027009  0.049113   \n",
       "211837  0.678090  0.628388  0.353318  0.453431  0.739291  0.025910  0.042127   \n",
       "211838  0.654149  0.611106  0.378361  0.497827  0.714944  0.052790  0.019468   \n",
       "211839  0.658793  0.608914  0.360460  0.440642  0.810988  0.033291  0.024055   \n",
       "\n",
       "        joint_28  joint_29  \n",
       "0       0.012882  0.010178  \n",
       "1       0.013892  0.029085  \n",
       "2       0.016286  0.040638  \n",
       "3       0.008568  0.018730  \n",
       "4       0.008189  0.013444  \n",
       "...          ...       ...  \n",
       "211835  0.006292  0.070267  \n",
       "211836  0.029173  0.068884  \n",
       "211837  0.011971  0.079291  \n",
       "211838  0.015994  0.106760  \n",
       "211839  0.047641  0.099013  \n",
       "\n",
       "[211840 rows x 23 columns]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show test_final\n",
    "X_test_final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # FIXED Enhanced Model Competition Submission\n",
    "# print(\"üîß FIXING ENHANCED MODEL SUBMISSION\")\n",
    "# print(\"=\" * 50)\n",
    "\n",
    "# # The issue is that we're using the competition_enhanced_loader which might not preserve sample indices\n",
    "# # Let's create the submission manually using the competition_enhanced_ds dataset\n",
    "\n",
    "# print(\"üéØ Creating submission with correct sample indices...\")\n",
    "\n",
    "# demo_enhanced_model.eval()\n",
    "\n",
    "# # Create submission data with proper indices\n",
    "# fixed_submission_data = []\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for i in range(len(competition_enhanced_ds)):\n",
    "#         # Get individual sample\n",
    "#         continuous_sample, categorical_sample, sample_idx = competition_enhanced_ds[i]\n",
    "        \n",
    "#         # Add batch dimension and move to device\n",
    "#         continuous_batch = continuous_sample.unsqueeze(0).to(device)\n",
    "#         categorical_batch = {k: v.unsqueeze(0).to(device) for k, v in categorical_sample.items()}\n",
    "        \n",
    "#         # Get prediction\n",
    "#         logits = demo_enhanced_model(continuous_batch, categorical_batch)\n",
    "#         pred = logits.argmax(dim=1).item()\n",
    "        \n",
    "#         # Convert to label\n",
    "#         label_map = {0: \"no_pain\", 1: \"low_pain\", 2: \"high_pain\"}\n",
    "#         pred_label = label_map[pred]\n",
    "        \n",
    "#         # Add to submission data with correct index formatting\n",
    "#         fixed_submission_data.append({\n",
    "#             \"sample_index\": f\"{i:03d}\",  # Format as 000, 001, 002, ..., 1323\n",
    "#             \"label\": pred_label\n",
    "#         })\n",
    "        \n",
    "#         # Progress indicator\n",
    "#         if (i + 1) % 200 == 0 or (i + 1) == len(competition_enhanced_ds):\n",
    "#             print(f\"Processed {i + 1}/{len(competition_enhanced_ds)} samples\")\n",
    "\n",
    "# # Create DataFrame\n",
    "# fixed_submission_df = pd.DataFrame(fixed_submission_data)\n",
    "\n",
    "# # Save corrected submission\n",
    "# fixed_filename = f\"enhanced_submission_FIXED_early_stopped_{current_datetime}.csv\"\n",
    "# fixed_submission_df.to_csv(fixed_filename, index=False)\n",
    "\n",
    "# print(f\"\\n‚úÖ FIXED submission saved: {fixed_filename}\")\n",
    "# print(f\"üìä Shape: {fixed_submission_df.shape}\")\n",
    "\n",
    "# # Verify the fix\n",
    "# sample_idx_counts = fixed_submission_df['sample_index'].value_counts()\n",
    "# print(f\"\\nüîç Verification:\")\n",
    "# print(f\"   Unique sample indices: {len(sample_idx_counts)}\")\n",
    "# print(f\"   Expected: 1324\")\n",
    "# print(f\"   Index range: {fixed_submission_df['sample_index'].min()} to {fixed_submission_df['sample_index'].max()}\")\n",
    "\n",
    "# if len(sample_idx_counts) == 1324:\n",
    "#     print(\"   ‚úÖ Sample indexing FIXED!\")\n",
    "# else:\n",
    "#     print(\"   ‚ùå Sample indexing still has issues\")\n",
    "\n",
    "# # Show label distribution\n",
    "# label_dist = fixed_submission_df['label'].value_counts()\n",
    "# label_pct = fixed_submission_df['label'].value_counts(normalize=True) * 100\n",
    "\n",
    "# print(f\"\\nüìä FIXED Label Distribution:\")\n",
    "# for label in ['no_pain', 'low_pain', 'high_pain']:\n",
    "#     count = label_dist.get(label, 0)\n",
    "#     pct = label_pct.get(label, 0)\n",
    "#     print(f\"   {label:>10}: {count:4d} ({pct:5.1f}%)\")\n",
    "\n",
    "# print(f\"\\nüéØ SUBMISSION STATUS:\")\n",
    "# if len(sample_idx_counts) == 1324:\n",
    "#     print(\"   ‚úÖ Ready for competition submission!\")\n",
    "#     print(f\"   üìÅ File: {fixed_filename}\")\n",
    "    \n",
    "#     high_pain_pct = label_pct.get('high_pain', 0)\n",
    "#     if high_pain_pct > 50:\n",
    "#         print(f\"   ‚ö†Ô∏è  Note: High 'high_pain' predictions ({high_pain_pct:.1f}%)\")\n",
    "#         print(\"   This suggests the model may still be overfitting\")\n",
    "#         print(\"   Expected test improvement: 76-82% (from previous 72.75%)\")\n",
    "# else:\n",
    "#     print(\"   ‚ùå Still has indexing issues - do not submit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Enhanced Model Summary & Recommendations\n",
    "\n",
    "### ‚úÖ **What We Fixed:**\n",
    "1. **Early Stopping Implementation** - Prevented overfitting by stopping at best validation epoch\n",
    "2. **Enhanced Model Architecture** - Added embedding layers for categorical features \n",
    "3. **Sample Index Bug** - Fixed submission format to have unique sample indices\n",
    "4. **Model Inference** - Corrected to use both continuous and categorical inputs\n",
    "\n",
    "### ‚ö†Ô∏è **Remaining Concerns:**\n",
    "\n",
    "#### **Label Distribution Imbalance:**\n",
    "- Training: 77.3% no_pain, 14.3% low_pain, 8.4% high_pain  \n",
    "- Predictions: 27.1% no_pain, 19.6% low_pain, 53.2% high_pain\n",
    "- **Issue:** Model heavily predicts \"high_pain\" (53.2% vs 8.4% in training)\n",
    "\n",
    "#### **Expected Performance:**\n",
    "- **Previous Test Accuracy:** 72.75%\n",
    "- **Early Stopped Model Expected:** 76-82%\n",
    "- **With Current Imbalance:** Might be lower due to class distribution mismatch\n",
    "\n",
    "### üöÄ **Next Actions:**\n",
    "\n",
    "#### **Option 1: Submit Current Model (Recommended)**\n",
    "- File: `enhanced_submission_FIXED_early_stopped_13-11-11-18.csv`\n",
    "- **Pros:** Uses early stopping, should generalize better than previous overfitted model\n",
    "- **Expected:** 3-8% improvement over 72.75%\n",
    "\n",
    "#### **Option 2: Address Class Imbalance (If Time Permits)**\n",
    "- Retrain with balanced sampling\n",
    "- Adjust loss function class weights\n",
    "- Use threshold tuning for predictions\n",
    "\n",
    "### üìä **Model Comparison:**\n",
    "```\n",
    "Original Model (Overfitted):\n",
    "- Validation F1: 91.34% \n",
    "- Test Accuracy: 72.75%\n",
    "- Gap: 18.59% (severe overfitting)\n",
    "\n",
    "Enhanced Model (Early Stopped):\n",
    "- Validation F1: ~90% (estimated at best epoch)\n",
    "- Expected Test: 76-82%\n",
    "- Gap: 8-14% (improved generalization)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fc966286"
   },
   "source": [
    "### 15. Save Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5578b7fe",
    "outputId": "1e780b87-66d1-425f-e6bb-ef8765722691"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model configuration saved to 'models/LSTM_bi_12-11-22-18_config.json'\n",
      "\n",
      "--- Saved Model Configuration ---\n",
      "{\n",
      "    \"EXPERIMENT_NAME\": \"LSTM_bi_12-11-22-18\",\n",
      "    \"RNN_TYPE\": \"LSTM\",\n",
      "    \"BIDIRECTIONAL\": true,\n",
      "    \"input_size\": 21,\n",
      "    \"num_classes\": 3,\n",
      "    \"HIDDEN_SIZE\": 42,\n",
      "    \"HIDDEN_LAYERS\": 2,\n",
      "    \"DROPOUT_RATE\": 0.3,\n",
      "    \"LEARNING_RATE\": 0.001,\n",
      "    \"EPOCHS\": 500,\n",
      "    \"PATIENCE\": 50,\n",
      "    \"L1_LAMBDA\": 1e-06,\n",
      "    \"L2_LAMBDA\": 0.001,\n",
      "    \"BATCH_SIZE\": 512,\n",
      "    \"WINDOW_SIZE\": 68,\n",
      "    \"STRIDE\": 17,\n",
      "    \"SEED\": 42,\n",
      "    \"one_pirate_window\": true\n",
      "}\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Gather all relevant model and training parameters\n",
    "model_config = {\n",
    "    'EXPERIMENT_NAME': EXPERIMENT_NAME,\n",
    "    'RNN_TYPE': RNN_TYPE,\n",
    "    'BIDIRECTIONAL': BIDIRECTIONAL,\n",
    "    'input_size': input_shape[-1],\n",
    "    'num_classes': num_classes,\n",
    "    'HIDDEN_SIZE': HIDDEN_SIZE,\n",
    "    'HIDDEN_LAYERS': HIDDEN_LAYERS,\n",
    "    'DROPOUT_RATE': DROPOUT_RATE,\n",
    "    'LEARNING_RATE': LEARNING_RATE,\n",
    "    'EPOCHS': EPOCHS,\n",
    "    'PATIENCE': PATIENCE,\n",
    "    'L1_LAMBDA': L1_LAMBDA,\n",
    "    'L2_LAMBDA': L2_LAMBDA,\n",
    "    'BATCH_SIZE': BATCH_SIZE,\n",
    "    'WINDOW_SIZE': WINDOW_SIZE,\n",
    "    'STRIDE': STRIDE,\n",
    "    'SEED': SEED,\n",
    "    'one_pirate_window': one_pirate_window,\n",
    "    # 'TEST_F1': f'{test_f1:.4f}'\n",
    "}\n",
    "\n",
    "# Define the path to save the config file\n",
    "config_filepath = os.path.join(models_dir, f\"{EXPERIMENT_NAME}_config.json\")\n",
    "\n",
    "# Save the dictionary as a JSON file\n",
    "with open(config_filepath, 'w') as f:\n",
    "    json.dump(model_config, f, indent=4)\n",
    "\n",
    "print(f\"‚úÖ Model configuration saved to '{config_filepath}'\")\n",
    "\n",
    "# Display the saved configuration\n",
    "print(\"\\n--- Saved Model Configuration ---\")\n",
    "print(json.dumps(model_config, indent=4))\n",
    "print(\"-----------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 381
    },
    "id": "m2xyVfs7YAYH",
    "outputId": "5c0fe176-6d0f-4795-a6ee-b4332274dd85"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "dataframe",
       "variable_name": "df_train"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-810019b4-1f66-450a-a51d-9755a2a3bd6a\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_index</th>\n",
       "      <th>time</th>\n",
       "      <th>pain_survey_1</th>\n",
       "      <th>pain_survey_2</th>\n",
       "      <th>pain_survey_3</th>\n",
       "      <th>pain_survey_4</th>\n",
       "      <th>joint_00</th>\n",
       "      <th>joint_01</th>\n",
       "      <th>joint_02</th>\n",
       "      <th>joint_03</th>\n",
       "      <th>...</th>\n",
       "      <th>joint_08</th>\n",
       "      <th>joint_09</th>\n",
       "      <th>joint_10</th>\n",
       "      <th>joint_11</th>\n",
       "      <th>joint_12</th>\n",
       "      <th>joint_26</th>\n",
       "      <th>joint_27</th>\n",
       "      <th>joint_28</th>\n",
       "      <th>joint_29</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.777046</td>\n",
       "      <td>0.738252</td>\n",
       "      <td>0.779512</td>\n",
       "      <td>0.804419</td>\n",
       "      <td>...</td>\n",
       "      <td>0.478382</td>\n",
       "      <td>0.753815</td>\n",
       "      <td>0.272106</td>\n",
       "      <td>0.269510</td>\n",
       "      <td>0.762947</td>\n",
       "      <td>0.014214</td>\n",
       "      <td>0.011376</td>\n",
       "      <td>0.018978</td>\n",
       "      <td>0.020291</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.805855</td>\n",
       "      <td>0.765147</td>\n",
       "      <td>0.761153</td>\n",
       "      <td>0.838021</td>\n",
       "      <td>...</td>\n",
       "      <td>0.486231</td>\n",
       "      <td>0.761224</td>\n",
       "      <td>0.217448</td>\n",
       "      <td>0.245846</td>\n",
       "      <td>0.727910</td>\n",
       "      <td>0.010748</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009473</td>\n",
       "      <td>0.010006</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.767110</td>\n",
       "      <td>0.721439</td>\n",
       "      <td>0.772834</td>\n",
       "      <td>0.777832</td>\n",
       "      <td>...</td>\n",
       "      <td>0.441994</td>\n",
       "      <td>0.725601</td>\n",
       "      <td>0.207995</td>\n",
       "      <td>0.258133</td>\n",
       "      <td>0.760757</td>\n",
       "      <td>0.013097</td>\n",
       "      <td>0.006830</td>\n",
       "      <td>0.017065</td>\n",
       "      <td>0.016856</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.665528</td>\n",
       "      <td>0.810416</td>\n",
       "      <td>0.763971</td>\n",
       "      <td>0.785928</td>\n",
       "      <td>...</td>\n",
       "      <td>0.469554</td>\n",
       "      <td>0.751688</td>\n",
       "      <td>0.238584</td>\n",
       "      <td>0.250324</td>\n",
       "      <td>0.767434</td>\n",
       "      <td>0.009505</td>\n",
       "      <td>0.006274</td>\n",
       "      <td>0.020264</td>\n",
       "      <td>0.017981</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.773829</td>\n",
       "      <td>0.773366</td>\n",
       "      <td>0.772162</td>\n",
       "      <td>0.767017</td>\n",
       "      <td>...</td>\n",
       "      <td>0.477740</td>\n",
       "      <td>0.749873</td>\n",
       "      <td>0.221475</td>\n",
       "      <td>0.290464</td>\n",
       "      <td>0.772967</td>\n",
       "      <td>0.004216</td>\n",
       "      <td>0.002132</td>\n",
       "      <td>0.023389</td>\n",
       "      <td>0.018477</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.813575</td>\n",
       "      <td>0.765834</td>\n",
       "      <td>0.746716</td>\n",
       "      <td>0.772337</td>\n",
       "      <td>...</td>\n",
       "      <td>0.424332</td>\n",
       "      <td>0.758200</td>\n",
       "      <td>0.283672</td>\n",
       "      <td>0.281967</td>\n",
       "      <td>0.803481</td>\n",
       "      <td>0.004861</td>\n",
       "      <td>0.005427</td>\n",
       "      <td>0.023442</td>\n",
       "      <td>0.017338</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.728054</td>\n",
       "      <td>0.809135</td>\n",
       "      <td>0.705452</td>\n",
       "      <td>0.698926</td>\n",
       "      <td>...</td>\n",
       "      <td>0.496126</td>\n",
       "      <td>0.748269</td>\n",
       "      <td>0.216302</td>\n",
       "      <td>0.288977</td>\n",
       "      <td>0.741954</td>\n",
       "      <td>0.005143</td>\n",
       "      <td>0.005407</td>\n",
       "      <td>0.022523</td>\n",
       "      <td>0.013901</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.737113</td>\n",
       "      <td>0.778694</td>\n",
       "      <td>0.806507</td>\n",
       "      <td>0.799577</td>\n",
       "      <td>...</td>\n",
       "      <td>0.416611</td>\n",
       "      <td>0.714467</td>\n",
       "      <td>0.231869</td>\n",
       "      <td>0.281431</td>\n",
       "      <td>0.804961</td>\n",
       "      <td>0.012911</td>\n",
       "      <td>0.004546</td>\n",
       "      <td>0.025178</td>\n",
       "      <td>0.011477</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.698434</td>\n",
       "      <td>0.753477</td>\n",
       "      <td>0.739042</td>\n",
       "      <td>0.831681</td>\n",
       "      <td>...</td>\n",
       "      <td>0.496911</td>\n",
       "      <td>0.707258</td>\n",
       "      <td>0.222234</td>\n",
       "      <td>0.291893</td>\n",
       "      <td>0.770717</td>\n",
       "      <td>0.016622</td>\n",
       "      <td>0.007172</td>\n",
       "      <td>0.006115</td>\n",
       "      <td>0.011130</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9 rows √ó 24 columns</p>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-810019b4-1f66-450a-a51d-9755a2a3bd6a')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-810019b4-1f66-450a-a51d-9755a2a3bd6a button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-810019b4-1f66-450a-a51d-9755a2a3bd6a');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    <div id=\"df-af551b3c-0e36-4f0a-a55c-465ccd7e6ffc\">\n",
       "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-af551b3c-0e36-4f0a-a55c-465ccd7e6ffc')\"\n",
       "                title=\"Suggest charts\"\n",
       "                style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "      </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "      <script>\n",
       "        async function quickchart(key) {\n",
       "          const quickchartButtonEl =\n",
       "            document.querySelector('#' + key + ' button');\n",
       "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "          try {\n",
       "            const charts = await google.colab.kernel.invokeFunction(\n",
       "                'suggestCharts', [key], {});\n",
       "          } catch (error) {\n",
       "            console.error('Error during call to suggestCharts:', error);\n",
       "          }\n",
       "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "        }\n",
       "        (() => {\n",
       "          let quickchartButtonEl =\n",
       "            document.querySelector('#df-af551b3c-0e36-4f0a-a55c-465ccd7e6ffc button');\n",
       "          quickchartButtonEl.style.display =\n",
       "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "        })();\n",
       "      </script>\n",
       "    </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "   sample_index  time  pain_survey_1  pain_survey_2  pain_survey_3  \\\n",
       "0             0     0            1.0            0.0            1.0   \n",
       "1             0     1            1.0            1.0            1.0   \n",
       "2             0     2            1.0            0.0            1.0   \n",
       "3             0     3            1.0            1.0            1.0   \n",
       "4             0     4            1.0            1.0            1.0   \n",
       "5             0     5            1.0            0.0            1.0   \n",
       "6             0     6            1.0            0.5            1.0   \n",
       "7             0     7            1.0            1.0            1.0   \n",
       "8             0     8            1.0            1.0            0.0   \n",
       "\n",
       "   pain_survey_4  joint_00  joint_01  joint_02  joint_03  ...  joint_08  \\\n",
       "0            0.5  0.777046  0.738252  0.779512  0.804419  ...  0.478382   \n",
       "1            1.0  0.805855  0.765147  0.761153  0.838021  ...  0.486231   \n",
       "2            1.0  0.767110  0.721439  0.772834  0.777832  ...  0.441994   \n",
       "3            1.0  0.665528  0.810416  0.763971  0.785928  ...  0.469554   \n",
       "4            1.0  0.773829  0.773366  0.772162  0.767017  ...  0.477740   \n",
       "5            0.5  0.813575  0.765834  0.746716  0.772337  ...  0.424332   \n",
       "6            0.5  0.728054  0.809135  0.705452  0.698926  ...  0.496126   \n",
       "7            1.0  0.737113  0.778694  0.806507  0.799577  ...  0.416611   \n",
       "8            0.5  0.698434  0.753477  0.739042  0.831681  ...  0.496911   \n",
       "\n",
       "   joint_09  joint_10  joint_11  joint_12  joint_26  joint_27  joint_28  \\\n",
       "0  0.753815  0.272106  0.269510  0.762947  0.014214  0.011376  0.018978   \n",
       "1  0.761224  0.217448  0.245846  0.727910  0.010748  0.000000  0.009473   \n",
       "2  0.725601  0.207995  0.258133  0.760757  0.013097  0.006830  0.017065   \n",
       "3  0.751688  0.238584  0.250324  0.767434  0.009505  0.006274  0.020264   \n",
       "4  0.749873  0.221475  0.290464  0.772967  0.004216  0.002132  0.023389   \n",
       "5  0.758200  0.283672  0.281967  0.803481  0.004861  0.005427  0.023442   \n",
       "6  0.748269  0.216302  0.288977  0.741954  0.005143  0.005407  0.022523   \n",
       "7  0.714467  0.231869  0.281431  0.804961  0.012911  0.004546  0.025178   \n",
       "8  0.707258  0.222234  0.291893  0.770717  0.016622  0.007172  0.006115   \n",
       "\n",
       "   joint_29  label  \n",
       "0  0.020291      0  \n",
       "1  0.010006      0  \n",
       "2  0.016856      0  \n",
       "3  0.017981      0  \n",
       "4  0.018477      0  \n",
       "5  0.017338      0  \n",
       "6  0.013901      0  \n",
       "7  0.011477      0  \n",
       "8  0.011130      0  \n",
       "\n",
       "[9 rows x 24 columns]"
      ]
     },
     "execution_count": 922,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhanced Model Submission Section\n",
    "Modified version of the submission code to use the enhanced model with embeddings instead of the original RNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Model input size derived from enhanced features: {enhanced_input_size}\")\n",
    "\n",
    "# # Create enhanced model instance for submission (using the trained demo_enhanced_model)\n",
    "# enhanced_model_submission = demo_enhanced_model  # Use the already trained enhanced model\n",
    "\n",
    "# print(f\"Enhanced Model created with input_size={enhanced_input_size}, hidden_size={HIDDEN_SIZE}\")\n",
    "# # Model is already trained and loaded, so we just set to evaluation mode\n",
    "# enhanced_model_submission.eval()\n",
    "\n",
    "# print(\"Starting inference on actual test set using Enhanced Model...\")\n",
    "\n",
    "# # --- Enhanced Model Inference Pipeline ---\n",
    "# final_test_preds = []\n",
    "# final_test_probabilities = []\n",
    "# sample_indices = []\n",
    "\n",
    "# print(f\"Running enhanced inference on {len(test_enhanced_loader)} batches...\")\n",
    "\n",
    "# with torch.no_grad():  # Disable gradient computation for inference\n",
    "#     for batch_idx, (continuous_batch, categorical_batch, _) in enumerate(test_enhanced_loader):\n",
    "#         # Move batches to device\n",
    "#         continuous_batch = continuous_batch.to(device)\n",
    "#         for key in categorical_batch:\n",
    "#             categorical_batch[key] = categorical_batch[key].to(device)\n",
    "\n",
    "#         # Verify batch dimensions\n",
    "#         if batch_idx == 0:\n",
    "#             print(f\"Continuous batch shape: {continuous_batch.shape}\")\n",
    "#             print(f\"Categorical batch keys: {list(categorical_batch.keys())}\")\n",
    "\n",
    "#         # Get enhanced model predictions\n",
    "#         logits = enhanced_model_submission(continuous_batch, categorical_batch)\n",
    "#         preds = logits.argmax(dim=1).cpu().numpy()\n",
    "\n",
    "#         # Get probabilities for confidence analysis\n",
    "#         probabilities = torch.softmax(logits, dim=1).cpu().numpy()\n",
    "\n",
    "#         # Store batch results\n",
    "#         final_test_preds.append(preds)\n",
    "#         final_test_probabilities.append(probabilities)\n",
    "\n",
    "#         # Progress indicator\n",
    "#         if (batch_idx + 1) % 10 == 0 or (batch_idx + 1) == len(test_enhanced_loader):\n",
    "#             print(f\"Processed batch {batch_idx + 1}/{len(test_enhanced_loader)}\")\n",
    "\n",
    "\n",
    "# # Combine all batches into single arrays\n",
    "# final_test_preds = np.concatenate(final_test_preds)\n",
    "# final_test_probabilities = np.concatenate(final_test_probabilities)\n",
    "\n",
    "# print(f\"\\nInference on actual test set completed successfully!\")\n",
    "# print(f\"Total predictions: {len(final_test_preds)}\")\n",
    "# print(f\"Predictions shape: {final_test_preds.shape}\")\n",
    "# print(f\"Probabilities shape: {final_test_probabilities.shape}\")\n",
    "\n",
    "\n",
    "# # --- Create Enhanced Model Submission File ---\n",
    "# # Map numerical predictions back to original labels\n",
    "# label_map = {0: \"no_pain\", 1: \"low_pain\", 2: \"high_pain\"}\n",
    "# pred_labels = [label_map[p] for p in final_test_preds]\n",
    "\n",
    "# final_results = [] # Initialize final_results list\n",
    "\n",
    "# for sample_id in X_test_final_df['sample_index'].unique():\n",
    "#     # extract rows for this sample\n",
    "#     temp = X_test_final_df[X_test_final_df['sample_index'] == sample_id]\n",
    "\n",
    "#     # If the sample has fewer rows than WINDOW_SIZE, pad it with zeros\n",
    "#     if len(temp) < WINDOW_SIZE:\n",
    "#         padding = pd.DataFrame(0, index=np.arange(WINDOW_SIZE - len(temp)), columns=temp.columns)\n",
    "#         temp = pd.concat([temp, padding], ignore_index=True)\n",
    "\n",
    "#     # For enhanced model, we need to separate continuous and categorical features\n",
    "#     continuous_cols = [col for col in temp.columns if col not in pain_survey_cols and col != 'sample_index']\n",
    "#     temp_cont = temp[continuous_cols].values\n",
    "#     temp_cat = {}\n",
    "#     for col in pain_survey_cols:\n",
    "#         if col in temp.columns:\n",
    "#             temp_cat[col] = temp[col].values\n",
    "\n",
    "#     # build sequences for this sample (continuous part)\n",
    "#     seqs_cont = build_sequences_test(temp_cont, window=WINDOW_SIZE, stride=STRIDE)\n",
    "\n",
    "#     # build sequences for categorical features\n",
    "#     seqs_cat = {}\n",
    "#     for col in pain_survey_cols:\n",
    "#         if col in temp_cat:\n",
    "#             seqs_cat[col] = build_sequences_test(temp_cat[col].reshape(-1, 1), window=WINDOW_SIZE, stride=STRIDE).squeeze(-1)\n",
    "\n",
    "#     # sometimes build_sequences_test might still return zero sequences if len(temp) < stride\n",
    "#     if len(seqs_cont) == 0:\n",
    "#         # fallback: take the last WINDOW_SIZE rows\n",
    "#         seqs_cont = temp_cont[-WINDOW_SIZE:][np.newaxis, :, :]\n",
    "#         for col in pain_survey_cols:\n",
    "#             if col in temp_cat:\n",
    "#                 seqs_cat[col] = temp_cat[col][-WINDOW_SIZE:][np.newaxis, :]\n",
    "\n",
    "#     # Convert to tensors\n",
    "#     seqs_cont = torch.tensor(seqs_cont, dtype=torch.float32).to(device)\n",
    "#     for col in seqs_cat:\n",
    "#         seqs_cat[col] = torch.tensor(seqs_cat[col], dtype=torch.long).to(device)\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         logits = enhanced_model_submission(seqs_cont, seqs_cat)\n",
    "#         probs = torch.softmax(logits, dim=1).cpu().numpy()\n",
    "#         preds = np.argmax(probs, axis=1)\n",
    "\n",
    "#     # Final decision for this sample_index\n",
    "#     final_class = np.bincount(preds).argmax()  # majority vote\n",
    "#     final_results.append({\n",
    "#         \"sample_index\": sample_id,\n",
    "#         \"prediction\": label_map[final_class]\n",
    "#     })\n",
    "\n",
    "# # Create enhanced submission filename\n",
    "# enhanced_submission_filename = f\"enhanced_{SUBMISSION_FILENAME}\"\n",
    "# submission = pd.DataFrame(final_results)\n",
    "# submission.to_csv(enhanced_submission_filename, index=False)\n",
    "\n",
    "# print(submission.head())\n",
    "# print(f\"‚úÖ Saved enhanced submission with {len(submission)} rows should be 1324\")\n",
    "\n",
    "# # --- Analyze class distribution in final predictions ---\n",
    "# label_counts = submission['prediction'].value_counts(normalize=True) * 100\n",
    "\n",
    "# print(\"\\nüìä Distribution of predicted classes (Enhanced Model):\")\n",
    "# for label in ['no_pain', 'low_pain', 'high_pain']:\n",
    "#     pct = label_counts.get(label, 0.0)\n",
    "#     print(f\"   {label:10s}: {pct:6.2f}%\")\n",
    "\n",
    "# # Optional: quick sanity check for imbalance\n",
    "# majority_label = label_counts.idxmax()\n",
    "# print(f\"\\nüß≠ Most common predicted label: {majority_label} ({label_counts.max():.2f}%) \\n\")\n",
    "\n",
    "# # Optional: visualize as a bar chart\n",
    "# import matplotlib.pyplot as plt\n",
    "# desired_order = [\"no_pain\", \"low_pain\", \"high_pain\"]\n",
    "# label_counts_ordered = label_counts.reindex(desired_order)\n",
    "# plt.figure(figsize=(5,3))\n",
    "# (label_counts_ordered\n",
    "#  .plot(kind='bar', rot=0, title='Enhanced Model Predicted Class Distribution', ylabel='Percentage (%)'))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhanced Model Submission - Using Existing Enhanced Data Pipeline\n",
    "\n",
    "Instead of reprocessing the test data, we'll use the already prepared `test_enhanced_loader` that has the correct data format for the enhanced model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # === Enhanced Model Test Data Processing ===\n",
    "# # Note: We use the existing test_enhanced_loader that was created with proper preprocessing\n",
    "# # This loader already handles:\n",
    "# # 1. Categorical features (pain_survey_1, pain_survey_2, pain_survey_3, pain_survey_4) \n",
    "# # 2. Continuous features (joint features)\n",
    "# # 3. Proper normalization and sequence building\n",
    "\n",
    "# print(\"üîç Enhanced Model Test Data Info:\")\n",
    "# print(f\"üìä Enhanced input size: {enhanced_input_size}\")\n",
    "# print(f\"üîÑ Test enhanced loader batches: {len(test_enhanced_loader)}\")\n",
    "# print(f\"üìã Categorical features: {pain_survey_cols}\")\n",
    "\n",
    "# # Verify the enhanced loader structure\n",
    "# sample_batch = next(iter(test_enhanced_loader))\n",
    "# continuous_batch, categorical_batch, _ = sample_batch\n",
    "\n",
    "# print(f\"\\nüìê Batch Shapes:\")\n",
    "# print(f\"   Continuous: {continuous_batch.shape}\")\n",
    "# print(f\"   Categorical keys: {list(categorical_batch.keys())}\")\n",
    "# for key, tensor in categorical_batch.items():\n",
    "#     print(f\"   {key}: {tensor.shape}\")\n",
    "\n",
    "# # Set model to evaluation mode\n",
    "# demo_enhanced_model.eval()\n",
    "\n",
    "# print(f\"\\nüöÄ Model ready for inference using enhanced data pipeline!\")\n",
    "# print(f\"üéØ Will generate submission file: enhanced_{SUBMISSION_FILENAME}\")\n",
    "\n",
    "# # === Load Enhanced Model Weights ===\n",
    "# # Check if we need to load saved model weights\n",
    "# try:\n",
    "#     # You can specify your enhanced model path here if you have a saved model\n",
    "#     # model_path = \"models/your_enhanced_model.pt\"  \n",
    "#     # checkpoint = torch.load(model_path, map_location=device)\n",
    "#     # demo_enhanced_model.load_state_dict(checkpoint)\n",
    "#     # print(f\"‚úì Enhanced model loaded from {model_path}\")\n",
    "    \n",
    "#     # For now, using the current demo_enhanced_model that should already be trained\n",
    "#     total_params = sum(p.numel() for p in demo_enhanced_model.parameters())\n",
    "#     print(f\"‚úì Enhanced model ready with {total_params:,} parameters\")\n",
    "    \n",
    "# except Exception as e:\n",
    "#     print(f\"‚ÑπÔ∏è  Using current demo_enhanced_model (should be already trained)\")\n",
    "#     print(f\"   Model parameters: {sum(p.numel() for p in demo_enhanced_model.parameters()):,}\")\n",
    "\n",
    "# # === Enhanced Model Inference ===\n",
    "# print(\"\\nüöÄ Starting Enhanced Model Inference...\")\n",
    "\n",
    "# final_test_preds = []\n",
    "# final_test_probabilities = []\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for batch_idx, (continuous_batch, categorical_batch, _) in enumerate(test_enhanced_loader):\n",
    "#         # Move data to device\n",
    "#         continuous_batch = continuous_batch.to(device)\n",
    "#         for key in categorical_batch:\n",
    "#             categorical_batch[key] = categorical_batch[key].to(device)\n",
    "        \n",
    "#         # Forward pass with enhanced model\n",
    "#         logits = demo_enhanced_model(continuous_batch, categorical_batch)\n",
    "#         preds = logits.argmax(dim=1).cpu().numpy()\n",
    "#         probabilities = torch.softmax(logits, dim=1).cpu().numpy()\n",
    "        \n",
    "#         # Store results\n",
    "#         final_test_preds.append(preds)\n",
    "#         final_test_probabilities.append(probabilities)\n",
    "        \n",
    "#         # Progress indicator\n",
    "#         if (batch_idx + 1) % 10 == 0 or (batch_idx + 1) == len(test_enhanced_loader):\n",
    "#             print(f\"   Processed batch {batch_idx + 1}/{len(test_enhanced_loader)}\")\n",
    "\n",
    "# # Combine results\n",
    "# final_test_preds = np.concatenate(final_test_preds)\n",
    "# final_test_probabilities = np.concatenate(final_test_probabilities)\n",
    "\n",
    "# print(f\"\\n‚úÖ Inference completed!\")\n",
    "# print(f\"üìä Total predictions: {len(final_test_preds)}\")\n",
    "# print(f\"üìà Predictions shape: {final_test_preds.shape}\")\n",
    "# print(f\"üé≤ Probabilities shape: {final_test_probabilities.shape}\")\n",
    "\n",
    "# # === Create Enhanced Submission File ===\n",
    "# label_map = {0: \"no_pain\", 1: \"low_pain\", 2: \"high_pain\"}\n",
    "\n",
    "# # Create submission data - the enhanced loader handles sequences automatically\n",
    "# submission_data = []\n",
    "# for i, prediction in enumerate(final_test_preds):\n",
    "#     submission_data.append({\n",
    "#         'sample_index': i,\n",
    "#         'pain_level': int(prediction)\n",
    "#     })\n",
    "\n",
    "# submission_df = pd.DataFrame(submission_data)\n",
    "\n",
    "# # Save enhanced submission\n",
    "# enhanced_submission_filename = f\"enhanced_{SUBMISSION_FILENAME}\"\n",
    "# submission_df.to_csv(enhanced_submission_filename, index=False)\n",
    "\n",
    "# print(f\"\\nüíæ Enhanced submission saved: {enhanced_submission_filename}\")\n",
    "# print(f\"üìä Submission shape: {submission_df.shape}\")\n",
    "\n",
    "# # Display sample and distribution\n",
    "# print(f\"\\nüìã Sample submission data:\")\n",
    "# print(submission_df.head(10))\n",
    "\n",
    "# print(f\"\\nüìà Prediction distribution:\")\n",
    "# distribution = submission_df['pain_level'].value_counts().sort_index()\n",
    "# for level, count in distribution.items():\n",
    "#     label = label_map[level]\n",
    "#     percentage = (count / len(submission_df)) * 100\n",
    "#     print(f\"   {label}: {count} ({percentage:.2f}%)\")\n",
    "\n",
    "# print(f\"\\nüéØ Enhanced model submission ready for competition!\")\n",
    "# print(f\"üìÅ File: {enhanced_submission_filename}\")\n",
    "# print(f\"üìä Total samples: {len(submission_df)}\")\n",
    "\n",
    "# # Optional: Comparison with expected format\n",
    "# try:\n",
    "#     sample_submission = pd.read_csv('an2dl2526c1/sample_submission.csv')\n",
    "#     print(f\"\\nüîç Format validation:\")\n",
    "#     print(f\"   Expected samples: {len(sample_submission)}\")\n",
    "#     print(f\"   Our samples: {len(submission_df)}\")\n",
    "#     print(f\"   Expected columns: {list(sample_submission.columns)}\")\n",
    "#     print(f\"   Our columns: {list(submission_df.columns)}\")\n",
    "    \n",
    "#     if len(submission_df) == len(sample_submission):\n",
    "#         print(\"   ‚úÖ Sample count matches!\")\n",
    "#     else:\n",
    "#         print(\"   ‚ö†Ô∏è  Sample count mismatch!\")\n",
    "        \n",
    "# except:\n",
    "#     print(\"   ‚ÑπÔ∏è  Could not load sample_submission.csv for validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model input size derived from test sequences: 21\n",
      "Created embedding for pain_survey_1: 3 -> 2\n",
      "Created embedding for pain_survey_2: 3 -> 2\n",
      "Created embedding for pain_survey_3: 3 -> 2\n",
      "Created embedding for pain_survey_4: 3 -> 2\n",
      "Total RNN input size: 17 (continuous) + 8 (embeddings) = 25\n",
      "Model created with input_size=21, hidden_size=42\n",
      "Error during dummy forward pass: EnhancedRecurrentClassifier.forward() missing 1 required positional argument: 'categorical_indices'\n",
      "‚úó ERROR: Model file not found at /c/Users/ortol/Desktop/Polimi/ANN_challenge/Artificial-neural-networks-and-deep-learning-/models/enhanced_lstm_embeddings_F10.91_20251113_134602.pt\n",
      "Please ensure the model was trained and saved properly.\n",
      "You may need to retrain the model or check the file path.\n",
      "Starting inference on actual test set...\n",
      "Running inference on 12 batches...\n",
      "Continuous batch input shape: torch.Size([480, 100, 17])\n",
      "Categorical batch keys: ['pain_survey_1', 'pain_survey_2', 'pain_survey_3', 'pain_survey_4']\n",
      "Expected continuous: (batch_size, 100, 17)\n",
      "Processed batch 10/12\n",
      "Processed batch 12/12\n",
      "\n",
      "Inference on actual test set completed successfully!\n",
      "Total predictions: 5296\n",
      "Predictions shape: (5296,)\n",
      "Probabilities shape: (5296, 3)\n",
      "Processed batch 10/12\n",
      "Processed batch 12/12\n",
      "\n",
      "Inference on actual test set completed successfully!\n",
      "Total predictions: 5296\n",
      "Predictions shape: (5296,)\n",
      "Probabilities shape: (5296, 3)\n",
      "   sample_index prediction\n",
      "0           0.0   low_pain\n",
      "1           1.0   low_pain\n",
      "2           2.0   low_pain\n",
      "3           3.0   low_pain\n",
      "4           4.0   low_pain\n",
      "‚úÖ Saved submission with 1324 rows should be 1324\n",
      "\n",
      "üìä Distribution of predicted classes:\n",
      "   no_pain   :   0.00%\n",
      "   low_pain  : 100.00%\n",
      "   high_pain :   0.00%\n",
      "\n",
      "üß≠ Most common predicted label: low_pain (100.00%) \n",
      "\n",
      "   sample_index prediction\n",
      "0           0.0   low_pain\n",
      "1           1.0   low_pain\n",
      "2           2.0   low_pain\n",
      "3           3.0   low_pain\n",
      "4           4.0   low_pain\n",
      "‚úÖ Saved submission with 1324 rows should be 1324\n",
      "\n",
      "üìä Distribution of predicted classes:\n",
      "   no_pain   :   0.00%\n",
      "   low_pain  : 100.00%\n",
      "   high_pain :   0.00%\n",
      "\n",
      "üß≠ Most common predicted label: low_pain (100.00%) \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f\"Model input size derived from test sequences: {actual_input_size}\")\n",
    "\n",
    "# rnn_model = RecurrentClassifier(\n",
    "#     input_size=actual_input_size,\n",
    "#     hidden_size=HIDDEN_SIZE,\n",
    "#     num_layers=HIDDEN_LAYERS,\n",
    "#     num_classes=num_classes,\n",
    "#     dropout_rate=DROPOUT_RATE,\n",
    "#     bidirectional=BIDIRECTIONAL,\n",
    "#     rnn_type=RNN_TYPE\n",
    "#     ).to(device)\n",
    "\n",
    "demo_enhanced_model = EnhancedRecurrentClassifier(\n",
    "    continuous_input_size=17,\n",
    "    categorical_features=categorical_feature_config,\n",
    "    embedding_dims=embedding_dims,\n",
    "    hidden_size=HIDDEN_SIZE,  # Smaller for faster training\n",
    "    num_layers=2,\n",
    "    num_classes=3,\n",
    "    rnn_type='LSTM',\n",
    "    bidirectional=True,\n",
    "    dropout_rate=0.2\n",
    ").to(device)\n",
    "\n",
    "print(f\"Model created with input_size={actual_input_size}, hidden_size={HIDDEN_SIZE}\")\n",
    "# It's good practice to summarize the model with the actual input shape it will receive\n",
    "recurrent_summary(demo_enhanced_model, input_size=X_test_sequences.shape[1:])\n",
    "\n",
    "\n",
    "# Load the trained model\n",
    "# model_path = MODEL_LOAD_PATH\n",
    "model_path = \"/c/Users/ortol/Desktop/Polimi/ANN_challenge/Artificial-neural-networks-and-deep-learning-/models/enhanced_lstm_embeddings_F10.91_20251113_134602.pt\"  # Example hardcoded path\n",
    "\n",
    "\n",
    "try:\n",
    "    # Load the state dict\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    demo_enhanced_model.load_state_dict(checkpoint)\n",
    "    print(f\"‚úì Model successfully loaded from {model_path}\")\n",
    "\n",
    "    # Verify model architecture matches\n",
    "    total_params = sum(p.numel() for p in demo_enhanced_model.parameters())\n",
    "    print(f\"‚úì Model loaded with {total_params:,} parameters\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"‚úó ERROR: Model file not found at {model_path}\")\n",
    "    print(\"Please ensure the model was trained and saved properly.\")\n",
    "    print(\"You may need to retrain the model or check the file path.\")\n",
    "    # Exit or handle the error appropriately if the model cannot be loaded\n",
    "\n",
    "except RuntimeError as e:\n",
    "    print(f\"‚úó ERROR: Model architecture mismatch!\")\n",
    "    print(f\"Error details: {str(e)}\")\n",
    "    print(\"\\nThis usually happens when the saved model has a different architecture\")\n",
    "    print(\"than the current model definition (input_size, hidden_size, num_layers, bidirectional).\")\n",
    "    print(\"Please ensure the current model definition matches the saved model.\")\n",
    "    # Re-raise the exception after providing diagnostic information\n",
    "    raise e\n",
    "\n",
    "\n",
    "demo_enhanced_model.eval()  # Set model to evaluation mode\n",
    "\n",
    "print(\"Starting inference on actual test set...\")\n",
    "\n",
    "# --- Inference Pipeline ---\n",
    "final_test_preds = []\n",
    "final_test_probabilities = []\n",
    "sample_indices = []\n",
    "\n",
    "# Use the enhanced test loader instead of the regular one\n",
    "print(f\"Running inference on {len(test_enhanced_final_loader)} batches...\")\n",
    "\n",
    "with torch.no_grad():  # Disable gradient computation for inference\n",
    "    for batch_idx, (continuous_batch, categorical_batch, labels_batch) in enumerate(test_enhanced_final_loader):\n",
    "        # Move data to device\n",
    "        continuous_batch = continuous_batch.to(device)\n",
    "        categorical_batch = {k: v.to(device) for k, v in categorical_batch.items()}\n",
    "\n",
    "        # Verify batch dimensions\n",
    "        if batch_idx == 0:\n",
    "            print(f\"Continuous batch input shape: {continuous_batch.shape}\")\n",
    "            print(f\"Categorical batch keys: {list(categorical_batch.keys())}\")\n",
    "            print(f\"Expected continuous: (batch_size, {WINDOW_SIZE}, 17)\")\n",
    "\n",
    "        # Get model predictions - Enhanced model expects continuous and categorical data\n",
    "        logits = demo_enhanced_model(continuous_batch, categorical_batch)\n",
    "        preds = logits.argmax(dim=1).cpu().numpy()\n",
    "\n",
    "        # Get probabilities for confidence analysis\n",
    "        probabilities = torch.softmax(logits, dim=1).cpu().numpy()\n",
    "\n",
    "        # Store batch results\n",
    "        final_test_preds.append(preds)\n",
    "        final_test_probabilities.append(probabilities)\n",
    "\n",
    "        # Progress indicator\n",
    "        if (batch_idx + 1) % 10 == 0 or (batch_idx + 1) == len(test_enhanced_final_loader):\n",
    "            print(f\"Processed batch {batch_idx + 1}/{len(test_enhanced_final_loader)}\")\n",
    "\n",
    "\n",
    "# Combine all batches into single arrays\n",
    "final_test_preds = np.concatenate(final_test_preds)\n",
    "final_test_probabilities = np.concatenate(final_test_probabilities)\n",
    "\n",
    "print(f\"\\nInference on actual test set completed successfully!\")\n",
    "print(f\"Total predictions: {len(final_test_preds)}\")\n",
    "print(f\"Predictions shape: {final_test_preds.shape}\")\n",
    "print(f\"Probabilities shape: {final_test_probabilities.shape}\")\n",
    "\n",
    "\n",
    "# --- Create Submission File ---\n",
    "# Map numerical predictions back to original labels\n",
    "label_map = {0: \"no_pain\", 1: \"low_pain\", 2: \"high_pain\"}\n",
    "pred_labels = [label_map[p] for p in final_test_preds]\n",
    "\n",
    "# The sample_index for the submission file should correspond to the original sample_index from X_test_final_df\n",
    "# Since build_sequences creates multiple sequences per sample_index (if stride < window),\n",
    "# we need to associate each prediction with its original sample_index.\n",
    "# A simple way is to assume each sequence corresponds to the sample_index it came from.\n",
    "# This might not be perfectly accurate if predictions should be per-sample instead of per-sequence.\n",
    "# For this submission, we'll create a submission row for each sequence prediction.\n",
    "\n",
    "# Generate sample indices for the sequences\n",
    "# The number of sequences is len(final_test_preds)\n",
    "# We need to replicate the original sample_indices based on how many sequences were generated per sample.\n",
    "# This requires re-running or adapting the logic from build_sequences to track original indices.\n",
    "\n",
    "# A simpler approach for submission, if predictions are expected per original sample_index,\n",
    "# is to average predictions per sample_index or take the majority vote.\n",
    "# However, the competition usually expects one prediction per sequence/window if the model outputs per sequence.\n",
    "# Let's assume the submission requires one prediction per sequence generated.\n",
    "\n",
    "final_results = [] # Initialize final_results list\n",
    "\n",
    "for sample_id in X_test_final_df['sample_index'].unique():\n",
    "\n",
    "    # extract rows for this sample\n",
    "    temp = X_test_final_df[X_test_final_df['sample_index'] == sample_id]\n",
    "\n",
    "    # If the sample has fewer rows than WINDOW_SIZE, pad it with zeros\n",
    "    if len(temp) < WINDOW_SIZE:\n",
    "        padding = pd.DataFrame(0, index=np.arange(WINDOW_SIZE - len(temp)), columns=temp.columns)\n",
    "        temp = pd.concat([temp, padding], ignore_index=True)\n",
    "\n",
    "    # build sequences for this sample using enhanced version - note: returns 3 values\n",
    "    continuous_seqs, categorical_seqs, _ = build_sequences_test_enhanced(temp, window=WINDOW_SIZE, stride=STRIDE)\n",
    "    \n",
    "    # sometimes build_sequences might still return zero sequences if len(temp) < stride\n",
    "    if len(continuous_seqs) == 0:\n",
    "        # fallback: take the last WINDOW_SIZE rows and process them\n",
    "        temp_fallback = temp.iloc[-WINDOW_SIZE:]\n",
    "        \n",
    "        # Separate continuous and categorical features\n",
    "        continuous_cols = [col for col in data.columns if col.startswith('joint_')]\n",
    "        categorical_cols = [col for col in data.columns if col.startswith('pain_survey_')]\n",
    "        \n",
    "        continuous_data = temp_fallback[continuous_cols].values\n",
    "        categorical_data = {}\n",
    "        for col in categorical_cols:\n",
    "            categorical_data[col] = temp_fallback[col].values\n",
    "        \n",
    "        # Add batch dimension\n",
    "        continuous_seqs = continuous_data[np.newaxis, :, :]\n",
    "        categorical_seqs = {k: v[np.newaxis, :] for k, v in categorical_data.items()}\n",
    "\n",
    "    # Convert to tensors\n",
    "    continuous_seqs = torch.tensor(continuous_seqs, dtype=torch.float32).to(device)\n",
    "    categorical_seqs = {k: torch.tensor(v, dtype=torch.long).to(device) for k, v in categorical_seqs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = demo_enhanced_model(continuous_seqs, categorical_seqs)\n",
    "        probs = torch.softmax(logits, dim=1).cpu().numpy()\n",
    "        preds = np.argmax(probs, axis=1)\n",
    "\n",
    "    # Final decision for this sample_index\n",
    "    final_class = np.bincount(preds).argmax()  # majority vote\n",
    "    final_results.append({\n",
    "        \"sample_index\": sample_id,\n",
    "        \"prediction\": label_map[final_class]\n",
    "    })\n",
    "\n",
    "submission = pd.DataFrame(final_results)\n",
    "submission.to_csv(SUBMISSION_FILENAME, index=False)\n",
    "\n",
    "print(submission.head())\n",
    "print(f\"‚úÖ Saved submission with {len(submission)} rows should be 1324\")\n",
    "\n",
    "# --- Analyze class distribution in final predictions ---\n",
    "label_counts = submission['prediction'].value_counts(normalize=True) * 100\n",
    "\n",
    "print(\"\\nüìä Distribution of predicted classes:\")\n",
    "for label in ['no_pain', 'low_pain', 'high_pain']:\n",
    "    pct = label_counts.get(label, 0.0)\n",
    "    print(f\"   {label:10s}: {pct:6.2f}%\")\n",
    "\n",
    "# Optional: quick sanity check for imbalance\n",
    "majority_label = label_counts.idxmax()\n",
    "print(f\"\\nüß≠ Most common predicted label: {majority_label} ({label_counts.max():.2f}%) \\n\")\n",
    "\n",
    "# Optional: visualize as a bar chart\n",
    "import matplotlib.pyplot as plt\n",
    "desired_order = [\"no_pain\", \"low_pain\", \"high_pain\"]\n",
    "label_counts_ordered = label_counts.reindex(desired_order)\n",
    "plt.figure(figsize=(5,3))\n",
    "(label_counts_ordered\n",
    " .plot(kind='bar', rot=0, title='Predicted Class Distribution', ylabel='Percentage (%)'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Enhanced Model Submission Pipeline\n",
    "\n",
    "This section processes the actual test CSV data according to the enhanced model requirements and generates the final submission file in the correct competition format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÜ Starting Enhanced Model Competition Submission Pipeline\n",
      "======================================================================\n",
      "\n",
      "üìÇ Step 1: Loading Competition Test Data...\n",
      "‚úì Loaded test data: (211840, 40)\n",
      "  üìä Columns: ['sample_index', 'time', 'pain_survey_1', 'pain_survey_2', 'pain_survey_3', 'pain_survey_4', 'n_legs', 'n_hands', 'n_eyes', 'joint_00', 'joint_01', 'joint_02', 'joint_03', 'joint_04', 'joint_05', 'joint_06', 'joint_07', 'joint_08', 'joint_09', 'joint_10', 'joint_11', 'joint_12', 'joint_13', 'joint_14', 'joint_15', 'joint_16', 'joint_17', 'joint_18', 'joint_19', 'joint_20', 'joint_21', 'joint_22', 'joint_23', 'joint_24', 'joint_25', 'joint_26', 'joint_27', 'joint_28', 'joint_29', 'joint_30']\n",
      "  üî¢ Unique samples: 1324\n",
      "  ‚è±Ô∏è  Time steps per sample: ~160\n",
      "\n",
      "üìã Sample submission format: (1324, 2)\n",
      "  üìù Columns: ['sample_index', 'label']\n",
      "  üéØ Expected samples: 1324\n",
      "  üìä Label distribution: {'no_pain': 456, 'high_pain': 437, 'low_pain': 431}\n",
      "\n",
      "üîß Step 2: Preprocessing Test Data for Enhanced Model...\n",
      "  üìù Mapping categorical features...\n",
      "  ‚úì Categorical mappings applied\n",
      "  ‚úì Data types converted to float32\n",
      "  ‚úì Removed features: ['n_legs', 'n_hands', 'n_eyes', 'joint_13', 'joint_14', 'joint_15', 'joint_16', 'joint_17', 'joint_18', 'joint_19', 'joint_20', 'joint_21', 'joint_22', 'joint_23', 'joint_24', 'joint_25', 'joint_30']\n",
      "  üî¢ Applying normalization...\n",
      "  ‚úì Normalized 17 continuous features\n",
      "  üìä Final test data shape: (211840, 23)\n",
      "\n",
      "üîÑ Step 3: Preparing Enhanced Model Data Format...\n",
      "  üìã Categorical features: ['pain_survey_1', 'pain_survey_2', 'pain_survey_3', 'pain_survey_4']\n",
      "  üìä Continuous features (17): ['joint_00', 'joint_01', 'joint_02', 'joint_03', 'joint_04']...\n",
      "  üîÑ Generating sequences for enhanced model...\n",
      "  ‚úì Generated sequences for 1324 samples\n",
      "  üìê Continuous data shape: (1324, 68, 17)\n",
      "  üìê Categorical shapes: [('pain_survey_1', (1324, 68)), ('pain_survey_2', (1324, 68)), ('pain_survey_3', (1324, 68)), ('pain_survey_4', (1324, 68))]\n",
      "\n",
      "üîÑ Step 4: Creating Enhanced Dataset...\n",
      "  ‚úì Created enhanced dataset with 1324 samples\n",
      "  ‚úì Created dataloader with 3 batches\n",
      "\n",
      "üöÄ Step 5: Generating Predictions with Enhanced Model...\n",
      "  üìä Processed batch 3/3\n",
      "  ‚úÖ Generated 1324 predictions\n",
      "  üìà Prediction distribution: [570 172 582]\n",
      "\n",
      "üìù Step 6: Creating Competition Submission File...\n",
      "  üíæ Saved submission: enhanced_competition_LSTM_bi_13-11-13-47.csv\n",
      "  üìä Submission shape: (1324, 2)\n",
      "\n",
      "‚úÖ Step 7: Final Validation and Summary...\n",
      "  üîç Format validation:\n",
      "    Expected samples: 1324\n",
      "    Our samples: 1324\n",
      "    Sample index format: ['000', '001', '002', '003', '004']\n",
      "    Label values: ['high_pain', 'low_pain', 'no_pain']\n",
      "    ‚úÖ Sample count matches!\n",
      "\n",
      "  üìä Final Prediction Distribution:\n",
      "    no_pain   :  570 samples ( 43.0%)\n",
      "    low_pain  :  172 samples ( 13.0%)\n",
      "    high_pain :  582 samples ( 44.0%)\n",
      "\n",
      "  üéØ Prediction Confidence:\n",
      "    Mean confidence: 0.710\n",
      "    Min confidence:  0.391\n",
      "    Max confidence:  0.924\n",
      "\n",
      "  üìã Sample Submission Preview:\n",
      "sample_index     label\n",
      "         000 high_pain\n",
      "         001 high_pain\n",
      "         002   no_pain\n",
      "         003   no_pain\n",
      "         004 high_pain\n",
      "         005 high_pain\n",
      "         006   no_pain\n",
      "         007   no_pain\n",
      "         008   no_pain\n",
      "         009 high_pain\n",
      "\n",
      "üèÜ Enhanced Model Competition Submission Complete!\n",
      "üìÅ File: enhanced_competition_LSTM_bi_13-11-13-47.csv\n",
      "üìä Ready for competition upload!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# ENHANCED MODEL SUBMISSION PIPELINE FOR COMPETITION\n",
    "# ===================================================================\n",
    "\n",
    "print(\"üèÜ Starting Enhanced Model Competition Submission Pipeline\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# --- Step 1: Load and Examine Test Data ---\n",
    "print(\"\\nüìÇ Step 1: Loading Competition Test Data...\")\n",
    "\n",
    "# Load the test dataset\n",
    "test_data_path = 'an2dl2526c1/pirate_pain_test.csv'\n",
    "X_test_competition = pd.read_csv(test_data_path)\n",
    "\n",
    "print(f\"‚úì Loaded test data: {X_test_competition.shape}\")\n",
    "print(f\"  üìä Columns: {list(X_test_competition.columns)}\")\n",
    "print(f\"  üî¢ Unique samples: {X_test_competition['sample_index'].nunique()}\")\n",
    "print(f\"  ‚è±Ô∏è  Time steps per sample: ~{len(X_test_competition) // X_test_competition['sample_index'].nunique()}\")\n",
    "\n",
    "# Load sample submission to check format\n",
    "sample_submission = pd.read_csv('an2dl2526c1/sample_submission.csv')\n",
    "print(f\"\\nüìã Sample submission format: {sample_submission.shape}\")\n",
    "print(f\"  üìù Columns: {list(sample_submission.columns)}\")\n",
    "print(f\"  üéØ Expected samples: {len(sample_submission)}\")\n",
    "print(f\"  üìä Label distribution: {sample_submission['label'].value_counts().to_dict()}\")\n",
    "\n",
    "# --- Step 2: Preprocess Test Data for Enhanced Model ---\n",
    "print(f\"\\nüîß Step 2: Preprocessing Test Data for Enhanced Model...\")\n",
    "\n",
    "# Apply same preprocessing as training data\n",
    "print(\"  üìù Mapping categorical features...\")\n",
    "\n",
    "# Map string columns to numeric values (same as training)\n",
    "map_dict = {'two': 2, 'one+peg_leg': 1}\n",
    "X_test_competition['n_legs'] = X_test_competition['n_legs'].map(map_dict)\n",
    "\n",
    "map_dict = {'two': 2, 'one+hook_hand': 1}\n",
    "X_test_competition['n_hands'] = X_test_competition['n_hands'].map(map_dict)\n",
    "\n",
    "map_dict = {'two': 2, 'one+eye_patch': 1}\n",
    "X_test_competition['n_eyes'] = X_test_competition['n_eyes'].map(map_dict)\n",
    "\n",
    "print(\"  ‚úì Categorical mappings applied\")\n",
    "\n",
    "# Convert to float32\n",
    "X_test_competition = X_test_competition.astype(np.float32)\n",
    "print(\"  ‚úì Data types converted to float32\")\n",
    "\n",
    "# Remove features that were excluded during training\n",
    "list_to_remove = ['n_legs', 'n_hands', 'n_eyes', 'joint_13', 'joint_14', 'joint_15', \n",
    "                  'joint_16', 'joint_17', 'joint_18', 'joint_19', 'joint_20', 'joint_21', \n",
    "                  'joint_22', 'joint_23', 'joint_24', 'joint_25', 'joint_30']\n",
    "\n",
    "# Remove columns that exist in the test data\n",
    "existing_to_remove = [col for col in list_to_remove if col in X_test_competition.columns]\n",
    "if existing_to_remove:\n",
    "    X_test_competition = X_test_competition.drop(columns=existing_to_remove)\n",
    "    print(f\"  ‚úì Removed features: {existing_to_remove}\")\n",
    "\n",
    "# Normalize using training statistics (same as training preprocessing)\n",
    "print(\"  üî¢ Applying normalization...\")\n",
    "scale_columns = [col for col in X_test_competition.columns \n",
    "                 if col not in ['sample_index', 'time'] and 'pain_survey' not in col]\n",
    "\n",
    "# Use the normalization statistics from training data (should be available from training)\n",
    "# Note: In production, these would be saved from training. Here we approximate.\n",
    "for column in scale_columns:\n",
    "    if column in X_test_competition.columns:\n",
    "        col_min = X_test_competition[column].min()\n",
    "        col_max = X_test_competition[column].max()\n",
    "        if col_max != col_min:\n",
    "            X_test_competition[column] = (X_test_competition[column] - col_min) / (col_max - col_min)\n",
    "\n",
    "print(f\"  ‚úì Normalized {len(scale_columns)} continuous features\")\n",
    "print(f\"  üìä Final test data shape: {X_test_competition.shape}\")\n",
    "\n",
    "# --- Step 3: Prepare Enhanced Data Format ---\n",
    "print(f\"\\nüîÑ Step 3: Preparing Enhanced Model Data Format...\")\n",
    "\n",
    "# Separate continuous and categorical features for enhanced model\n",
    "categorical_cols = ['pain_survey_1', 'pain_survey_2', 'pain_survey_3', 'pain_survey_4']\n",
    "continuous_cols = [col for col in X_test_competition.columns \n",
    "                   if col not in categorical_cols + ['sample_index', 'time']]\n",
    "\n",
    "print(f\"  üìã Categorical features: {categorical_cols}\")\n",
    "print(f\"  üìä Continuous features ({len(continuous_cols)}): {continuous_cols[:5]}...\")\n",
    "\n",
    "# Prepare data by sample for sequence generation\n",
    "print(f\"  üîÑ Generating sequences for enhanced model...\")\n",
    "\n",
    "all_continuous_sequences = []\n",
    "all_categorical_sequences = {col: [] for col in categorical_cols}\n",
    "sample_ids = []\n",
    "\n",
    "for sample_id in sorted(X_test_competition['sample_index'].unique()):\n",
    "    # Get sample data\n",
    "    sample_data = X_test_competition[X_test_competition['sample_index'] == sample_id]\n",
    "    \n",
    "    # Skip 'sample_index' and 'time' for feature extraction\n",
    "    continuous_features = sample_data[continuous_cols].values\n",
    "    \n",
    "    # Build sequences for continuous features using numpy array processing\n",
    "    def build_sequences_numpy(data, window, stride):\n",
    "        \"\"\"Helper function to build sequences from numpy arrays\"\"\"\n",
    "        if len(data) < window:\n",
    "            # Pad if necessary\n",
    "            padding_needed = window - len(data)\n",
    "            if len(data.shape) == 1:\n",
    "                padded_data = np.concatenate([data, np.zeros(padding_needed)])\n",
    "            else:\n",
    "                padded_data = np.vstack([data, np.zeros((padding_needed, data.shape[1]))])\n",
    "            return padded_data[np.newaxis, :, :] if len(padded_data.shape) == 2 else padded_data[np.newaxis, :]\n",
    "        \n",
    "        # Generate sequences with sliding window\n",
    "        sequences = []\n",
    "        for i in range(0, len(data) - window + 1, stride):\n",
    "            seq = data[i:i + window]\n",
    "            sequences.append(seq)\n",
    "        \n",
    "        if len(sequences) == 0:\n",
    "            # Fallback: use last window_size elements\n",
    "            if len(data.shape) == 1:\n",
    "                sequences = [data[-window:]]\n",
    "            else:\n",
    "                sequences = [data[-window:, :]]\n",
    "        \n",
    "        return np.array(sequences)\n",
    "    \n",
    "    # Build sequences for continuous features\n",
    "    cont_seqs = build_sequences_numpy(continuous_features, WINDOW_SIZE, STRIDE)\n",
    "    \n",
    "    # Build sequences for categorical features\n",
    "    cat_seqs = {}\n",
    "    for col in categorical_cols:\n",
    "        if col in sample_data.columns:\n",
    "            cat_values = sample_data[col].values\n",
    "            cat_seq = build_sequences_numpy(cat_values, WINDOW_SIZE, STRIDE)\n",
    "            cat_seqs[col] = cat_seq.squeeze(-1) if len(cat_seq.shape) == 3 else cat_seq\n",
    "        else:\n",
    "            # If column doesn't exist, fill with zeros\n",
    "            cat_seqs[col] = np.zeros((cont_seqs.shape[0], WINDOW_SIZE))\n",
    "    \n",
    "    # Store sequences (use first sequence if multiple)\n",
    "    all_continuous_sequences.append(cont_seqs[0])  # Take first sequence\n",
    "    for col in categorical_cols:\n",
    "        all_categorical_sequences[col].append(cat_seqs[col][0])  # Take first sequence\n",
    "    \n",
    "    sample_ids.append(sample_id)\n",
    "\n",
    "# Convert to arrays\n",
    "continuous_data = np.array(all_continuous_sequences)\n",
    "categorical_data = {col: np.array(all_categorical_sequences[col]) for col in categorical_cols}\n",
    "\n",
    "print(f\"  ‚úì Generated sequences for {len(sample_ids)} samples\")\n",
    "print(f\"  üìê Continuous data shape: {continuous_data.shape}\")\n",
    "print(f\"  üìê Categorical shapes: {[(col, arr.shape) for col, arr in categorical_data.items()]}\")\n",
    "\n",
    "# --- Step 4: Create Enhanced Dataset and DataLoader ---\n",
    "print(f\"\\nüîÑ Step 4: Creating Enhanced Dataset...\")\n",
    "\n",
    "# Convert to tensors\n",
    "continuous_tensor = torch.tensor(continuous_data, dtype=torch.float32)\n",
    "categorical_tensors = {col: torch.tensor(categorical_data[col], dtype=torch.long) \n",
    "                      for col in categorical_cols}\n",
    "\n",
    "# Create dummy labels (not used for prediction)\n",
    "dummy_labels = torch.zeros(len(sample_ids), dtype=torch.long)\n",
    "\n",
    "# Create enhanced dataset\n",
    "competition_enhanced_ds = EnhancedDataset(\n",
    "    continuous_tensor, categorical_tensors, dummy_labels\n",
    ")\n",
    "\n",
    "# Create dataloader\n",
    "competition_enhanced_loader = make_enhanced_loader(\n",
    "    competition_enhanced_ds, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False, \n",
    "    drop_last=False\n",
    ")\n",
    "\n",
    "print(f\"  ‚úì Created enhanced dataset with {len(competition_enhanced_ds)} samples\")\n",
    "print(f\"  ‚úì Created dataloader with {len(competition_enhanced_loader)} batches\")\n",
    "\n",
    "# --- Step 5: Generate Predictions with Enhanced Model ---\n",
    "print(f\"\\nüöÄ Step 5: Generating Predictions with Enhanced Model...\")\n",
    "\n",
    "demo_enhanced_model.eval()\n",
    "predictions = []\n",
    "prediction_probabilities = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (continuous_batch, categorical_batch, _) in enumerate(competition_enhanced_loader):\n",
    "        # Move to device\n",
    "        continuous_batch = continuous_batch.to(device)\n",
    "        for key in categorical_batch:\n",
    "            categorical_batch[key] = categorical_batch[key].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = demo_enhanced_model(continuous_batch, categorical_batch)\n",
    "        batch_preds = logits.argmax(dim=1).cpu().numpy()\n",
    "        batch_probs = torch.softmax(logits, dim=1).cpu().numpy()\n",
    "        \n",
    "        predictions.extend(batch_preds)\n",
    "        prediction_probabilities.extend(batch_probs)\n",
    "        \n",
    "        if (batch_idx + 1) % 10 == 0 or (batch_idx + 1) == len(competition_enhanced_loader):\n",
    "            print(f\"  üìä Processed batch {batch_idx + 1}/{len(competition_enhanced_loader)}\")\n",
    "\n",
    "predictions = np.array(predictions)\n",
    "prediction_probabilities = np.array(prediction_probabilities)\n",
    "\n",
    "print(f\"  ‚úÖ Generated {len(predictions)} predictions\")\n",
    "print(f\"  üìà Prediction distribution: {np.bincount(predictions)}\")\n",
    "\n",
    "# --- Step 6: Create Competition Submission File ---\n",
    "print(f\"\\nüìù Step 6: Creating Competition Submission File...\")\n",
    "\n",
    "# Map predictions to labels\n",
    "label_mapping = {0: \"no_pain\", 1: \"low_pain\", 2: \"high_pain\"}\n",
    "predicted_labels = [label_mapping[pred] for pred in predictions]\n",
    "\n",
    "# Create submission dataframe with correct format\n",
    "submission_data = []\n",
    "for i, (sample_id, pred_label) in enumerate(zip(sample_ids, predicted_labels)):\n",
    "    submission_data.append({\n",
    "        'sample_index': f\"{int(sample_id):03d}\",  # Convert to int then format as 3-digit string\n",
    "        'label': pred_label\n",
    "    })\n",
    "\n",
    "# Create submission DataFrame\n",
    "submission_df = pd.DataFrame(submission_data)\n",
    "\n",
    "# Sort by sample_index to ensure correct order\n",
    "submission_df = submission_df.sort_values('sample_index').reset_index(drop=True)\n",
    "\n",
    "# Save submission file\n",
    "enhanced_submission_filename = f\"enhanced_competition_{SUBMISSION_FILENAME}\"\n",
    "submission_df.to_csv(enhanced_submission_filename, index=False)\n",
    "\n",
    "print(f\"  üíæ Saved submission: {enhanced_submission_filename}\")\n",
    "print(f\"  üìä Submission shape: {submission_df.shape}\")\n",
    "\n",
    "# --- Step 7: Validation and Summary ---\n",
    "print(f\"\\n‚úÖ Step 7: Final Validation and Summary...\")\n",
    "\n",
    "# Validate format\n",
    "print(f\"  üîç Format validation:\")\n",
    "print(f\"    Expected samples: {len(sample_submission)}\")\n",
    "print(f\"    Our samples: {len(submission_df)}\")\n",
    "print(f\"    Sample index format: {submission_df['sample_index'].iloc[:5].tolist()}\")\n",
    "print(f\"    Label values: {sorted(submission_df['label'].unique())}\")\n",
    "\n",
    "# Check if we have the right number of samples\n",
    "if len(submission_df) == len(sample_submission):\n",
    "    print(f\"    ‚úÖ Sample count matches!\")\n",
    "else:\n",
    "    print(f\"    ‚ö†Ô∏è  Sample count mismatch: expected {len(sample_submission)}, got {len(submission_df)}\")\n",
    "\n",
    "# Show prediction distribution\n",
    "pred_dist = submission_df['label'].value_counts()\n",
    "pred_dist_pct = (pred_dist / len(submission_df) * 100).round(2)\n",
    "\n",
    "print(f\"\\n  üìä Final Prediction Distribution:\")\n",
    "for label in ['no_pain', 'low_pain', 'high_pain']:\n",
    "    count = pred_dist.get(label, 0)\n",
    "    pct = pred_dist_pct.get(label, 0.0)\n",
    "    print(f\"    {label:10s}: {count:4d} samples ({pct:5.1f}%)\")\n",
    "\n",
    "# Show confidence statistics\n",
    "confidence_scores = np.max(prediction_probabilities, axis=1)\n",
    "print(f\"\\n  üéØ Prediction Confidence:\")\n",
    "print(f\"    Mean confidence: {confidence_scores.mean():.3f}\")\n",
    "print(f\"    Min confidence:  {confidence_scores.min():.3f}\")\n",
    "print(f\"    Max confidence:  {confidence_scores.max():.3f}\")\n",
    "\n",
    "# Display sample of submission\n",
    "print(f\"\\n  üìã Sample Submission Preview:\")\n",
    "print(submission_df.head(10).to_string(index=False))\n",
    "\n",
    "print(f\"\\nüèÜ Enhanced Model Competition Submission Complete!\")\n",
    "print(f\"üìÅ File: {enhanced_submission_filename}\")\n",
    "print(f\"üìä Ready for competition upload!\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
