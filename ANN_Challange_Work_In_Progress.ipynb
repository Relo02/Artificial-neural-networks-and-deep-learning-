{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jy5OeJu9o8Rt"
   },
   "source": [
    "# üè¥‚Äç‚ò†Ô∏è Pirate Pain Classification Challenge\n",
    "\n",
    "> ‚öì *\"Even pirates feel pain ‚Äî let's teach the model to feel it too.\"*\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Table of Contents\n",
    "0. [README](#readme)  \n",
    "1. [Setup & Configuration](#setup)  \n",
    "2. [Data Loading](#data-loading)  \n",
    "3. [Import Libraries](#import-libraries)  \n",
    "4. [Data Preprocessing](#data-preprocessing)  \n",
    "5. [Sequence Building](#sequence-building)  \n",
    "6. [DataLoaders](#dataloaders)  \n",
    "7. [Network Hyperparameters](#hyperparameters)\n",
    "8. [Model Architecture](#model-architecture)  \n",
    "9. [Training Functions](#training-functions)  \n",
    "10. [Model Training](#model-training)  \n",
    "11. [Evaluation & Metrics](#evaluation)  \n",
    "12. [Model Loading & Final Testing](#model-loading)  \n",
    "13. [Competition Submission](#submission)\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è Quick Configuration Map\n",
    "\n",
    "> üß≠ *\"If ye seek to tweak the code, here be where to look!\"*\n",
    "\n",
    "- üß∫ **Batch Size:** ‚Üí [DataLoaders](#dataloaders)  \n",
    "- ‚öóÔ∏è **Hyperparameters:** ‚Üí [Network Hyperparameters](#hyperparameters)  \n",
    "- ü™û **Window Size & Stride:** ‚Üí [Sequence Building](#sequence-building)  \n",
    "- ‚öôÔ∏è **Model Type:** ‚Üí [Setup & Configuration](#setup)  \n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### üí∞ Treasure Storage ‚Äî Models & Submissions\n",
    "> üè¥‚Äç‚ò†Ô∏è *\"A wise pirate always knows where his treasure be buried ‚Äî guard yer models and submissions well!\"*\n",
    "\n",
    "- üíæ **Model & Submission Save/Load Path:** ‚Üí [Setup & Configuration](#setup)  \n",
    "  - üóÇÔ∏è Models be saved in a **`models/`** folder with the name:\n",
    "    **`experiment_name_dd-mm-HH-MM.pt`** (day-month-hour-minute).\n",
    "  - üìú Submissions be saved in a **`submissions/`** folder with the filename format:  \n",
    "    **`experiment_name_dd-mm-HH-MM.csv`** .\n",
    "  - üî° All related model parameters are saved in **`models/`** folder with the  name **`experiment_name_dd-mm-HH-MM_config.json`** .\n",
    "\n",
    "  \n",
    "  *‚ùóThe experiment name is set as **`RnnType_Bi_dd-mm-HH-MM`** or **`RnnType_dd-mm-HH-MM`** depending on if it is bidirectional or not*\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4oU_xMZwJUrZ"
   },
   "source": [
    "<a id=\"readme\"></a>\n",
    "## 0. Info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mjy_NO-5HPw3"
   },
   "source": [
    "\n",
    "\n",
    "This section lists all the main parameters that can be modified to control data loading, model behavior, and training.\n",
    "\n",
    "---\n",
    "\n",
    "### üìÅ File Paths\n",
    "| Variable | Description | Default Value |\n",
    "|-----------|--------------|----------------|\n",
    "| `TRAIN_DATA_PATH` | Training features | `'pirate_pain_train.csv'` |\n",
    "| `TRAIN_LABELS_PATH` | Training labels | `'pirate_pain_train_labels.csv'` |\n",
    "| `TEST_DATA_PATH` | Test set for inference | `'pirate_pain_test.csv'` *(optional)* |\n",
    "| `MODEL_SAVE_PATH` | Output model file | `'pirate_model.pt'` |\n",
    "| `RESULTS_FILE` | CSV for predictions | `'results_<date-time>.csv'` |\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Model & Architecture\n",
    "| Parameter | Description | Typical Values |\n",
    "|------------|--------------|----------------|\n",
    "| `model_type` | Choose model class | `'RNN'`, `'LSTM'`, `'GRU'`, `'ANN'` |\n",
    "| `input_size` | Number of features per time step | *auto-detected from data* |\n",
    "| `hidden_size` | Hidden layer size | `64`, `128`, `256` |\n",
    "| `num_layers` | Number of RNN layers | `1-4` |\n",
    "| `dropout` | Dropout probability | `0.2‚Äì0.5` |\n",
    "| `num_classes` | Output classes (pain levels) | *from label set* |\n",
    "\n",
    "---\n",
    "\n",
    "### üèãÔ∏è Training Hyperparameters\n",
    "| Parameter | Description | Default / Range |\n",
    "|------------|--------------|-----------------|\n",
    "| `batch_size` | Samples per batch | `512/2^n` |\n",
    "| `learning_rate` | Optimizer learning rate | `1e-3` |\n",
    "| `num_epochs` | Training iterations | `500` |\n",
    "| `optimizer` | Optimization algorithm | `'AdamW'` |\n",
    "| `criterion` | Loss function | `CrossEntropyLoss()` |\n",
    "| `seed` | Random seed for reproducibility | `42` |\n",
    "\n",
    "---\n",
    "\n",
    "### üì§ Inference\n",
    "| Parameter | Description |\n",
    "|------------|--------------|\n",
    "| `LOAD_MODEL_PATH` | Path to pretrained `.pt` model (optional) |\n",
    "| `save_results` | Whether to write output CSV | `True` |\n",
    "\n",
    "---\n",
    "\n",
    "> üí° *Tip:* Adjust hyperparameters in the ‚ÄúConfiguration‚Äù or ‚ÄúTraining Setup‚Äù cell before running the notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BZLBQ6tJrcBB"
   },
   "source": [
    "<a id=\"setup\"></a>\n",
    "## 1. Setup & Configuration\n",
    "\n",
    "*Optional: Connect to Google Drive (for Colab users)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nig16xZNnmnz",
    "outputId": "9ed125ad-b3f2-42bf-bab2-a47c01e23134"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount(\"/gdrive\")\n",
    "# current_dir = \"/gdrive/My\\\\ Drive/[2025 - 2026]\\\\ AN2DL/Challenge 1/Personal Challenge 1\"\n",
    "# %cd $current_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZL1iYHipaeMD"
   },
   "source": [
    "*Set Model Type*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "uWaTlLkTKgk5"
   },
   "outputs": [],
   "source": [
    "RNN_TYPE = 'LSTM'            # 'RNN', 'LSTM', or 'GRU'\n",
    "BIDIRECTIONAL = True        # True / False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "up7Qo6v-o8Ru"
   },
   "source": [
    "*Set Model Save Name*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wkBnTJHuo8Rv",
    "outputId": "2a2110ee-6044-4a9b-a42f-8b4a9ccddb3f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment name: LSTM_bi_13-11-19-04\n",
      "Submission filename: LSTM_bi_13-11-19-04.csv\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Get current date and time for submission filename\n",
    "current_datetime = datetime.now().strftime(\"%d-%m-%H-%M\")\n",
    "\n",
    "if BIDIRECTIONAL:\n",
    "    EXPERIMENT_NAME = f\"{RNN_TYPE}_bi_{current_datetime}\"\n",
    "else:\n",
    "    EXPERIMENT_NAME = f\"{RNN_TYPE}_{current_datetime}\"\n",
    "\n",
    "SUBMISSION_FILENAME = f\"{EXPERIMENT_NAME}.csv\"\n",
    "print(f\"Experiment name: {EXPERIMENT_NAME}\")\n",
    "print(f\"Submission filename: {SUBMISSION_FILENAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VdaKCgHVvvHX"
   },
   "source": [
    "<a id=\"data-loading\"></a>\n",
    "## 2. Data Loading\n",
    "\n",
    "Load training and test datasets from CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "oLyI938Jvn-J"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "X_train = pd.read_csv('an2dl2526c1/pirate_pain_train.csv')\n",
    "y_train = pd.read_csv('an2dl2526c1/pirate_pain_train_labels.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p3Lre5NWwCyk"
   },
   "source": [
    "<a id=\"import-libraries\"></a>\n",
    "## 3. Import Libraries\n",
    "\n",
    "Set random seeds for reproducibility and import all necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nt3AnE8SwJg1",
    "outputId": "c14a149d-422f-440c-8432-e1e6d7e46435"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'pkill' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "A subdirectory or file -p already exists.\n",
      "Error occurred while processing: -p.\n",
      "A subdirectory or file models already exists.\n",
      "Error occurred while processing: models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.10.0.dev20251109+cu128\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Set seed for reproducibility\n",
    "SEED = 42\n",
    "\n",
    "# Import necessary libraries\n",
    "import os\n",
    "\n",
    "# Set environment variables before importing modules\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "os.environ['MPLCONFIGDIR'] = os.getcwd() + '/configs/'\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=Warning)\n",
    "\n",
    "# Import necessary modules\n",
    "import logging\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Set seeds for random number generators in NumPy and Python\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# Import PyTorch\n",
    "import torch\n",
    "torch.manual_seed(SEED)\n",
    "from torch import nn\n",
    "from torchsummary import summary\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Directory configuration\n",
    "logs_dir = \"tensorboard\"\n",
    "models_dir = \"models\"\n",
    "\n",
    "\n",
    "\n",
    "# Model save/load paths\n",
    "MODEL_SAVE_PATH = f\"{models_dir}/{EXPERIMENT_NAME}_model.pt\"\n",
    "MODEL_LOAD_PATH = f\"{models_dir}/{EXPERIMENT_NAME}_model.pt\"\n",
    "\n",
    "!pkill -f tensorboard\n",
    "%load_ext tensorboard\n",
    "!mkdir -p {models_dir}\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Import other libraries\n",
    "import copy\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "from itertools import product\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "\n",
    "# Configure plot display settings\n",
    "sns.set(font_scale=1.4)\n",
    "sns.set_style('white')\n",
    "plt.rc('font', size=14)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vWnQz-p-xyhD"
   },
   "source": [
    "<a id=\"data-preprocessing\"></a>\n",
    "## 4. Data Preprocessing\n",
    "\n",
    "Explore data, split into train/val/test sets, normalize features, and encode labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 603
    },
    "id": "hHK2Aw7Ix4S8",
    "outputId": "2f8b219c-ba90-4688-94e5-40a17786f4bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (105760, 40)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_index</th>\n",
       "      <th>time</th>\n",
       "      <th>pain_survey_1</th>\n",
       "      <th>pain_survey_2</th>\n",
       "      <th>pain_survey_3</th>\n",
       "      <th>pain_survey_4</th>\n",
       "      <th>n_legs</th>\n",
       "      <th>n_hands</th>\n",
       "      <th>n_eyes</th>\n",
       "      <th>joint_00</th>\n",
       "      <th>...</th>\n",
       "      <th>joint_21</th>\n",
       "      <th>joint_22</th>\n",
       "      <th>joint_23</th>\n",
       "      <th>joint_24</th>\n",
       "      <th>joint_25</th>\n",
       "      <th>joint_26</th>\n",
       "      <th>joint_27</th>\n",
       "      <th>joint_28</th>\n",
       "      <th>joint_29</th>\n",
       "      <th>joint_30</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>two</td>\n",
       "      <td>two</td>\n",
       "      <td>two</td>\n",
       "      <td>1.094705</td>\n",
       "      <td>...</td>\n",
       "      <td>3.499558e-06</td>\n",
       "      <td>1.945042e-06</td>\n",
       "      <td>3.999558e-06</td>\n",
       "      <td>1.153299e-05</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.017592</td>\n",
       "      <td>0.013508</td>\n",
       "      <td>0.026798</td>\n",
       "      <td>0.027815</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>two</td>\n",
       "      <td>two</td>\n",
       "      <td>two</td>\n",
       "      <td>1.135183</td>\n",
       "      <td>...</td>\n",
       "      <td>3.976952e-07</td>\n",
       "      <td>6.765107e-07</td>\n",
       "      <td>6.019627e-06</td>\n",
       "      <td>4.643774e-08</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013352</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013377</td>\n",
       "      <td>0.013716</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>two</td>\n",
       "      <td>two</td>\n",
       "      <td>two</td>\n",
       "      <td>1.080745</td>\n",
       "      <td>...</td>\n",
       "      <td>1.533820e-07</td>\n",
       "      <td>1.698525e-07</td>\n",
       "      <td>1.446051e-06</td>\n",
       "      <td>2.424536e-06</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.016225</td>\n",
       "      <td>0.008110</td>\n",
       "      <td>0.024097</td>\n",
       "      <td>0.023105</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>two</td>\n",
       "      <td>two</td>\n",
       "      <td>two</td>\n",
       "      <td>0.938017</td>\n",
       "      <td>...</td>\n",
       "      <td>1.006865e-05</td>\n",
       "      <td>5.511079e-07</td>\n",
       "      <td>1.847597e-06</td>\n",
       "      <td>5.432416e-08</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011832</td>\n",
       "      <td>0.007450</td>\n",
       "      <td>0.028613</td>\n",
       "      <td>0.024648</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>two</td>\n",
       "      <td>two</td>\n",
       "      <td>two</td>\n",
       "      <td>1.090185</td>\n",
       "      <td>...</td>\n",
       "      <td>4.437266e-06</td>\n",
       "      <td>1.735459e-07</td>\n",
       "      <td>1.552722e-06</td>\n",
       "      <td>5.825366e-08</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.005360</td>\n",
       "      <td>0.002532</td>\n",
       "      <td>0.033026</td>\n",
       "      <td>0.025328</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>two</td>\n",
       "      <td>two</td>\n",
       "      <td>two</td>\n",
       "      <td>1.146031</td>\n",
       "      <td>...</td>\n",
       "      <td>1.073167e-06</td>\n",
       "      <td>1.753837e-07</td>\n",
       "      <td>2.957340e-07</td>\n",
       "      <td>6.217311e-08</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.006150</td>\n",
       "      <td>0.006444</td>\n",
       "      <td>0.033101</td>\n",
       "      <td>0.023767</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>two</td>\n",
       "      <td>two</td>\n",
       "      <td>two</td>\n",
       "      <td>1.025870</td>\n",
       "      <td>...</td>\n",
       "      <td>1.074800e-06</td>\n",
       "      <td>1.772156e-07</td>\n",
       "      <td>1.976558e-06</td>\n",
       "      <td>1.576086e-06</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.006495</td>\n",
       "      <td>0.006421</td>\n",
       "      <td>0.031804</td>\n",
       "      <td>0.019056</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>two</td>\n",
       "      <td>two</td>\n",
       "      <td>two</td>\n",
       "      <td>1.038597</td>\n",
       "      <td>...</td>\n",
       "      <td>8.829074e-07</td>\n",
       "      <td>1.790415e-07</td>\n",
       "      <td>2.210562e-06</td>\n",
       "      <td>1.485741e-06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015998</td>\n",
       "      <td>0.005397</td>\n",
       "      <td>0.035552</td>\n",
       "      <td>0.015732</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>two</td>\n",
       "      <td>two</td>\n",
       "      <td>two</td>\n",
       "      <td>0.984251</td>\n",
       "      <td>...</td>\n",
       "      <td>1.621055e-06</td>\n",
       "      <td>1.165161e-06</td>\n",
       "      <td>3.030164e-07</td>\n",
       "      <td>5.416678e-07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020539</td>\n",
       "      <td>0.008517</td>\n",
       "      <td>0.008635</td>\n",
       "      <td>0.015257</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>two</td>\n",
       "      <td>two</td>\n",
       "      <td>two</td>\n",
       "      <td>1.054999</td>\n",
       "      <td>...</td>\n",
       "      <td>1.609114e-06</td>\n",
       "      <td>3.959558e-06</td>\n",
       "      <td>2.017157e-06</td>\n",
       "      <td>1.154349e-06</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.007682</td>\n",
       "      <td>0.021383</td>\n",
       "      <td>0.034006</td>\n",
       "      <td>0.028966</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows √ó 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   sample_index  time  pain_survey_1  pain_survey_2  pain_survey_3  \\\n",
       "0             0     0              2              0              2   \n",
       "1             0     1              2              2              2   \n",
       "2             0     2              2              0              2   \n",
       "3             0     3              2              2              2   \n",
       "4             0     4              2              2              2   \n",
       "5             0     5              2              0              2   \n",
       "6             0     6              2              1              2   \n",
       "7             0     7              2              2              2   \n",
       "8             0     8              2              2              0   \n",
       "9             0     9              0              2              2   \n",
       "\n",
       "   pain_survey_4 n_legs n_hands n_eyes  joint_00  ...      joint_21  \\\n",
       "0              1    two     two    two  1.094705  ...  3.499558e-06   \n",
       "1              2    two     two    two  1.135183  ...  3.976952e-07   \n",
       "2              2    two     two    two  1.080745  ...  1.533820e-07   \n",
       "3              2    two     two    two  0.938017  ...  1.006865e-05   \n",
       "4              2    two     two    two  1.090185  ...  4.437266e-06   \n",
       "5              1    two     two    two  1.146031  ...  1.073167e-06   \n",
       "6              1    two     two    two  1.025870  ...  1.074800e-06   \n",
       "7              2    two     two    two  1.038597  ...  8.829074e-07   \n",
       "8              1    two     two    two  0.984251  ...  1.621055e-06   \n",
       "9              2    two     two    two  1.054999  ...  1.609114e-06   \n",
       "\n",
       "       joint_22      joint_23      joint_24  joint_25  joint_26  joint_27  \\\n",
       "0  1.945042e-06  3.999558e-06  1.153299e-05  0.000004  0.017592  0.013508   \n",
       "1  6.765107e-07  6.019627e-06  4.643774e-08  0.000000  0.013352  0.000000   \n",
       "2  1.698525e-07  1.446051e-06  2.424536e-06  0.000003  0.016225  0.008110   \n",
       "3  5.511079e-07  1.847597e-06  5.432416e-08  0.000000  0.011832  0.007450   \n",
       "4  1.735459e-07  1.552722e-06  5.825366e-08  0.000007  0.005360  0.002532   \n",
       "5  1.753837e-07  2.957340e-07  6.217311e-08  0.000007  0.006150  0.006444   \n",
       "6  1.772156e-07  1.976558e-06  1.576086e-06  0.000005  0.006495  0.006421   \n",
       "7  1.790415e-07  2.210562e-06  1.485741e-06  0.000000  0.015998  0.005397   \n",
       "8  1.165161e-06  3.030164e-07  5.416678e-07  0.000000  0.020539  0.008517   \n",
       "9  3.959558e-06  2.017157e-06  1.154349e-06  0.000007  0.007682  0.021383   \n",
       "\n",
       "   joint_28  joint_29  joint_30  \n",
       "0  0.026798  0.027815       0.5  \n",
       "1  0.013377  0.013716       0.5  \n",
       "2  0.024097  0.023105       0.5  \n",
       "3  0.028613  0.024648       0.5  \n",
       "4  0.033026  0.025328       0.5  \n",
       "5  0.033101  0.023767       0.5  \n",
       "6  0.031804  0.019056       0.5  \n",
       "7  0.035552  0.015732       0.5  \n",
       "8  0.008635  0.015257       0.5  \n",
       "9  0.034006  0.028966       0.5  \n",
       "\n",
       "[10 rows x 40 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the shape of the dataset\n",
    "print(f\"Dataset shape: {X_train.shape}\")\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "X_train.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GPMtvy5Fo8Rw"
   },
   "source": [
    "### 4.1 Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LfBKIdsrQDW3",
    "outputId": "eccea43a-a861-4fde-eb3c-e75ad62dcc47"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_legs': ['two', 'one+peg_leg'],\n",
       " 'n_hands': ['two', 'one+hook_hand'],\n",
       " 'n_eyes': ['two', 'one+eye_patch']}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Merge features and labels\n",
    "data = X_train.merge(y_train, on='sample_index')\n",
    "\n",
    "# Create a mapping dictionary to convert categorical labels to numerical values\n",
    "# map_dict = {'none': 0, 'one': 1, 'two': 2}\n",
    "# data['n_legs'] = data['n_legs'].map(map_dict)\n",
    "# data['n_hands'] = data['n_hands'].map(map_dict)\n",
    "# data['n_eyes'] = data['n_eyes'].map(map_dict)\n",
    "\n",
    "# print(\"Loading test dataset for final evaluation...\")\n",
    "\n",
    "cols = ['n_legs', 'n_hands', 'n_eyes']\n",
    "unique_values = {col: X_train[col].unique().tolist() for col in cols}\n",
    "\n",
    "unique_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wN-gWBGzuxfE",
    "outputId": "cbd124a1-e798-4357-ad4c-7e6b8e1fc0c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapped string columns to numeric values!\n",
      "   sample_index  time  pain_survey_1  pain_survey_2  pain_survey_3  \\\n",
      "0             0     0              2              0              2   \n",
      "1             0     1              2              2              2   \n",
      "2             0     2              2              0              2   \n",
      "3             0     3              2              2              2   \n",
      "4             0     4              2              2              2   \n",
      "\n",
      "   pain_survey_4  n_legs  n_hands  n_eyes  joint_00  ...      joint_22  \\\n",
      "0              1       2        2       2  1.094705  ...  1.945042e-06   \n",
      "1              2       2        2       2  1.135183  ...  6.765107e-07   \n",
      "2              2       2        2       2  1.080745  ...  1.698525e-07   \n",
      "3              2       2        2       2  0.938017  ...  5.511079e-07   \n",
      "4              2       2        2       2  1.090185  ...  1.735459e-07   \n",
      "\n",
      "   joint_23      joint_24  joint_25  joint_26  joint_27  joint_28  joint_29  \\\n",
      "0  0.000004  1.153299e-05  0.000004  0.017592  0.013508  0.026798  0.027815   \n",
      "1  0.000006  4.643774e-08  0.000000  0.013352  0.000000  0.013377  0.013716   \n",
      "2  0.000001  2.424536e-06  0.000003  0.016225  0.008110  0.024097  0.023105   \n",
      "3  0.000002  5.432416e-08  0.000000  0.011832  0.007450  0.028613  0.024648   \n",
      "4  0.000002  5.825366e-08  0.000007  0.005360  0.002532  0.033026  0.025328   \n",
      "\n",
      "   joint_30    label  \n",
      "0       0.5  no_pain  \n",
      "1       0.5  no_pain  \n",
      "2       0.5  no_pain  \n",
      "3       0.5  no_pain  \n",
      "4       0.5  no_pain  \n",
      "\n",
      "[5 rows x 41 columns]\n"
     ]
    }
   ],
   "source": [
    "map_dict = {'two': 2, 'one+peg_leg': 1}\n",
    "data['n_legs'] = data['n_legs'].map(map_dict)\n",
    "\n",
    "map_dict = {'two': 2, 'one+hook_hand': 1}\n",
    "data['n_hands'] = data['n_hands'].map(map_dict)\n",
    "\n",
    "map_dict = {'two': 2, 'one+eye_patch': 1}\n",
    "data['n_eyes'] = data['n_eyes'].map(map_dict)\n",
    "\n",
    "print(\"Mapped string columns to numeric values!\")\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 349
    },
    "id": "yiX54ULeXQdV",
    "outputId": "e878a7ba-ef8a-452c-99d9-ae0dda7bf952"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_index</th>\n",
       "      <th>time</th>\n",
       "      <th>pain_survey_1</th>\n",
       "      <th>pain_survey_2</th>\n",
       "      <th>pain_survey_3</th>\n",
       "      <th>pain_survey_4</th>\n",
       "      <th>n_legs</th>\n",
       "      <th>n_hands</th>\n",
       "      <th>n_eyes</th>\n",
       "      <th>joint_00</th>\n",
       "      <th>...</th>\n",
       "      <th>joint_21</th>\n",
       "      <th>joint_22</th>\n",
       "      <th>joint_23</th>\n",
       "      <th>joint_24</th>\n",
       "      <th>joint_25</th>\n",
       "      <th>joint_26</th>\n",
       "      <th>joint_27</th>\n",
       "      <th>joint_28</th>\n",
       "      <th>joint_29</th>\n",
       "      <th>joint_30</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>105760.000000</td>\n",
       "      <td>105760.000000</td>\n",
       "      <td>105760.000000</td>\n",
       "      <td>105760.000000</td>\n",
       "      <td>105760.000000</td>\n",
       "      <td>105760.000000</td>\n",
       "      <td>105760.000000</td>\n",
       "      <td>105760.000000</td>\n",
       "      <td>105760.000000</td>\n",
       "      <td>105760.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.057600e+05</td>\n",
       "      <td>1.057600e+05</td>\n",
       "      <td>1.057600e+05</td>\n",
       "      <td>1.057600e+05</td>\n",
       "      <td>1.057600e+05</td>\n",
       "      <td>105760.000000</td>\n",
       "      <td>105760.000000</td>\n",
       "      <td>105760.000000</td>\n",
       "      <td>105760.000000</td>\n",
       "      <td>105760.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>330.000000</td>\n",
       "      <td>79.500000</td>\n",
       "      <td>1.633746</td>\n",
       "      <td>1.654851</td>\n",
       "      <td>1.653640</td>\n",
       "      <td>1.663134</td>\n",
       "      <td>1.990923</td>\n",
       "      <td>1.990923</td>\n",
       "      <td>1.990923</td>\n",
       "      <td>0.943095</td>\n",
       "      <td>...</td>\n",
       "      <td>3.972126e-05</td>\n",
       "      <td>4.176794e-05</td>\n",
       "      <td>3.561780e-05</td>\n",
       "      <td>3.138109e-05</td>\n",
       "      <td>1.024604e-04</td>\n",
       "      <td>0.041905</td>\n",
       "      <td>0.058244</td>\n",
       "      <td>0.049886</td>\n",
       "      <td>0.062273</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>190.814948</td>\n",
       "      <td>46.187338</td>\n",
       "      <td>0.682423</td>\n",
       "      <td>0.669639</td>\n",
       "      <td>0.666649</td>\n",
       "      <td>0.661994</td>\n",
       "      <td>0.094841</td>\n",
       "      <td>0.094841</td>\n",
       "      <td>0.094841</td>\n",
       "      <td>0.202051</td>\n",
       "      <td>...</td>\n",
       "      <td>4.974496e-03</td>\n",
       "      <td>5.472244e-03</td>\n",
       "      <td>1.235450e-03</td>\n",
       "      <td>4.062914e-04</td>\n",
       "      <td>3.206128e-03</td>\n",
       "      <td>0.060293</td>\n",
       "      <td>0.079819</td>\n",
       "      <td>0.060773</td>\n",
       "      <td>0.072597</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.510494e-07</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.063144e-08</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000203</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>165.000000</td>\n",
       "      <td>39.750000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.828277</td>\n",
       "      <td>...</td>\n",
       "      <td>6.545878e-08</td>\n",
       "      <td>3.321650e-07</td>\n",
       "      <td>3.275038e-07</td>\n",
       "      <td>2.841805e-07</td>\n",
       "      <td>7.161332e-07</td>\n",
       "      <td>0.009885</td>\n",
       "      <td>0.012652</td>\n",
       "      <td>0.016290</td>\n",
       "      <td>0.019638</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>330.000000</td>\n",
       "      <td>79.500000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.005126</td>\n",
       "      <td>...</td>\n",
       "      <td>8.302747e-07</td>\n",
       "      <td>1.095971e-06</td>\n",
       "      <td>1.024209e-06</td>\n",
       "      <td>8.746147e-07</td>\n",
       "      <td>3.126723e-06</td>\n",
       "      <td>0.021898</td>\n",
       "      <td>0.031739</td>\n",
       "      <td>0.031843</td>\n",
       "      <td>0.039041</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>495.000000</td>\n",
       "      <td>119.250000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.081039</td>\n",
       "      <td>...</td>\n",
       "      <td>2.800090e-06</td>\n",
       "      <td>3.079465e-06</td>\n",
       "      <td>3.021830e-06</td>\n",
       "      <td>2.507548e-06</td>\n",
       "      <td>9.946107e-06</td>\n",
       "      <td>0.048579</td>\n",
       "      <td>0.071051</td>\n",
       "      <td>0.058741</td>\n",
       "      <td>0.079518</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>660.000000</td>\n",
       "      <td>159.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.407968</td>\n",
       "      <td>...</td>\n",
       "      <td>1.442198e+00</td>\n",
       "      <td>1.305001e+00</td>\n",
       "      <td>2.742411e-01</td>\n",
       "      <td>3.643074e-02</td>\n",
       "      <td>9.473540e-01</td>\n",
       "      <td>1.223617</td>\n",
       "      <td>1.187419</td>\n",
       "      <td>1.412037</td>\n",
       "      <td>1.370765</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows √ó 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        sample_index           time  pain_survey_1  pain_survey_2  \\\n",
       "count  105760.000000  105760.000000  105760.000000  105760.000000   \n",
       "mean      330.000000      79.500000       1.633746       1.654851   \n",
       "std       190.814948      46.187338       0.682423       0.669639   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%       165.000000      39.750000       2.000000       2.000000   \n",
       "50%       330.000000      79.500000       2.000000       2.000000   \n",
       "75%       495.000000     119.250000       2.000000       2.000000   \n",
       "max       660.000000     159.000000       2.000000       2.000000   \n",
       "\n",
       "       pain_survey_3  pain_survey_4         n_legs        n_hands  \\\n",
       "count  105760.000000  105760.000000  105760.000000  105760.000000   \n",
       "mean        1.653640       1.663134       1.990923       1.990923   \n",
       "std         0.666649       0.661994       0.094841       0.094841   \n",
       "min         0.000000       0.000000       1.000000       1.000000   \n",
       "25%         2.000000       2.000000       2.000000       2.000000   \n",
       "50%         2.000000       2.000000       2.000000       2.000000   \n",
       "75%         2.000000       2.000000       2.000000       2.000000   \n",
       "max         2.000000       2.000000       2.000000       2.000000   \n",
       "\n",
       "              n_eyes       joint_00  ...      joint_21      joint_22  \\\n",
       "count  105760.000000  105760.000000  ...  1.057600e+05  1.057600e+05   \n",
       "mean        1.990923       0.943095  ...  3.972126e-05  4.176794e-05   \n",
       "std         0.094841       0.202051  ...  4.974496e-03  5.472244e-03   \n",
       "min         1.000000       0.000000  ...  0.000000e+00  1.510494e-07   \n",
       "25%         2.000000       0.828277  ...  6.545878e-08  3.321650e-07   \n",
       "50%         2.000000       1.005126  ...  8.302747e-07  1.095971e-06   \n",
       "75%         2.000000       1.081039  ...  2.800090e-06  3.079465e-06   \n",
       "max         2.000000       1.407968  ...  1.442198e+00  1.305001e+00   \n",
       "\n",
       "           joint_23      joint_24      joint_25       joint_26       joint_27  \\\n",
       "count  1.057600e+05  1.057600e+05  1.057600e+05  105760.000000  105760.000000   \n",
       "mean   3.561780e-05  3.138109e-05  1.024604e-04       0.041905       0.058244   \n",
       "std    1.235450e-03  4.062914e-04  3.206128e-03       0.060293       0.079819   \n",
       "min    0.000000e+00  1.063144e-08  0.000000e+00       0.000203       0.000000   \n",
       "25%    3.275038e-07  2.841805e-07  7.161332e-07       0.009885       0.012652   \n",
       "50%    1.024209e-06  8.746147e-07  3.126723e-06       0.021898       0.031739   \n",
       "75%    3.021830e-06  2.507548e-06  9.946107e-06       0.048579       0.071051   \n",
       "max    2.742411e-01  3.643074e-02  9.473540e-01       1.223617       1.187419   \n",
       "\n",
       "            joint_28       joint_29  joint_30  \n",
       "count  105760.000000  105760.000000  105760.0  \n",
       "mean        0.049886       0.062273       0.5  \n",
       "std         0.060773       0.072597       0.0  \n",
       "min         0.000000       0.000000       0.5  \n",
       "25%         0.016290       0.019638       0.5  \n",
       "50%         0.031843       0.039041       0.5  \n",
       "75%         0.058741       0.079518       0.5  \n",
       "max         1.412037       1.370765       0.5  \n",
       "\n",
       "[8 rows x 40 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "qx24EFjkGb4o"
   },
   "outputs": [],
   "source": [
    "list_to_remove = ['n_legs', 'n_hands', 'n_eyes', 'joint_13', 'joint_14', 'joint_15', 'joint_16', 'joint_17', 'joint_18', 'joint_19', 'joint_20', 'joint_21', 'joint_22', 'joint_23', 'joint_24', 'joint_25', 'joint_30']\n",
    "\n",
    "if data.columns.isin(list_to_remove).any():\n",
    "  data = data.drop(columns=['n_legs', 'n_hands', 'n_eyes', 'joint_13', 'joint_14', 'joint_15', 'joint_16', 'joint_17', 'joint_18', 'joint_19', 'joint_20', 'joint_21', 'joint_22', 'joint_23', 'joint_24', 'joint_25', 'joint_30'])\n",
    "  data.head()\n",
    "else:\n",
    "  print(\"Usless features already removed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continuous features: 17\n",
      "Categorical features: 4\n"
     ]
    }
   ],
   "source": [
    "# Count the continouse and categorical features\n",
    "\n",
    "continuous_cols = [col for col in data.columns if col.startswith('joint_')]\n",
    "categorical_cols = [col for col in data.columns if col.startswith('pain_survey_')]\n",
    "\n",
    "print(f\"Continuous features: {len(continuous_cols)}\")\n",
    "print(f\"Categorical features: {len(categorical_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns after removal:\n",
      "['sample_index', 'time', 'pain_survey_1', 'pain_survey_2', 'pain_survey_3', 'pain_survey_4', 'joint_00', 'joint_01', 'joint_02', 'joint_03', 'joint_04', 'joint_05', 'joint_06', 'joint_07', 'joint_08', 'joint_09', 'joint_10', 'joint_11', 'joint_12', 'joint_26', 'joint_27', 'joint_28', 'joint_29', 'label']\n",
      "\n",
      "Number of features remaining: 24\n",
      "\n",
      "Categorical features identified: ['pain_survey_1', 'pain_survey_2', 'pain_survey_3', 'pain_survey_4']\n",
      "Remaining joint features: ['joint_00', 'joint_01', 'joint_02', 'joint_03', 'joint_04', 'joint_05', 'joint_06', 'joint_07', 'joint_08', 'joint_09', 'joint_10', 'joint_11', 'joint_12', 'joint_26', 'joint_27', 'joint_28', 'joint_29']\n",
      "Number of joint features: 17\n"
     ]
    }
   ],
   "source": [
    "# Check what columns remain after removal\n",
    "print(\"Columns after removal:\")\n",
    "print(data.columns.tolist())\n",
    "print(f\"\\nNumber of features remaining: {len(data.columns)}\")\n",
    "\n",
    "# Identify categorical features (int64 pain_survey columns)\n",
    "categorical_features = []\n",
    "for col in data.columns:\n",
    "    if 'pain_survey' in col and data[col].dtype == 'int64':\n",
    "        categorical_features.append(col)\n",
    "\n",
    "print(f\"\\nCategorical features identified: {categorical_features}\")\n",
    "\n",
    "# Check the remaining joint features\n",
    "joint_features = [col for col in data.columns if col.startswith('joint_')]\n",
    "print(f\"Remaining joint features: {joint_features}\")\n",
    "print(f\"Number of joint features: {len(joint_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding dimensions: {'pain_survey_1': 2, 'pain_survey_2': 2, 'pain_survey_3': 2, 'pain_survey_4': 2}\n",
      "Total embedding output size: 8\n",
      "Continuous features (joints): 17\n",
      "Total input size to LSTM: 25\n"
     ]
    }
   ],
   "source": [
    "# Define embedding dimensions for categorical features\n",
    "# For 3 unique values (0, 1, 2), embedding dimension of 2 is reasonable\n",
    "embedding_dims = {\n",
    "    'pain_survey_1': 2,\n",
    "    'pain_survey_2': 2,\n",
    "    'pain_survey_3': 2,\n",
    "    'pain_survey_4': 2\n",
    "}\n",
    "\n",
    "print(f\"Embedding dimensions: {embedding_dims}\")\n",
    "print(f\"Total embedding output size: {sum(embedding_dims.values())}\")\n",
    "print(f\"Continuous features (joints): {len(joint_features)}\")\n",
    "print(f\"Total input size to LSTM: {sum(embedding_dims.values()) + len(joint_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "id": "6pqnMiAQHmlK",
    "outputId": "8c382c77-2a6c-4878-91cf-6e0db2036d78"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_index</th>\n",
       "      <th>time</th>\n",
       "      <th>pain_survey_1</th>\n",
       "      <th>pain_survey_2</th>\n",
       "      <th>pain_survey_3</th>\n",
       "      <th>pain_survey_4</th>\n",
       "      <th>joint_00</th>\n",
       "      <th>joint_01</th>\n",
       "      <th>joint_02</th>\n",
       "      <th>joint_03</th>\n",
       "      <th>...</th>\n",
       "      <th>joint_08</th>\n",
       "      <th>joint_09</th>\n",
       "      <th>joint_10</th>\n",
       "      <th>joint_11</th>\n",
       "      <th>joint_12</th>\n",
       "      <th>joint_26</th>\n",
       "      <th>joint_27</th>\n",
       "      <th>joint_28</th>\n",
       "      <th>joint_29</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.094705</td>\n",
       "      <td>0.985281</td>\n",
       "      <td>1.018302</td>\n",
       "      <td>1.010385</td>\n",
       "      <td>...</td>\n",
       "      <td>0.712989</td>\n",
       "      <td>1.050142</td>\n",
       "      <td>0.529555</td>\n",
       "      <td>0.447370</td>\n",
       "      <td>1.091046</td>\n",
       "      <td>0.017592</td>\n",
       "      <td>0.013508</td>\n",
       "      <td>0.026798</td>\n",
       "      <td>0.027815</td>\n",
       "      <td>no_pain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.135183</td>\n",
       "      <td>1.021175</td>\n",
       "      <td>0.994343</td>\n",
       "      <td>1.052364</td>\n",
       "      <td>...</td>\n",
       "      <td>0.722685</td>\n",
       "      <td>1.060313</td>\n",
       "      <td>0.446810</td>\n",
       "      <td>0.414432</td>\n",
       "      <td>1.045862</td>\n",
       "      <td>0.013352</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013377</td>\n",
       "      <td>0.013716</td>\n",
       "      <td>no_pain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.080745</td>\n",
       "      <td>0.962842</td>\n",
       "      <td>1.009588</td>\n",
       "      <td>0.977169</td>\n",
       "      <td>...</td>\n",
       "      <td>0.668043</td>\n",
       "      <td>1.011410</td>\n",
       "      <td>0.432499</td>\n",
       "      <td>0.431535</td>\n",
       "      <td>1.088221</td>\n",
       "      <td>0.016225</td>\n",
       "      <td>0.008110</td>\n",
       "      <td>0.024097</td>\n",
       "      <td>0.023105</td>\n",
       "      <td>no_pain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.938017</td>\n",
       "      <td>1.081592</td>\n",
       "      <td>0.998021</td>\n",
       "      <td>0.987283</td>\n",
       "      <td>...</td>\n",
       "      <td>0.702085</td>\n",
       "      <td>1.047223</td>\n",
       "      <td>0.478806</td>\n",
       "      <td>0.420665</td>\n",
       "      <td>1.096832</td>\n",
       "      <td>0.011832</td>\n",
       "      <td>0.007450</td>\n",
       "      <td>0.028613</td>\n",
       "      <td>0.024648</td>\n",
       "      <td>no_pain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.090185</td>\n",
       "      <td>1.032145</td>\n",
       "      <td>1.008710</td>\n",
       "      <td>0.963658</td>\n",
       "      <td>...</td>\n",
       "      <td>0.712197</td>\n",
       "      <td>1.044731</td>\n",
       "      <td>0.452906</td>\n",
       "      <td>0.476537</td>\n",
       "      <td>1.103968</td>\n",
       "      <td>0.005360</td>\n",
       "      <td>0.002532</td>\n",
       "      <td>0.033026</td>\n",
       "      <td>0.025328</td>\n",
       "      <td>no_pain</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   sample_index  time  pain_survey_1  pain_survey_2  pain_survey_3  \\\n",
       "0             0     0              2              0              2   \n",
       "1             0     1              2              2              2   \n",
       "2             0     2              2              0              2   \n",
       "3             0     3              2              2              2   \n",
       "4             0     4              2              2              2   \n",
       "\n",
       "   pain_survey_4  joint_00  joint_01  joint_02  joint_03  ...  joint_08  \\\n",
       "0              1  1.094705  0.985281  1.018302  1.010385  ...  0.712989   \n",
       "1              2  1.135183  1.021175  0.994343  1.052364  ...  0.722685   \n",
       "2              2  1.080745  0.962842  1.009588  0.977169  ...  0.668043   \n",
       "3              2  0.938017  1.081592  0.998021  0.987283  ...  0.702085   \n",
       "4              2  1.090185  1.032145  1.008710  0.963658  ...  0.712197   \n",
       "\n",
       "   joint_09  joint_10  joint_11  joint_12  joint_26  joint_27  joint_28  \\\n",
       "0  1.050142  0.529555  0.447370  1.091046  0.017592  0.013508  0.026798   \n",
       "1  1.060313  0.446810  0.414432  1.045862  0.013352  0.000000  0.013377   \n",
       "2  1.011410  0.432499  0.431535  1.088221  0.016225  0.008110  0.024097   \n",
       "3  1.047223  0.478806  0.420665  1.096832  0.011832  0.007450  0.028613   \n",
       "4  1.044731  0.452906  0.476537  1.103968  0.005360  0.002532  0.033026   \n",
       "\n",
       "   joint_29    label  \n",
       "0  0.027815  no_pain  \n",
       "1  0.013716  no_pain  \n",
       "2  0.023105  no_pain  \n",
       "3  0.024648  no_pain  \n",
       "4  0.025328  no_pain  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in data:\n",
      "['sample_index', 'time', 'pain_survey_1', 'pain_survey_2', 'pain_survey_3', 'pain_survey_4', 'joint_00', 'joint_01', 'joint_02', 'joint_03', 'joint_04', 'joint_05', 'joint_06', 'joint_07', 'joint_08', 'joint_09', 'joint_10', 'joint_11', 'joint_12', 'joint_26', 'joint_27', 'joint_28', 'joint_29', 'label']\n",
      "\n",
      "Data types:\n",
      "sample_index       int64\n",
      "time               int64\n",
      "pain_survey_1      int64\n",
      "pain_survey_2      int64\n",
      "pain_survey_3      int64\n",
      "pain_survey_4      int64\n",
      "joint_00         float64\n",
      "joint_01         float64\n",
      "joint_02         float64\n",
      "joint_03         float64\n",
      "joint_04         float64\n",
      "joint_05         float64\n",
      "joint_06         float64\n",
      "joint_07         float64\n",
      "joint_08         float64\n",
      "joint_09         float64\n",
      "joint_10         float64\n",
      "joint_11         float64\n",
      "joint_12         float64\n",
      "joint_26         float64\n",
      "joint_27         float64\n",
      "joint_28         float64\n",
      "joint_29         float64\n",
      "label             object\n",
      "dtype: object\n",
      "\n",
      "Pain survey columns: ['pain_survey_1', 'pain_survey_2', 'pain_survey_3', 'pain_survey_4']\n",
      "\n",
      "Unique values in pain_survey_1:\n",
      "[2 0 1]\n",
      "Number of unique values: 3\n",
      "\n",
      "Unique values in pain_survey_2:\n",
      "[0 2 1]\n",
      "Number of unique values: 3\n",
      "\n",
      "Unique values in pain_survey_3:\n",
      "[2 0 1]\n",
      "Number of unique values: 3\n",
      "\n",
      "Unique values in pain_survey_4:\n",
      "[1 2 0]\n",
      "Number of unique values: 3\n"
     ]
    }
   ],
   "source": [
    "# Check the columns and their types\n",
    "print(\"Columns in data:\")\n",
    "print(data.columns.tolist())\n",
    "print(\"\\nData types:\")\n",
    "print(data.dtypes)\n",
    "\n",
    "# Check for categorical features (pain_survey columns)\n",
    "pain_survey_cols = [col for col in data.columns if 'pain_survey' in col]\n",
    "print(f\"\\nPain survey columns: {pain_survey_cols}\")\n",
    "\n",
    "# Check unique values in pain_survey columns\n",
    "for col in pain_survey_cols:\n",
    "    print(f\"\\nUnique values in {col}:\")\n",
    "    print(data[col].unique())\n",
    "    print(f\"Number of unique values: {data[col].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ojlNM9r8u8x8"
   },
   "source": [
    "## Preprocessing testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tBEcYysvu8A9",
    "outputId": "70da8c96-e590-4aeb-886a-1ffebd59d8bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapped string columns to numeric values!\n",
      "   sample_index  time  pain_survey_1  pain_survey_2  pain_survey_3  \\\n",
      "0             0     0              2              2              2   \n",
      "1             0     1              2              2              2   \n",
      "2             0     2              2              2              2   \n",
      "3             0     3              1              2              2   \n",
      "4             0     4              2              2              2   \n",
      "\n",
      "   pain_survey_4  n_legs  n_hands  n_eyes  joint_00  ...  joint_21  joint_22  \\\n",
      "0              2       2        2       2  0.842535  ...  0.000003  0.000004   \n",
      "1              2       2        2       2  0.898836  ...  0.000003  0.000004   \n",
      "2              2       2        2       2  0.957765  ...  0.000006  0.000004   \n",
      "3              2       2        2       2  0.832596  ...  0.000005  0.000004   \n",
      "4              0       2        2       2  0.805971  ...  0.000006  0.000004   \n",
      "\n",
      "   joint_23  joint_24  joint_25  joint_26  joint_27  joint_28  joint_29  \\\n",
      "0  0.000003  0.000003  0.000068  0.019372  0.066324  0.022228  0.013576   \n",
      "1  0.000004  0.000003  0.000029  0.069747  0.080417  0.023650  0.038793   \n",
      "2  0.000009  0.000004  0.000008  0.054968  0.058811  0.027023  0.054202   \n",
      "3  0.000003  0.000004  0.000015  0.048695  0.047128  0.016151  0.024983   \n",
      "4  0.000003  0.000003  0.000008  0.019762  0.031116  0.015618  0.017931   \n",
      "\n",
      "   joint_30  \n",
      "0       0.5  \n",
      "1       0.5  \n",
      "2       0.5  \n",
      "3       0.5  \n",
      "4       0.5  \n",
      "\n",
      "[5 rows x 40 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load the actual test dataset (this doesn't have labels)\n",
    "X_test_final_df = pd.read_csv('an2dl2526c1/pirate_pain_test.csv')\n",
    "\n",
    "# Map string columns to numeric values first\n",
    "map_dict = {'two': 2, 'one+peg_leg': 1}\n",
    "X_test_final_df['n_legs'] = X_test_final_df['n_legs'].map(map_dict)\n",
    "\n",
    "map_dict = {'two': 2, 'one+hook_hand': 1}\n",
    "X_test_final_df['n_hands'] = X_test_final_df['n_hands'].map(map_dict)\n",
    "\n",
    "map_dict = {'two': 2, 'one+eye_patch': 1}\n",
    "X_test_final_df['n_eyes'] = X_test_final_df['n_eyes'].map(map_dict)\n",
    "\n",
    "print(\"Mapped string columns to numeric values!\")\n",
    "print(X_test_final_df.head())\n",
    "\n",
    "# Now convert inputs from float64 to float32\n",
    "X_test_final_df = X_test_final_df.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "OoKAZSkvvEam"
   },
   "outputs": [],
   "source": [
    "def build_sequences_test(df, window=200, stride=200):\n",
    "    assert window % stride == 0\n",
    "\n",
    "    dataset = []\n",
    "\n",
    "    # Get feature columns (exclude sample_index and time)\n",
    "    columns = [col for col in df.columns if col not in ['sample_index', 'time']]\n",
    "\n",
    "    for id in df['sample_index'].unique():\n",
    "        temp = df[df['sample_index'] == id][columns].values\n",
    "\n",
    "        # Padding\n",
    "        padding_len = (window - len(temp) % window) % window\n",
    "        padding = np.zeros((padding_len, len(columns)), dtype='float32')\n",
    "        temp = np.concatenate((temp, padding))\n",
    "\n",
    "        # Build windows\n",
    "        idx = 0\n",
    "        while idx + window <= len(temp):\n",
    "            dataset.append(temp[idx:idx + window])\n",
    "            idx += stride\n",
    "\n",
    "    return np.array(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "id": "EEu1WYIgvMvl",
    "outputId": "738c6bf8-cb8b-473c-f017-58efdd1323de"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_index</th>\n",
       "      <th>time</th>\n",
       "      <th>pain_survey_1</th>\n",
       "      <th>pain_survey_2</th>\n",
       "      <th>pain_survey_3</th>\n",
       "      <th>pain_survey_4</th>\n",
       "      <th>joint_00</th>\n",
       "      <th>joint_01</th>\n",
       "      <th>joint_02</th>\n",
       "      <th>joint_03</th>\n",
       "      <th>...</th>\n",
       "      <th>joint_07</th>\n",
       "      <th>joint_08</th>\n",
       "      <th>joint_09</th>\n",
       "      <th>joint_10</th>\n",
       "      <th>joint_11</th>\n",
       "      <th>joint_12</th>\n",
       "      <th>joint_26</th>\n",
       "      <th>joint_27</th>\n",
       "      <th>joint_28</th>\n",
       "      <th>joint_29</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.561563</td>\n",
       "      <td>0.553352</td>\n",
       "      <td>0.419037</td>\n",
       "      <td>0.270175</td>\n",
       "      <td>...</td>\n",
       "      <td>0.654303</td>\n",
       "      <td>0.737832</td>\n",
       "      <td>0.742275</td>\n",
       "      <td>0.100076</td>\n",
       "      <td>0.146564</td>\n",
       "      <td>0.745300</td>\n",
       "      <td>0.014909</td>\n",
       "      <td>0.045098</td>\n",
       "      <td>0.012882</td>\n",
       "      <td>0.010178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.599088</td>\n",
       "      <td>0.532067</td>\n",
       "      <td>0.461325</td>\n",
       "      <td>0.327922</td>\n",
       "      <td>...</td>\n",
       "      <td>0.684443</td>\n",
       "      <td>0.772454</td>\n",
       "      <td>0.710705</td>\n",
       "      <td>0.103457</td>\n",
       "      <td>0.174403</td>\n",
       "      <td>0.594262</td>\n",
       "      <td>0.053679</td>\n",
       "      <td>0.055375</td>\n",
       "      <td>0.013892</td>\n",
       "      <td>0.029085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.638365</td>\n",
       "      <td>0.583960</td>\n",
       "      <td>0.445804</td>\n",
       "      <td>0.308796</td>\n",
       "      <td>...</td>\n",
       "      <td>0.676488</td>\n",
       "      <td>0.799646</td>\n",
       "      <td>0.722061</td>\n",
       "      <td>0.143175</td>\n",
       "      <td>0.159973</td>\n",
       "      <td>0.652024</td>\n",
       "      <td>0.042305</td>\n",
       "      <td>0.039620</td>\n",
       "      <td>0.016286</td>\n",
       "      <td>0.040638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.554938</td>\n",
       "      <td>0.488719</td>\n",
       "      <td>0.443494</td>\n",
       "      <td>0.355023</td>\n",
       "      <td>...</td>\n",
       "      <td>0.650563</td>\n",
       "      <td>0.738087</td>\n",
       "      <td>0.709363</td>\n",
       "      <td>0.141007</td>\n",
       "      <td>0.167449</td>\n",
       "      <td>0.709558</td>\n",
       "      <td>0.037477</td>\n",
       "      <td>0.031101</td>\n",
       "      <td>0.008568</td>\n",
       "      <td>0.018730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.537192</td>\n",
       "      <td>0.528780</td>\n",
       "      <td>0.413159</td>\n",
       "      <td>0.363199</td>\n",
       "      <td>...</td>\n",
       "      <td>0.653239</td>\n",
       "      <td>0.703021</td>\n",
       "      <td>0.681513</td>\n",
       "      <td>0.140234</td>\n",
       "      <td>0.186249</td>\n",
       "      <td>0.590142</td>\n",
       "      <td>0.015210</td>\n",
       "      <td>0.019426</td>\n",
       "      <td>0.008189</td>\n",
       "      <td>0.013444</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   sample_index  time  pain_survey_1  pain_survey_2  pain_survey_3  \\\n",
       "0           0.0   0.0            1.0            1.0            1.0   \n",
       "1           0.0   1.0            1.0            1.0            1.0   \n",
       "2           0.0   2.0            1.0            1.0            1.0   \n",
       "3           0.0   3.0            0.5            1.0            1.0   \n",
       "4           0.0   4.0            1.0            1.0            1.0   \n",
       "\n",
       "   pain_survey_4  joint_00  joint_01  joint_02  joint_03  ...  joint_07  \\\n",
       "0            1.0  0.561563  0.553352  0.419037  0.270175  ...  0.654303   \n",
       "1            1.0  0.599088  0.532067  0.461325  0.327922  ...  0.684443   \n",
       "2            1.0  0.638365  0.583960  0.445804  0.308796  ...  0.676488   \n",
       "3            1.0  0.554938  0.488719  0.443494  0.355023  ...  0.650563   \n",
       "4            0.0  0.537192  0.528780  0.413159  0.363199  ...  0.653239   \n",
       "\n",
       "   joint_08  joint_09  joint_10  joint_11  joint_12  joint_26  joint_27  \\\n",
       "0  0.737832  0.742275  0.100076  0.146564  0.745300  0.014909  0.045098   \n",
       "1  0.772454  0.710705  0.103457  0.174403  0.594262  0.053679  0.055375   \n",
       "2  0.799646  0.722061  0.143175  0.159973  0.652024  0.042305  0.039620   \n",
       "3  0.738087  0.709363  0.141007  0.167449  0.709558  0.037477  0.031101   \n",
       "4  0.703021  0.681513  0.140234  0.186249  0.590142  0.015210  0.019426   \n",
       "\n",
       "   joint_28  joint_29  \n",
       "0  0.012882  0.010178  \n",
       "1  0.013892  0.029085  \n",
       "2  0.016286  0.040638  \n",
       "3  0.008568  0.018730  \n",
       "4  0.008189  0.013444  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ------ Normalize test data ------\n",
    "\n",
    "list_to_remove = ['n_legs', 'n_hands', 'n_eyes', 'joint_13', 'joint_14', 'joint_15', 'joint_16', 'joint_17', 'joint_18', 'joint_19', 'joint_20', 'joint_21', 'joint_22', 'joint_23', 'joint_24', 'joint_25', 'joint_30']\n",
    "\n",
    "if X_test_final_df.columns.isin(list_to_remove).any():\n",
    "    X_test_final_df = X_test_final_df.drop(columns=['n_legs', 'n_hands', 'n_eyes', 'joint_13', 'joint_14', 'joint_15', 'joint_16', 'joint_17', 'joint_18', 'joint_19', 'joint_20', 'joint_21', 'joint_22', 'joint_23', 'joint_24', 'joint_25', 'joint_30'])\n",
    "else:\n",
    "  print(\"Usless features already removed\")\n",
    "\n",
    "# --- Load and preprocess the actual test dataset ---\n",
    "# Define the columns to be normalised (use training statistics for proper normalization)\n",
    "# Exclude 'sample_index', 'time', and 'label' as they were excluded during training sequence building\n",
    "# Also exclude 'joint_30' as it was removed from training data\n",
    "scale_columns = [col for col in X_test_final_df.columns\n",
    "                 if col != 'sample_index' and col != 'time' and col != 'joint_30']\n",
    "\n",
    "# Calculate the minimum and maximum values from the training data only\n",
    "mins_train = X_test_final_df[scale_columns].min()\n",
    "maxs_train = X_test_final_df[scale_columns].max()\n",
    "\n",
    "# Apply normalisation to the specified columns in all datasets\n",
    "for column in scale_columns:\n",
    "    # Normalise the testing set\n",
    "    if maxs_train[column] != mins_train[column]:\n",
    "      X_test_final_df[column] = (X_test_final_df[column] - mins_train[column]) / (maxs_train[column] - mins_train[column])\n",
    "\n",
    "X_test_final_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NlDwxJ38o8Rw"
   },
   "source": [
    "### 4.2 Categorical Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UqptiNjNQDW3",
    "outputId": "e5bc0f24-f309-4049-e40e-f6c7c06dc1c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original label distribution: {'no_pain': 511, 'low_pain': 94, 'high_pain': 56}\n",
      "Train label counts before balancing: {'no_pain': 385, 'low_pain': 71, 'high_pain': 42}\n",
      "Train label counts after balancing: {'no_pain': 385, 'low_pain': 71, 'high_pain': 42}\n",
      "Validation and Test distributions:\n",
      "Validation label counts: {'no_pain': 124, 'low_pain': 22, 'high_pain': 14}\n",
      "Test label counts: {'no_pain': 2, 'low_pain': 1}\n",
      "Label proportions:\n",
      "Train:\n",
      " label\n",
      "no_pain      0.773092\n",
      "low_pain     0.142570\n",
      "high_pain    0.084337\n",
      "Name: proportion, dtype: float64\n",
      "Val:\n",
      " label\n",
      "no_pain      0.7750\n",
      "low_pain     0.1375\n",
      "high_pain    0.0875\n",
      "Name: proportion, dtype: float64\n",
      "Test:\n",
      " label\n",
      "no_pain     0.666667\n",
      "low_pain    0.333333\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# df has columns: ['sample_index', 'label']\n",
    "N_NO_PAIN_KEEP = 1000   # how many \"no_pain\" pirates to keep in the training set (lower it to have a more balanced distribution of the labels)\n",
    "N_LOW_PAIN_KEEP = 150   # how many \"low_pain\" pirates to keep in the training set\n",
    "N_VAL_USERS = 160\n",
    "N_TEST_USERS = 3\n",
    "\n",
    "# --- Step 1: Compute each user's dominant label (or label distribution)\n",
    "user_labels = (\n",
    "    data.groupby('sample_index')['label']\n",
    "    .agg(lambda x: x.value_counts().index[0])  # dominant label per user\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "print(\"Original label distribution:\", user_labels['label'].value_counts().to_dict())\n",
    "\n",
    "# --- Step 2: Split into train/val/test keeping real label proportions\n",
    "train_users, temp_users = train_test_split(\n",
    "    user_labels['sample_index'],\n",
    "    test_size=(N_VAL_USERS + N_TEST_USERS) / len(user_labels),\n",
    "    stratify=user_labels['label'],\n",
    "    random_state=None\n",
    ")\n",
    "\n",
    "# Split temp into val/test (also stratified)\n",
    "temp_labels = user_labels[user_labels['sample_index'].isin(temp_users)]\n",
    "\n",
    "val_users, test_users = train_test_split(\n",
    "    temp_labels['sample_index'],\n",
    "    test_size=N_TEST_USERS / (N_VAL_USERS + N_TEST_USERS),\n",
    "    stratify=temp_labels['label'],\n",
    "    random_state=None\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# === Step 3: Create a partially balanced training set keeping a fixed number of pirates per label ===\n",
    "train_labels = user_labels[user_labels['sample_index'].isin(train_users)]\n",
    "\n",
    "# Count of how many pirates are present for each label in training set\n",
    "label_counts = train_labels['label'].value_counts()\n",
    "print(\"Train label counts before balancing:\", label_counts.to_dict())\n",
    "\n",
    "# --- Custom undersampling logic ---\n",
    "rng = random.Random()\n",
    "\n",
    "no_pain_users = train_labels[train_labels['label'] == 'no_pain']['sample_index'].tolist()\n",
    "low_pain_users = train_labels[train_labels['label'] == 'low_pain']['sample_index'].tolist()\n",
    "high_pain_users = train_labels[train_labels['label'] == 'high_pain']['sample_index'].tolist()\n",
    "\n",
    "# Choose how many to keep for each label\n",
    "no_pain_keep = min(N_NO_PAIN_KEEP, len(no_pain_users))\n",
    "low_pain_keep = min(N_LOW_PAIN_KEEP, len(low_pain_users))\n",
    "high_pain_keep = len(high_pain_users)  # keep all high_pain pirates\n",
    "\n",
    "selected_no_pain = rng.sample(no_pain_users, no_pain_keep)\n",
    "selected_low_pain = rng.sample(low_pain_users, low_pain_keep)\n",
    "selected_high_pain = high_pain_users  # keep all\n",
    "\n",
    "# Combine the selected users\n",
    "selected_users = selected_no_pain + selected_low_pain + selected_high_pain\n",
    "balanced_train_labels = train_labels[train_labels['sample_index'].isin(selected_users)]\n",
    "\n",
    "print(\"Train label counts after balancing:\", balanced_train_labels['label'].value_counts().to_dict())\n",
    "\n",
    "train_users = balanced_train_labels['sample_index']\n",
    "\n",
    "# Compute validation and test label distributions\n",
    "val_label_counts = user_labels[user_labels['sample_index'].isin(val_users)]['label'].value_counts().to_dict()\n",
    "test_label_counts = user_labels[user_labels['sample_index'].isin(test_users)]['label'].value_counts().to_dict()\n",
    "print(\"Validation and Test distributions:\")\n",
    "print(\"Validation label counts:\", val_label_counts)\n",
    "print(\"Test label counts:\", test_label_counts)\n",
    "\n",
    "\n",
    "# --- Step 4: Filter your main df\n",
    "df_train = data[data['sample_index'].isin(train_users)]\n",
    "df_val = data[data['sample_index'].isin(val_users)]\n",
    "df_test = data[data['sample_index'].isin(test_users)]\n",
    "\n",
    "# --- Step 5: Check label proportions\n",
    "print(\"Label proportions:\")\n",
    "print(\"Train:\\n\", df_train['label'].value_counts(normalize=True))\n",
    "print(\"Val:\\n\", df_val['label'].value_counts(normalize=True))\n",
    "print(\"Test:\\n\", df_test['label'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AUwlOzWBo8Rx"
   },
   "source": [
    "### 4.3 Stratified Train/Val/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kI13WygTQDW4",
    "outputId": "670777dc-871d-499d-d272-aa88a763d79d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((79680, 24), (25600, 24), (480, 24))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape, df_val.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PsE1KsTTQDW4",
    "outputId": "ab26e77b-f6f1-48f8-e143-775fc2382e2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pirates in training set: 498\n",
      "Total pirates in validation set: 160\n",
      "Total pirates in test set: 3\n"
     ]
    }
   ],
   "source": [
    "# Print the total number of pirates for each dataset\n",
    "print(f\"Total pirates in training set: {df_train['sample_index'].nunique()}\")\n",
    "print(f\"Total pirates in validation set: {df_val['sample_index'].nunique()}\")\n",
    "print(f\"Total pirates in test set: {df_test['sample_index'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "id": "A35PiqdxJsLQ",
    "outputId": "9c343fb9-d4d3-4269-82f3-e7066f0b250a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_index</th>\n",
       "      <th>time</th>\n",
       "      <th>pain_survey_1</th>\n",
       "      <th>pain_survey_2</th>\n",
       "      <th>pain_survey_3</th>\n",
       "      <th>pain_survey_4</th>\n",
       "      <th>joint_00</th>\n",
       "      <th>joint_01</th>\n",
       "      <th>joint_02</th>\n",
       "      <th>joint_03</th>\n",
       "      <th>...</th>\n",
       "      <th>joint_08</th>\n",
       "      <th>joint_09</th>\n",
       "      <th>joint_10</th>\n",
       "      <th>joint_11</th>\n",
       "      <th>joint_12</th>\n",
       "      <th>joint_26</th>\n",
       "      <th>joint_27</th>\n",
       "      <th>joint_28</th>\n",
       "      <th>joint_29</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.094705</td>\n",
       "      <td>0.985281</td>\n",
       "      <td>1.018302</td>\n",
       "      <td>1.010385</td>\n",
       "      <td>...</td>\n",
       "      <td>0.712989</td>\n",
       "      <td>1.050142</td>\n",
       "      <td>0.529555</td>\n",
       "      <td>0.447370</td>\n",
       "      <td>1.091046</td>\n",
       "      <td>0.017592</td>\n",
       "      <td>0.013508</td>\n",
       "      <td>0.026798</td>\n",
       "      <td>0.027815</td>\n",
       "      <td>no_pain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.135183</td>\n",
       "      <td>1.021175</td>\n",
       "      <td>0.994343</td>\n",
       "      <td>1.052364</td>\n",
       "      <td>...</td>\n",
       "      <td>0.722685</td>\n",
       "      <td>1.060313</td>\n",
       "      <td>0.446810</td>\n",
       "      <td>0.414432</td>\n",
       "      <td>1.045862</td>\n",
       "      <td>0.013352</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013377</td>\n",
       "      <td>0.013716</td>\n",
       "      <td>no_pain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.080745</td>\n",
       "      <td>0.962842</td>\n",
       "      <td>1.009588</td>\n",
       "      <td>0.977169</td>\n",
       "      <td>...</td>\n",
       "      <td>0.668043</td>\n",
       "      <td>1.011410</td>\n",
       "      <td>0.432499</td>\n",
       "      <td>0.431535</td>\n",
       "      <td>1.088221</td>\n",
       "      <td>0.016225</td>\n",
       "      <td>0.008110</td>\n",
       "      <td>0.024097</td>\n",
       "      <td>0.023105</td>\n",
       "      <td>no_pain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.938017</td>\n",
       "      <td>1.081592</td>\n",
       "      <td>0.998021</td>\n",
       "      <td>0.987283</td>\n",
       "      <td>...</td>\n",
       "      <td>0.702085</td>\n",
       "      <td>1.047223</td>\n",
       "      <td>0.478806</td>\n",
       "      <td>0.420665</td>\n",
       "      <td>1.096832</td>\n",
       "      <td>0.011832</td>\n",
       "      <td>0.007450</td>\n",
       "      <td>0.028613</td>\n",
       "      <td>0.024648</td>\n",
       "      <td>no_pain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.090185</td>\n",
       "      <td>1.032145</td>\n",
       "      <td>1.008710</td>\n",
       "      <td>0.963658</td>\n",
       "      <td>...</td>\n",
       "      <td>0.712197</td>\n",
       "      <td>1.044731</td>\n",
       "      <td>0.452906</td>\n",
       "      <td>0.476537</td>\n",
       "      <td>1.103968</td>\n",
       "      <td>0.005360</td>\n",
       "      <td>0.002532</td>\n",
       "      <td>0.033026</td>\n",
       "      <td>0.025328</td>\n",
       "      <td>no_pain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105755</th>\n",
       "      <td>660</td>\n",
       "      <td>155</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.051738</td>\n",
       "      <td>0.906653</td>\n",
       "      <td>0.852813</td>\n",
       "      <td>0.714132</td>\n",
       "      <td>...</td>\n",
       "      <td>0.793007</td>\n",
       "      <td>0.818443</td>\n",
       "      <td>0.696164</td>\n",
       "      <td>0.676377</td>\n",
       "      <td>1.065835</td>\n",
       "      <td>0.007856</td>\n",
       "      <td>0.026876</td>\n",
       "      <td>0.173566</td>\n",
       "      <td>0.221921</td>\n",
       "      <td>no_pain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105756</th>\n",
       "      <td>660</td>\n",
       "      <td>156</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.027651</td>\n",
       "      <td>0.894214</td>\n",
       "      <td>0.834575</td>\n",
       "      <td>0.790003</td>\n",
       "      <td>...</td>\n",
       "      <td>0.742828</td>\n",
       "      <td>0.861304</td>\n",
       "      <td>0.642332</td>\n",
       "      <td>0.677491</td>\n",
       "      <td>1.021720</td>\n",
       "      <td>0.026795</td>\n",
       "      <td>0.012778</td>\n",
       "      <td>0.075945</td>\n",
       "      <td>0.116763</td>\n",
       "      <td>no_pain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105757</th>\n",
       "      <td>660</td>\n",
       "      <td>157</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.113381</td>\n",
       "      <td>0.803824</td>\n",
       "      <td>0.856149</td>\n",
       "      <td>0.659963</td>\n",
       "      <td>...</td>\n",
       "      <td>0.781024</td>\n",
       "      <td>0.872811</td>\n",
       "      <td>0.723307</td>\n",
       "      <td>0.751857</td>\n",
       "      <td>1.031213</td>\n",
       "      <td>0.036982</td>\n",
       "      <td>0.028014</td>\n",
       "      <td>0.075978</td>\n",
       "      <td>0.078339</td>\n",
       "      <td>no_pain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105758</th>\n",
       "      <td>660</td>\n",
       "      <td>158</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.058100</td>\n",
       "      <td>0.902272</td>\n",
       "      <td>0.787495</td>\n",
       "      <td>0.685756</td>\n",
       "      <td>...</td>\n",
       "      <td>0.759178</td>\n",
       "      <td>0.790487</td>\n",
       "      <td>0.702029</td>\n",
       "      <td>0.678239</td>\n",
       "      <td>1.045568</td>\n",
       "      <td>0.046405</td>\n",
       "      <td>0.017922</td>\n",
       "      <td>0.097109</td>\n",
       "      <td>0.106807</td>\n",
       "      <td>no_pain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105759</th>\n",
       "      <td>660</td>\n",
       "      <td>159</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.008509</td>\n",
       "      <td>0.951576</td>\n",
       "      <td>0.764090</td>\n",
       "      <td>0.767467</td>\n",
       "      <td>...</td>\n",
       "      <td>0.821724</td>\n",
       "      <td>0.874577</td>\n",
       "      <td>0.689898</td>\n",
       "      <td>0.701382</td>\n",
       "      <td>1.123457</td>\n",
       "      <td>0.033489</td>\n",
       "      <td>0.041909</td>\n",
       "      <td>0.084751</td>\n",
       "      <td>0.163532</td>\n",
       "      <td>no_pain</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>79680 rows √ó 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        sample_index  time  pain_survey_1  pain_survey_2  pain_survey_3  \\\n",
       "0                  0     0              2              0              2   \n",
       "1                  0     1              2              2              2   \n",
       "2                  0     2              2              0              2   \n",
       "3                  0     3              2              2              2   \n",
       "4                  0     4              2              2              2   \n",
       "...              ...   ...            ...            ...            ...   \n",
       "105755           660   155              2              2              0   \n",
       "105756           660   156              2              2              0   \n",
       "105757           660   157              0              2              2   \n",
       "105758           660   158              2              2              2   \n",
       "105759           660   159              2              2              2   \n",
       "\n",
       "        pain_survey_4  joint_00  joint_01  joint_02  joint_03  ...  joint_08  \\\n",
       "0                   1  1.094705  0.985281  1.018302  1.010385  ...  0.712989   \n",
       "1                   2  1.135183  1.021175  0.994343  1.052364  ...  0.722685   \n",
       "2                   2  1.080745  0.962842  1.009588  0.977169  ...  0.668043   \n",
       "3                   2  0.938017  1.081592  0.998021  0.987283  ...  0.702085   \n",
       "4                   2  1.090185  1.032145  1.008710  0.963658  ...  0.712197   \n",
       "...               ...       ...       ...       ...       ...  ...       ...   \n",
       "105755              0  1.051738  0.906653  0.852813  0.714132  ...  0.793007   \n",
       "105756              2  1.027651  0.894214  0.834575  0.790003  ...  0.742828   \n",
       "105757              2  1.113381  0.803824  0.856149  0.659963  ...  0.781024   \n",
       "105758              2  1.058100  0.902272  0.787495  0.685756  ...  0.759178   \n",
       "105759              0  1.008509  0.951576  0.764090  0.767467  ...  0.821724   \n",
       "\n",
       "        joint_09  joint_10  joint_11  joint_12  joint_26  joint_27  joint_28  \\\n",
       "0       1.050142  0.529555  0.447370  1.091046  0.017592  0.013508  0.026798   \n",
       "1       1.060313  0.446810  0.414432  1.045862  0.013352  0.000000  0.013377   \n",
       "2       1.011410  0.432499  0.431535  1.088221  0.016225  0.008110  0.024097   \n",
       "3       1.047223  0.478806  0.420665  1.096832  0.011832  0.007450  0.028613   \n",
       "4       1.044731  0.452906  0.476537  1.103968  0.005360  0.002532  0.033026   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "105755  0.818443  0.696164  0.676377  1.065835  0.007856  0.026876  0.173566   \n",
       "105756  0.861304  0.642332  0.677491  1.021720  0.026795  0.012778  0.075945   \n",
       "105757  0.872811  0.723307  0.751857  1.031213  0.036982  0.028014  0.075978   \n",
       "105758  0.790487  0.702029  0.678239  1.045568  0.046405  0.017922  0.097109   \n",
       "105759  0.874577  0.689898  0.701382  1.123457  0.033489  0.041909  0.084751   \n",
       "\n",
       "        joint_29    label  \n",
       "0       0.027815  no_pain  \n",
       "1       0.013716  no_pain  \n",
       "2       0.023105  no_pain  \n",
       "3       0.024648  no_pain  \n",
       "4       0.025328  no_pain  \n",
       "...          ...      ...  \n",
       "105755  0.221921  no_pain  \n",
       "105756  0.116763  no_pain  \n",
       "105757  0.078339  no_pain  \n",
       "105758  0.106807  no_pain  \n",
       "105759  0.163532  no_pain  \n",
       "\n",
       "[79680 rows x 24 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "YkF51CmhQDW5"
   },
   "outputs": [],
   "source": [
    "scale_columns = [col for col in data.columns\n",
    "                 if col != 'sample_index' and col != 'joint_30' and col != 'label']\n",
    "\n",
    "# Calculate the minimum and maximum values from the training data only\n",
    "mins_train = df_train[scale_columns].min()\n",
    "maxs_train = df_train[scale_columns].max()\n",
    "\n",
    "mins_val = df_val[scale_columns].min()\n",
    "maxs_val = df_val[scale_columns].max()\n",
    "\n",
    "mins_test = df_test[scale_columns].min()\n",
    "maxs_test = df_test[scale_columns].max()\n",
    "\n",
    "# Apply normalisation to the specified columns in all datasets\n",
    "for column in scale_columns:\n",
    "    if maxs_train[column] != mins_train[column] and mins_val[column] != maxs_val[column] and mins_test[column] != maxs_test[column]:\n",
    "      df_train[column] = (df_train[column] - mins_train[column]) / (maxs_train[column] - mins_train[column])\n",
    "\n",
    "      # Normalise the validation set\n",
    "      df_val[column] = (df_val[column] - mins_val[column]) / (maxs_val[column] - mins_val[column])\n",
    "\n",
    "      # Normalise the test set\n",
    "      df_test[column] = (df_test[column] - mins_test[column]) / (maxs_test[column] - mins_test[column])\n",
    "\n",
    "    # elif column == 'n_hands' or column == 'n_eyes' or column == 'n_legs':\n",
    "    #   df_train[column] = 0.5\n",
    "    #   df_val[column] = 0.5\n",
    "    #   df_test[column] = 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "id": "R-cyjeAuYt4D",
    "outputId": "c7c283e3-0182-4533-fc93-b2f5d01d3089"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_index</th>\n",
       "      <th>time</th>\n",
       "      <th>pain_survey_1</th>\n",
       "      <th>pain_survey_2</th>\n",
       "      <th>pain_survey_3</th>\n",
       "      <th>pain_survey_4</th>\n",
       "      <th>joint_00</th>\n",
       "      <th>joint_01</th>\n",
       "      <th>joint_02</th>\n",
       "      <th>joint_03</th>\n",
       "      <th>...</th>\n",
       "      <th>joint_08</th>\n",
       "      <th>joint_09</th>\n",
       "      <th>joint_10</th>\n",
       "      <th>joint_11</th>\n",
       "      <th>joint_12</th>\n",
       "      <th>joint_26</th>\n",
       "      <th>joint_27</th>\n",
       "      <th>joint_28</th>\n",
       "      <th>joint_29</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.777046</td>\n",
       "      <td>0.738252</td>\n",
       "      <td>0.779512</td>\n",
       "      <td>0.804419</td>\n",
       "      <td>...</td>\n",
       "      <td>0.478382</td>\n",
       "      <td>0.753815</td>\n",
       "      <td>0.272106</td>\n",
       "      <td>0.269510</td>\n",
       "      <td>0.762947</td>\n",
       "      <td>0.014214</td>\n",
       "      <td>0.011376</td>\n",
       "      <td>0.018978</td>\n",
       "      <td>0.020291</td>\n",
       "      <td>no_pain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.006289</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.805855</td>\n",
       "      <td>0.765147</td>\n",
       "      <td>0.761153</td>\n",
       "      <td>0.838021</td>\n",
       "      <td>...</td>\n",
       "      <td>0.486231</td>\n",
       "      <td>0.761224</td>\n",
       "      <td>0.217448</td>\n",
       "      <td>0.245846</td>\n",
       "      <td>0.727910</td>\n",
       "      <td>0.010748</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009473</td>\n",
       "      <td>0.010006</td>\n",
       "      <td>no_pain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.012579</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.767110</td>\n",
       "      <td>0.721439</td>\n",
       "      <td>0.772834</td>\n",
       "      <td>0.777832</td>\n",
       "      <td>...</td>\n",
       "      <td>0.441994</td>\n",
       "      <td>0.725601</td>\n",
       "      <td>0.207995</td>\n",
       "      <td>0.258133</td>\n",
       "      <td>0.760757</td>\n",
       "      <td>0.013097</td>\n",
       "      <td>0.006830</td>\n",
       "      <td>0.017065</td>\n",
       "      <td>0.016856</td>\n",
       "      <td>no_pain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.018868</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.665528</td>\n",
       "      <td>0.810416</td>\n",
       "      <td>0.763971</td>\n",
       "      <td>0.785928</td>\n",
       "      <td>...</td>\n",
       "      <td>0.469554</td>\n",
       "      <td>0.751688</td>\n",
       "      <td>0.238584</td>\n",
       "      <td>0.250324</td>\n",
       "      <td>0.767434</td>\n",
       "      <td>0.009505</td>\n",
       "      <td>0.006274</td>\n",
       "      <td>0.020264</td>\n",
       "      <td>0.017981</td>\n",
       "      <td>no_pain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.025157</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.773829</td>\n",
       "      <td>0.773366</td>\n",
       "      <td>0.772162</td>\n",
       "      <td>0.767017</td>\n",
       "      <td>...</td>\n",
       "      <td>0.477740</td>\n",
       "      <td>0.749873</td>\n",
       "      <td>0.221475</td>\n",
       "      <td>0.290464</td>\n",
       "      <td>0.772967</td>\n",
       "      <td>0.004216</td>\n",
       "      <td>0.002132</td>\n",
       "      <td>0.023389</td>\n",
       "      <td>0.018477</td>\n",
       "      <td>no_pain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105755</th>\n",
       "      <td>660</td>\n",
       "      <td>0.974843</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.746465</td>\n",
       "      <td>0.679338</td>\n",
       "      <td>0.652703</td>\n",
       "      <td>0.567290</td>\n",
       "      <td>...</td>\n",
       "      <td>0.543164</td>\n",
       "      <td>0.585036</td>\n",
       "      <td>0.382161</td>\n",
       "      <td>0.434035</td>\n",
       "      <td>0.743398</td>\n",
       "      <td>0.006255</td>\n",
       "      <td>0.022634</td>\n",
       "      <td>0.122919</td>\n",
       "      <td>0.161896</td>\n",
       "      <td>no_pain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105756</th>\n",
       "      <td>660</td>\n",
       "      <td>0.981132</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.729322</td>\n",
       "      <td>0.670018</td>\n",
       "      <td>0.638728</td>\n",
       "      <td>0.628019</td>\n",
       "      <td>...</td>\n",
       "      <td>0.502539</td>\n",
       "      <td>0.616258</td>\n",
       "      <td>0.346602</td>\n",
       "      <td>0.434835</td>\n",
       "      <td>0.709191</td>\n",
       "      <td>0.021736</td>\n",
       "      <td>0.010761</td>\n",
       "      <td>0.053784</td>\n",
       "      <td>0.085181</td>\n",
       "      <td>no_pain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105757</th>\n",
       "      <td>660</td>\n",
       "      <td>0.987421</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.790338</td>\n",
       "      <td>0.602290</td>\n",
       "      <td>0.655260</td>\n",
       "      <td>0.523931</td>\n",
       "      <td>...</td>\n",
       "      <td>0.533463</td>\n",
       "      <td>0.624639</td>\n",
       "      <td>0.400091</td>\n",
       "      <td>0.488262</td>\n",
       "      <td>0.716552</td>\n",
       "      <td>0.030063</td>\n",
       "      <td>0.023592</td>\n",
       "      <td>0.053808</td>\n",
       "      <td>0.057150</td>\n",
       "      <td>no_pain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105758</th>\n",
       "      <td>660</td>\n",
       "      <td>0.993711</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.750993</td>\n",
       "      <td>0.676055</td>\n",
       "      <td>0.602652</td>\n",
       "      <td>0.544576</td>\n",
       "      <td>...</td>\n",
       "      <td>0.515776</td>\n",
       "      <td>0.564671</td>\n",
       "      <td>0.386035</td>\n",
       "      <td>0.435373</td>\n",
       "      <td>0.727682</td>\n",
       "      <td>0.037765</td>\n",
       "      <td>0.015093</td>\n",
       "      <td>0.068772</td>\n",
       "      <td>0.077918</td>\n",
       "      <td>no_pain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105759</th>\n",
       "      <td>660</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.715698</td>\n",
       "      <td>0.712997</td>\n",
       "      <td>0.584718</td>\n",
       "      <td>0.609981</td>\n",
       "      <td>...</td>\n",
       "      <td>0.566413</td>\n",
       "      <td>0.625926</td>\n",
       "      <td>0.378022</td>\n",
       "      <td>0.451999</td>\n",
       "      <td>0.788080</td>\n",
       "      <td>0.027207</td>\n",
       "      <td>0.035294</td>\n",
       "      <td>0.060020</td>\n",
       "      <td>0.119300</td>\n",
       "      <td>no_pain</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>79680 rows √ó 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        sample_index      time  pain_survey_1  pain_survey_2  pain_survey_3  \\\n",
       "0                  0  0.000000            1.0            0.0            1.0   \n",
       "1                  0  0.006289            1.0            1.0            1.0   \n",
       "2                  0  0.012579            1.0            0.0            1.0   \n",
       "3                  0  0.018868            1.0            1.0            1.0   \n",
       "4                  0  0.025157            1.0            1.0            1.0   \n",
       "...              ...       ...            ...            ...            ...   \n",
       "105755           660  0.974843            1.0            1.0            0.0   \n",
       "105756           660  0.981132            1.0            1.0            0.0   \n",
       "105757           660  0.987421            0.0            1.0            1.0   \n",
       "105758           660  0.993711            1.0            1.0            1.0   \n",
       "105759           660  1.000000            1.0            1.0            1.0   \n",
       "\n",
       "        pain_survey_4  joint_00  joint_01  joint_02  joint_03  ...  joint_08  \\\n",
       "0                 0.5  0.777046  0.738252  0.779512  0.804419  ...  0.478382   \n",
       "1                 1.0  0.805855  0.765147  0.761153  0.838021  ...  0.486231   \n",
       "2                 1.0  0.767110  0.721439  0.772834  0.777832  ...  0.441994   \n",
       "3                 1.0  0.665528  0.810416  0.763971  0.785928  ...  0.469554   \n",
       "4                 1.0  0.773829  0.773366  0.772162  0.767017  ...  0.477740   \n",
       "...               ...       ...       ...       ...       ...  ...       ...   \n",
       "105755            0.0  0.746465  0.679338  0.652703  0.567290  ...  0.543164   \n",
       "105756            1.0  0.729322  0.670018  0.638728  0.628019  ...  0.502539   \n",
       "105757            1.0  0.790338  0.602290  0.655260  0.523931  ...  0.533463   \n",
       "105758            1.0  0.750993  0.676055  0.602652  0.544576  ...  0.515776   \n",
       "105759            0.0  0.715698  0.712997  0.584718  0.609981  ...  0.566413   \n",
       "\n",
       "        joint_09  joint_10  joint_11  joint_12  joint_26  joint_27  joint_28  \\\n",
       "0       0.753815  0.272106  0.269510  0.762947  0.014214  0.011376  0.018978   \n",
       "1       0.761224  0.217448  0.245846  0.727910  0.010748  0.000000  0.009473   \n",
       "2       0.725601  0.207995  0.258133  0.760757  0.013097  0.006830  0.017065   \n",
       "3       0.751688  0.238584  0.250324  0.767434  0.009505  0.006274  0.020264   \n",
       "4       0.749873  0.221475  0.290464  0.772967  0.004216  0.002132  0.023389   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "105755  0.585036  0.382161  0.434035  0.743398  0.006255  0.022634  0.122919   \n",
       "105756  0.616258  0.346602  0.434835  0.709191  0.021736  0.010761  0.053784   \n",
       "105757  0.624639  0.400091  0.488262  0.716552  0.030063  0.023592  0.053808   \n",
       "105758  0.564671  0.386035  0.435373  0.727682  0.037765  0.015093  0.068772   \n",
       "105759  0.625926  0.378022  0.451999  0.788080  0.027207  0.035294  0.060020   \n",
       "\n",
       "        joint_29    label  \n",
       "0       0.020291  no_pain  \n",
       "1       0.010006  no_pain  \n",
       "2       0.016856  no_pain  \n",
       "3       0.017981  no_pain  \n",
       "4       0.018477  no_pain  \n",
       "...          ...      ...  \n",
       "105755  0.161896  no_pain  \n",
       "105756  0.085181  no_pain  \n",
       "105757  0.057150  no_pain  \n",
       "105758  0.077918  no_pain  \n",
       "105759  0.119300  no_pain  \n",
       "\n",
       "[79680 rows x 24 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "NiqXdpBj0089"
   },
   "outputs": [],
   "source": [
    "# Define a function to inspect sensor data for a specific label\n",
    "def inspect_label(label, df):\n",
    "    # Filter the DataFrame for the specified label and limit to 159 rows\n",
    "    data = df[df['label'] == label][['pain_survey_1', 'pain_survey_2', 'pain_survey_3', 'pain_survey_4']][:159]\n",
    "\n",
    "    # Plot the sensor data for each axis\n",
    "    axis = data.plot(subplots=True, figsize=(17, 9), title=label)\n",
    "\n",
    "    # Adjust legend position for each subplot\n",
    "    for ax in axis:\n",
    "        ax.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "PUBCloMc3Fyt"
   },
   "outputs": [],
   "source": [
    "# Define a function to inspect sensor data for a specific label\n",
    "def inspect_label(label, df):\n",
    "    # Filter the DataFrame for the specified label and limit to 159 rows\n",
    "    data = df[df['label'] == label][['joint_00', 'joint_01', 'joint_02', 'joint_03', 'joint_04', 'joint_05' , 'joint_06', 'joint_07' , 'joint_08', 'joint_09' , 'joint_10', 'joint_11' , 'joint_12']][:159]\n",
    "\n",
    "    # Plot the sensor data for each axis\n",
    "    axis = data.plot(subplots=True, figsize=(17, 9), title=label)\n",
    "\n",
    "    # Adjust legend position for each subplot\n",
    "    for ax in axis:\n",
    "        ax.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X9Pp8l1bo8Rx"
   },
   "source": [
    "### 4.4 Feature Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gDhUJUrkQDW6",
    "outputId": "c869c3b6-e5b2-4c8e-e989-dd8515b89861"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training labels: {'no_pain': 385, 'low_pain': 71, 'high_pain': 42}\n",
      "Validation labels: {'no_pain': 124, 'low_pain': 22, 'high_pain': 14}\n",
      "Test labels: {'no_pain': 2, 'low_pain': 1, 'high_pain': 0}\n"
     ]
    }
   ],
   "source": [
    "# Initialise a dictionary to count occurrences of each activity in the training set\n",
    "training_labels = {\n",
    "    'no_pain': 0,\n",
    "    'low_pain': 0,\n",
    "    'high_pain': 0\n",
    "}\n",
    "\n",
    "# Count occurrences of each activity for unique IDs in the training set\n",
    "for id in df_train['sample_index'].unique():\n",
    "    label = df_train[df_train['sample_index'] == id]['label'].values[0]\n",
    "    training_labels[label] += 1\n",
    "\n",
    "\n",
    "#if 'joint_30' in df_train.columns:\n",
    "#    df_train = df_train.drop(columns=['joint_30']) # we deleted this joint since has no final effect during training\n",
    "\n",
    "# Print the distribution of training labels\n",
    "print('Training labels:', training_labels)\n",
    "\n",
    "# Initialise a dictionary to count occurrences of each activity in the training set\n",
    "val_labels = {\n",
    "    'no_pain': 0,\n",
    "    'low_pain': 0,\n",
    "    'high_pain': 0\n",
    "}\n",
    "\n",
    "# Count occurrences of each activity for unique IDs in the training set\n",
    "for id in df_val['sample_index'].unique():\n",
    "    label = df_val[df_val['sample_index'] == id]['label'].values[0]\n",
    "    val_labels[label] += 1\n",
    "\n",
    "\n",
    "#if 'joint_30' in df_val.columns:\n",
    "#    df_val = df_val.drop(columns=['joint_30']) # we deleted this joint since has no final effect during training\n",
    "\n",
    "# Print the distribution of validation labels\n",
    "print('Validation labels:', val_labels)\n",
    "\n",
    "# Initialise a dictionary to count occurrences of each activity in the test set\n",
    "test_labels = {\n",
    "    'no_pain': 0,\n",
    "    'low_pain': 0,\n",
    "    'high_pain': 0\n",
    "}\n",
    "\n",
    "# Count occurrences of each activity for unique IDs in the test set\n",
    "for id in df_test['sample_index'].unique():\n",
    "    label = df_test[df_test['sample_index'] == id]['label'].values[0]\n",
    "    test_labels[label] += 1\n",
    "#if 'joint_30' in df_test.columns:\n",
    "#    df_test = df_test.drop(columns=['joint_30']) # we deleted this joint since has no final effect during training\n",
    "\n",
    "# Print the distribution of test labels\n",
    "print('Test labels:', test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cDuw27RUo8Rx"
   },
   "source": [
    "### 4.5 Label Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "DT3wxqdLQDW7"
   },
   "outputs": [],
   "source": [
    "# Define a training mapping of label names to integer labels\n",
    "train_label_mapping = {\n",
    "    'no_pain': 0,\n",
    "    'low_pain': 1,\n",
    "    'high_pain': 2\n",
    "}\n",
    "\n",
    "# Map label names to integers in the training set\n",
    "df_train['label'] = df_train['label'].map(train_label_mapping)\n",
    "\n",
    "# Define a validation mapping of label names to integer labels\n",
    "val_label_mapping = {\n",
    "    'no_pain': 0,\n",
    "    'low_pain': 1,\n",
    "    'high_pain': 2\n",
    "}\n",
    "\n",
    "# Map label names to integers in the validation set\n",
    "df_val['label'] = df_val['label'].map(val_label_mapping)\n",
    "\n",
    "test_label_mapping = {\n",
    "    'no_pain': 0,\n",
    "    'low_pain': 1,\n",
    "    'high_pain': 2\n",
    "}\n",
    "\n",
    "# Map label names to integers in the test set\n",
    "df_test['label'] = df_test['label'].map(test_label_mapping)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hOi0Yx5bo8Ry"
   },
   "source": [
    "<a id=\"sequence-building\"></a>\n",
    "## 5. Sequence Building\n",
    "\n",
    "Convert variable-length time-series into fixed-size windows for RNN input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "3qv_eAbDQDW7"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define window and stride boolean variable -> if True, during training we will visit more time the same pirate with overlapping windows\n",
    "# if False, each pirate will be visited only once during training\n",
    "one_pirate_window = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "sbmKJyH-QDW7"
   },
   "outputs": [],
   "source": [
    "if one_pirate_window:\n",
    "    # Define the window size\n",
    "    WINDOW_SIZE = 50      #68 \n",
    "\n",
    "    # Stride size\n",
    "    STRIDE = 10           #17\n",
    "else:\n",
    "    # Define the window size -> select an higher window size in order to get more pirates\n",
    "    WINDOW_SIZE = 160\n",
    "\n",
    "    # Stride size\n",
    "    STRIDE = 160"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ut3JifRfo8Ry"
   },
   "source": [
    "### 5.1 Window & Stride Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "dY-ksbv0QDW8"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "  GENERAL COMMENTS:\n",
    "  in this function we are restricting for each user data the number of samples of recorded data to a constant\n",
    "  value (window size), since every user data could be composed by different numbers of timestep. Therefore we are\n",
    "  \"normalizing\" the timesteps of a constant window size. Additionally is also defined a stride variable, which if is equal to\n",
    "  the window size, then we are not taking overlapping timestamp samples, instead if stride < window, we are letting some samples\n",
    "  to overlap in such a way that the RNN or other kind of NN architecture will analyze better the context.\n",
    "\"\"\"\n",
    "\n",
    "# Define a function to build sequences from the dataset\n",
    "def build_sequences(df, window=200, stride=200):\n",
    "    # Sanity check to ensure the window is divisible by the stride\n",
    "    assert window % stride == 0 # checks if the window size is divisible by the stride\n",
    "\n",
    "    # Initialise lists to store sequences and their corresponding labels\n",
    "    dataset = []\n",
    "    labels = []\n",
    "\n",
    "    # Iterate over unique IDs in the DataFrame\n",
    "    for id in df['sample_index'].unique():\n",
    "\n",
    "        # Extract pirate sample index data for the current sample index\n",
    "        columns = [col for col in df.columns if col not in ['sample_index', 'label', 'time']]\n",
    "\n",
    "        temp = df[df['sample_index'] == id][columns].values\n",
    "\n",
    "        # Retrieve the label for the current pirate\n",
    "        label = df[df['sample_index'] == id]['label'].values[0]\n",
    "\n",
    "        # Calculate padding length to ensure full windows\n",
    "        padding_len = window - len(temp) % window\n",
    "\n",
    "        # Create zero padding and concatenate with the data\n",
    "        padding = np.zeros((padding_len, len(columns)), dtype='float32')\n",
    "        temp = np.concatenate((temp, padding))\n",
    "\n",
    "        # Build feature windows and associate them with labels\n",
    "        idx = 0\n",
    "        while idx + window <= len(temp):\n",
    "            dataset.append(temp[idx:idx + window])\n",
    "            labels.append(label)\n",
    "            idx += stride\n",
    "\n",
    "    # Convert lists to numpy arrays for further processing\n",
    "    dataset = np.array(dataset)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    return dataset, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fUdcu3oGo8Ry"
   },
   "source": [
    "### 5.2 Build Sequences Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TPVEgwEmQDW8",
    "outputId": "98737a2e-46cf-4328-b785-5cb69294d138"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7968, 50, 21), (7968,), (2560, 50, 21), (2560,), (48, 50, 21), (48,))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate sequences and labels for the training set\n",
    "X_train, y_train = build_sequences(df_train, WINDOW_SIZE, STRIDE)\n",
    "\n",
    "# Generate sequences and labels for the validation set\n",
    "X_val, y_val = build_sequences(df_val, WINDOW_SIZE, STRIDE)\n",
    "\n",
    "# Generate sequences and labels for the test set\n",
    "X_test, y_test = build_sequences(df_test, WINDOW_SIZE, STRIDE)\n",
    "\n",
    "# Print the shapes of the generated datasets and their labels\n",
    "X_train.shape, y_train.shape, X_val.shape, y_val.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "utsTrSH5o8Ry"
   },
   "source": [
    "### 5.3 Generate Sequences for Train/Val/Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "X1mmtqJwQDW8"
   },
   "outputs": [],
   "source": [
    "# Convert dataset into float32 for PyTorch compatibility\n",
    "X_train = X_train.astype('float32')\n",
    "X_val = X_val.astype('float32')\n",
    "X_test = X_test.astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0jLnJ0YMo8Rz"
   },
   "source": [
    "### 5.4 Data Type Conversion & Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "nxfz2MegQDW8"
   },
   "outputs": [],
   "source": [
    "# Define the input shape based on the training data\n",
    "input_shape = X_train.shape[1:]\n",
    "\n",
    "# Define the number of classes based on the categorical labels\n",
    "num_classes = len(np.unique(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "JcSpP3DBQDW8"
   },
   "outputs": [],
   "source": [
    "# Discard nan values from the dataset\n",
    "if np.isnan(X_train).any() or np.isnan(X_val).any() or np.isnan(X_test).any():\n",
    "    X_train = np.nan_to_num(X_train)\n",
    "    X_val = np.nan_to_num(X_val)\n",
    "    X_test = np.nan_to_num(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "s45svGakQDW8"
   },
   "outputs": [],
   "source": [
    "# Convert numpy arrays to PyTorch datasets (pairs features with labels)\n",
    "train_ds = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n",
    "val_ds   = TensorDataset(torch.from_numpy(X_val), torch.from_numpy(y_val))\n",
    "test_ds  = TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5b-SGD2to8Rz"
   },
   "source": [
    "<a id=\"dataloaders\"></a>\n",
    "## 6. DataLoaders\n",
    "\n",
    "Create PyTorch DataLoaders for efficient batching and parallel loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "a2dd26aqQDW8"
   },
   "outputs": [],
   "source": [
    "# Define the batch size, which is the number of samples in each batch\n",
    "BATCH_SIZE = 480   # before: 512 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "8CPWMPHOQDW9"
   },
   "outputs": [],
   "source": [
    "def make_loader(ds, batch_size, shuffle, drop_last, sampler=None):\n",
    "    # Determine optimal number of worker processes for data loading\n",
    "    cpu_cores = os.cpu_count() or 2\n",
    "    num_workers = max(2, min(4, cpu_cores))\n",
    "\n",
    "    # Create DataLoader with performance optimizations\n",
    "    return DataLoader(\n",
    "        ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle if sampler is None else False,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,  # Faster GPU transfer\n",
    "        pin_memory_device=\"cuda\" if torch.cuda.is_available() else \"\",\n",
    "        prefetch_factor=4,  # Load 4 batches ahead\n",
    "        sampler=sampler\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "cPlQI3R8QDW9"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "# Compute class counts\n",
    "class_counts = np.bincount(y_train)\n",
    "class_weights = 1. / class_counts\n",
    "sample_weights = class_weights[y_train]\n",
    "sampler = WeightedRandomSampler(weights=torch.tensor(sample_weights, dtype=torch.float32), num_samples=len(sample_weights), replacement=True)\n",
    "\n",
    "# Create data loaders with different settings for each phase\n",
    "train_loader = make_loader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=False, sampler=sampler)\n",
    "val_loader   = make_loader(val_ds, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "test_loader  = make_loader(test_ds, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "89dc9b38",
    "outputId": "546c849b-0617-4231-824e-780bfe815273"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulated batch distribution: Counter({1: 1800, 2: 1751, 0: 1729})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "batch_labels = []\n",
    "for _, yb in train_loader:\n",
    "    batch_labels.extend(yb.tolist())\n",
    "    if len(batch_labels) > 5000:  # just check some batches\n",
    "        break\n",
    "\n",
    "print(\"Simulated batch distribution:\", Counter(batch_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "33LHsppfQDW-",
    "outputId": "bc9c1c56-3063-431e-e00a-b4456520d2b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features batch shape: torch.Size([480, 50, 21])\n",
      "Labels batch shape: torch.Size([480])\n"
     ]
    }
   ],
   "source": [
    "# Get one batch from the training data loader\n",
    "for xb, yb in train_loader:\n",
    "    print(\"Features batch shape:\", xb.shape)\n",
    "    print(\"Labels batch shape:\", yb.shape)\n",
    "    break # Stop after getting one batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "Jl4n83GrQDW-"
   },
   "outputs": [],
   "source": [
    "def recurrent_summary(model, input_size):\n",
    "    \"\"\"\n",
    "    Custom summary function that emulates torchinfo's output while correctly\n",
    "    counting parameters for RNN/GRU/LSTM layers.\n",
    "\n",
    "    This function is designed for models whose direct children are\n",
    "    nn.Linear, nn.RNN, nn.GRU, or nn.LSTM layers.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The model to analyze.\n",
    "        input_size (tuple): Shape of the input tensor (e.g., (seq_len, features)).\n",
    "    \"\"\"\n",
    "\n",
    "    # Dictionary to store output shapes captured by forward hooks\n",
    "    output_shapes = {}\n",
    "    # List to track hook handles for later removal\n",
    "    hooks = []\n",
    "\n",
    "    def get_hook(name):\n",
    "        \"\"\"Factory function to create a forward hook for a specific module.\"\"\"\n",
    "        def hook(module, input, output):\n",
    "            # Handle RNN layer outputs (returns a tuple)\n",
    "            if isinstance(output, tuple):\n",
    "                # output[0]: all hidden states with shape (batch, seq_len, hidden*directions)\n",
    "                shape1 = list(output[0].shape)\n",
    "                shape1[0] = -1  # Replace batch dimension with -1\n",
    "\n",
    "                # output[1]: final hidden state h_n (or tuple (h_n, c_n) for LSTM)\n",
    "                if isinstance(output[1], tuple):  # LSTM case: (h_n, c_n)\n",
    "                    shape2 = list(output[1][0].shape)  # Extract h_n only\n",
    "                else:  # RNN/GRU case: h_n only\n",
    "                    shape2 = list(output[1].shape)\n",
    "\n",
    "                # Replace batch dimension (middle position) with -1\n",
    "                shape2[1] = -1\n",
    "\n",
    "                output_shapes[name] = f\"[{shape1}, {shape2}]\"\n",
    "\n",
    "            # Handle standard layer outputs (e.g., Linear)\n",
    "            else:\n",
    "                shape = list(output.shape)\n",
    "                shape[0] = -1  # Replace batch dimension with -1\n",
    "                output_shapes[name] = f\"{shape}\"\n",
    "        return hook\n",
    "\n",
    "    # 1. Determine the device where model parameters reside\n",
    "    try:\n",
    "        device = next(model.parameters()).device\n",
    "    except StopIteration:\n",
    "        device = torch.device(\"cpu\")  # Fallback for models without parameters\n",
    "\n",
    "    # 2. Create a dummy input tensor with batch_size=1\n",
    "    dummy_input = torch.randn(1, *input_size).to(device)\n",
    "\n",
    "    # 3. Register forward hooks on target layers\n",
    "    # Iterate through direct children of the model (e.g., self.rnn, self.classifier)\n",
    "    for name, module in model.named_children():\n",
    "        if isinstance(module, (nn.Linear, nn.RNN, nn.GRU, nn.LSTM)):\n",
    "            # Register the hook and store its handle for cleanup\n",
    "            hook_handle = module.register_forward_hook(get_hook(name))\n",
    "            hooks.append(hook_handle)\n",
    "\n",
    "    # 4. Execute a dummy forward pass in evaluation mode\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            model(dummy_input)\n",
    "        except Exception as e:\n",
    "            print(f\"Error during dummy forward pass: {e}\")\n",
    "            # Clean up hooks even if an error occurs\n",
    "            for h in hooks:\n",
    "                h.remove()\n",
    "            return\n",
    "\n",
    "    # 5. Remove all registered hooks\n",
    "    for h in hooks:\n",
    "        h.remove()\n",
    "\n",
    "    # --- 6. Print the summary table ---\n",
    "\n",
    "    print(\"-\" * 79)\n",
    "    # Column headers\n",
    "    print(f\"{'Layer (type)':<25} {'Output Shape':<28} {'Param #':<18}\")\n",
    "    print(\"=\" * 79)\n",
    "\n",
    "    total_params = 0\n",
    "    total_trainable_params = 0\n",
    "\n",
    "    # Iterate through modules again to collect and display parameter information\n",
    "    for name, module in model.named_children():\n",
    "        if name in output_shapes:\n",
    "            # Count total and trainable parameters for this module\n",
    "            module_params = sum(p.numel() for p in module.parameters())\n",
    "            trainable_params = sum(p.numel() for p in module.parameters() if p.requires_grad)\n",
    "\n",
    "            total_params += module_params\n",
    "            total_trainable_params += trainable_params\n",
    "\n",
    "            # Format strings for display\n",
    "            layer_name = f\"{name} ({type(module).__name__})\"\n",
    "            output_shape_str = str(output_shapes[name])\n",
    "            params_str = f\"{trainable_params:,}\"\n",
    "\n",
    "            print(f\"{layer_name:<25} {output_shape_str:<28} {params_str:<15}\")\n",
    "\n",
    "    print(\"=\" * 79)\n",
    "    print(f\"Total params: {total_params:,}\")\n",
    "    print(f\"Trainable params: {total_trainable_params:,}\")\n",
    "    print(f\"Non-trainable params: {total_params - total_trainable_params:,}\")\n",
    "    print(\"-\" * 79)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o5VULB4UQDW_"
   },
   "source": [
    "<a id=\"hyperparameters\"></a>\n",
    "## 7. Network Hyperparameters\n",
    "\n",
    "Configure training settings, architecture parameters, and regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "0a183G6zQDW_"
   },
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "LEARNING_RATE = 1e-3\n",
    "EPOCHS = 500\n",
    "PATIENCE = 50\n",
    "\n",
    "# Architecture\n",
    "HIDDEN_LAYERS = 2        # Hidden layers\n",
    "HIDDEN_SIZE = 42  # Neurons per layer -> prev hidden size = 128\n",
    "\n",
    "# Regularisation\n",
    "DROPOUT_RATE = 0.30      # Dropout probability\n",
    "\n",
    "# For now disable weight decay\n",
    "L1_LAMBDA = 1e-6        # L1 penalty\n",
    "L2_LAMBDA = 1e-4          # L2 penalty\n",
    "\n",
    "\n",
    "# Set up loss function and optimizer\n",
    "\n",
    "# TO GIVE DIFFERENT WEIGHTS TO THE LOSS DEPENDING ON THE INVERSE OF EACH LABEL TOTAL NUMBER:\n",
    "# Set up loss function and optimizer\n",
    "# from sklearn.utils.class_weight import compute_class_weight\n",
    "# classes = np.unique(y_train)\n",
    "# class_weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_train)\n",
    "# class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "# criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "# print(f\"Class weights (order = {classes}): {class_weights.cpu().numpy()}\")\n",
    "\n",
    "\n",
    "# TO GIVE FIXED WEIGHTS TO THE LOSS FUNCTION DEPENDING ON THE LABELS DISTRIBUTION:\n",
    "# weights = torch.tensor([1.0, 1.2, 1.4]).to(device)\n",
    "# criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# TO NOT CHANGE THE WEIGHTS, BUT WITH LABEL SMOOTHING\n",
    "# alpha = torch.tensor([0.5, 1.5, 2.0])\n",
    "# criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "\n",
    "\n",
    "# TO WEIGHT MORE THE \"MORE DIFFICULT\" CASES AND THE LESS FREQUENT LABELS:\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=2.0):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.ce = nn.CrossEntropyLoss(weight=alpha, reduction='none')\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = self.ce(inputs, targets)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = ((1 - pt) ** self.gamma) * ce_loss\n",
    "        return focal_loss.mean()\n",
    "alpha = None\n",
    "# alpha = torch.tensor([0.7, 1.3, 1.7], dtype=torch.float32, device=device)  # None if we don't want to alterate the weights of each label losses (FocalLoss already do it)\n",
    "criterion = FocalLoss(alpha=alpha, gamma=2.3)  # gamma = 0 it's like Crossentropy(), gamma < 1 it's like in between Crossentropy and FocalLoss,\n",
    "                                               # gamma = 1 it's a good compromise, gamma = 1.5 or gamma = 2 to weight so much the less present labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "7-RAXl_BQDXA"
   },
   "outputs": [],
   "source": [
    "# Initialize best model tracking variables\n",
    "best_model = None\n",
    "best_performance = float('-inf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ouaaN67Ho8R3"
   },
   "source": [
    "<a id=\"model-architecture\"></a>\n",
    "## 8. Model Architecture\n",
    "\n",
    "Custom RNN/LSTM/GRU classifier with configurable bidirectionality and dropout."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enhanced LSTM Model with Embedding Layers\n",
    "\n",
    "This enhanced model uses embedding layers for the categorical `pain_survey` features before feeding them to the LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedRecurrentClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Enhanced RNN classifier with embedding layers for categorical features.\n",
    "    Combines embeddings with continuous features before passing to LSTM.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            continuous_input_size,    # Number of continuous features (joints)\n",
    "            categorical_features,     # Dict: {feature_name: num_unique_values}\n",
    "            embedding_dims,          # Dict: {feature_name: embedding_dimension}\n",
    "            hidden_size,\n",
    "            num_layers,\n",
    "            num_classes,\n",
    "            rnn_type='LSTM',\n",
    "            bidirectional=True,\n",
    "            dropout_rate=0.2\n",
    "            ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.rnn_type = rnn_type\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bidirectional = bidirectional\n",
    "        self.categorical_features = list(categorical_features.keys())\n",
    "        self.continuous_input_size = continuous_input_size\n",
    "        \n",
    "        # Create embedding layers for each categorical feature\n",
    "        self.embeddings = nn.ModuleDict()\n",
    "        total_embedding_size = 0\n",
    "        \n",
    "        for feature, num_values in categorical_features.items():\n",
    "            embed_dim = embedding_dims[feature]\n",
    "            # Add 1 to num_values to handle potential out-of-range values\n",
    "            self.embeddings[feature] = nn.Embedding(num_values + 1, embed_dim)\n",
    "            total_embedding_size += embed_dim\n",
    "            print(f\"Created embedding for {feature}: {num_values} -> {embed_dim}\")\n",
    "        \n",
    "        # Total input size to RNN = continuous features + all embedding outputs\n",
    "        rnn_input_size = continuous_input_size + total_embedding_size\n",
    "        print(f\"Total RNN input size: {continuous_input_size} (continuous) + {total_embedding_size} (embeddings) = {rnn_input_size}\")\n",
    "        \n",
    "        # Map string name to PyTorch RNN class\n",
    "        rnn_map = {\n",
    "            'RNN': nn.RNN,\n",
    "            'LSTM': nn.LSTM,\n",
    "            'GRU': nn.GRU\n",
    "        }\n",
    "        \n",
    "        if rnn_type not in rnn_map:\n",
    "            raise ValueError(\"rnn_type must be 'RNN', 'LSTM', or 'GRU'\")\n",
    "        \n",
    "        rnn_module = rnn_map[rnn_type]\n",
    "        \n",
    "        # Dropout is only applied between layers (if num_layers > 1)\n",
    "        dropout_val = dropout_rate if num_layers > 1 else 0\n",
    "        \n",
    "        # Create the recurrent layer\n",
    "        self.rnn = rnn_module(\n",
    "            input_size=rnn_input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional,\n",
    "            dropout=dropout_val\n",
    "        )\n",
    "        \n",
    "        # Calculate input size for the final classifier\n",
    "        if self.bidirectional:\n",
    "            classifier_input_size = hidden_size * 2\n",
    "        else:\n",
    "            classifier_input_size = hidden_size\n",
    "        \n",
    "        # Final classification layer\n",
    "        self.classifier = nn.Linear(classifier_input_size, num_classes)\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "    \n",
    "    def forward(self, x, categorical_indices):\n",
    "        \"\"\"\n",
    "        x: (batch_size, seq_length, continuous_features)\n",
    "        categorical_indices: Dict of {feature_name: (batch_size, seq_length)} tensors\n",
    "        \"\"\"\n",
    "        batch_size, seq_length, _ = x.shape\n",
    "        \n",
    "        # Process embeddings for each categorical feature\n",
    "        embedded_features = []\n",
    "        \n",
    "        for feature_name in self.categorical_features:\n",
    "            if feature_name in categorical_indices:\n",
    "                # Get the categorical indices for this feature\n",
    "                cat_indices = categorical_indices[feature_name]  # (batch_size, seq_length)\n",
    "                \n",
    "                # Apply embedding\n",
    "                embedded = self.embeddings[feature_name](cat_indices)  # (batch_size, seq_length, embed_dim)\n",
    "                embedded_features.append(embedded)\n",
    "        \n",
    "        # Concatenate all embeddings with continuous features\n",
    "        if embedded_features:\n",
    "            all_embeddings = torch.cat(embedded_features, dim=-1)  # (batch_size, seq_length, total_embed_dim)\n",
    "            combined_input = torch.cat([x, all_embeddings], dim=-1)  # (batch_size, seq_length, total_input_size)\n",
    "        else:\n",
    "            combined_input = x\n",
    "        \n",
    "        # Pass through RNN\n",
    "        rnn_out, hidden = self.rnn(combined_input)\n",
    "        \n",
    "        # Use the last hidden state for classification\n",
    "        if self.rnn_type in ['LSTM', 'GRU']:\n",
    "            if isinstance(hidden, tuple):  # LSTM returns (h_n, c_n)\n",
    "                last_hidden = hidden[0]\n",
    "            else:  # GRU returns h_n\n",
    "                last_hidden = hidden\n",
    "        else:  # RNN\n",
    "            last_hidden = hidden\n",
    "        \n",
    "        # Get the last hidden state from the last layer\n",
    "        if self.bidirectional:\n",
    "            # Concatenate forward and backward directions\n",
    "            last_hidden = torch.cat([last_hidden[-2], last_hidden[-1]], dim=1)\n",
    "        else:\n",
    "            last_hidden = last_hidden[-1]\n",
    "        \n",
    "        # Apply dropout and classification\n",
    "        output = self.dropout(last_hidden)\n",
    "        output = self.classifier(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_sequences_enhanced(df, window=200, stride=200, categorical_features=['pain_survey_1', 'pain_survey_2', 'pain_survey_3', 'pain_survey_4']):\n",
    "    \"\"\"\n",
    "    Enhanced version of build_sequences that separates categorical and continuous features.\n",
    "    \n",
    "    Returns:\n",
    "    - continuous_data: numpy array of continuous features\n",
    "    - categorical_data: dict of categorical features\n",
    "    - labels: numpy array of labels\n",
    "    \"\"\"\n",
    "    # Sanity check to ensure the window is divisible by the stride\n",
    "    assert window % stride == 0\n",
    "    \n",
    "    # Separate categorical and continuous features\n",
    "    exclude_cols = ['sample_index', 'label', 'time'] + categorical_features\n",
    "    continuous_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "    \n",
    "    print(f\"Categorical features: {categorical_features}\")\n",
    "    print(f\"Continuous features: {continuous_cols}\")\n",
    "    print(f\"Number of continuous features: {len(continuous_cols)}\")\n",
    "    \n",
    "    # Initialize lists to store sequences and their corresponding labels\n",
    "    continuous_dataset = []\n",
    "    categorical_datasets = {feature: [] for feature in categorical_features}\n",
    "    labels = []\n",
    "    \n",
    "    # Iterate over unique IDs in the DataFrame\n",
    "    for id in df['sample_index'].unique():\n",
    "        # Extract data for the current sample index\n",
    "        pirate_data = df[df['sample_index'] == id]\n",
    "        \n",
    "        # Extract continuous features\n",
    "        continuous_temp = pirate_data[continuous_cols].values\n",
    "        \n",
    "        # Extract categorical features\n",
    "        categorical_temps = {}\n",
    "        for feature in categorical_features:\n",
    "            categorical_temps[feature] = pirate_data[feature].values\n",
    "        \n",
    "        # Retrieve the label for the current pirate\n",
    "        label = pirate_data['label'].values[0]\n",
    "        \n",
    "        # Calculate padding length to ensure full windows\n",
    "        padding_len = window - len(continuous_temp) % window\n",
    "        \n",
    "        # Create zero padding for continuous features\n",
    "        continuous_padding = np.zeros((padding_len, len(continuous_cols)), dtype='float32')\n",
    "        continuous_temp = np.concatenate((continuous_temp, continuous_padding))\n",
    "        \n",
    "        # Create padding for categorical features (use 0 as padding)\n",
    "        for feature in categorical_features:\n",
    "            categorical_padding = np.zeros(padding_len, dtype='int64')\n",
    "            categorical_temps[feature] = np.concatenate((categorical_temps[feature], categorical_padding))\n",
    "        \n",
    "        # Build feature windows and associate them with labels\n",
    "        idx = 0\n",
    "        while idx + window <= len(continuous_temp):\n",
    "            # Continuous features\n",
    "            continuous_dataset.append(continuous_temp[idx:idx + window])\n",
    "            \n",
    "            # Categorical features\n",
    "            for feature in categorical_features:\n",
    "                categorical_datasets[feature].append(categorical_temps[feature][idx:idx + window])\n",
    "            \n",
    "            labels.append(label)\n",
    "            idx += stride\n",
    "    \n",
    "    # Convert lists to numpy arrays for further processing\n",
    "    continuous_dataset = np.array(continuous_dataset, dtype='float32')\n",
    "    for feature in categorical_features:\n",
    "        categorical_datasets[feature] = np.array(categorical_datasets[feature], dtype='int64')\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    print(f\"Continuous data shape: {continuous_dataset.shape}\")\n",
    "    for feature in categorical_features:\n",
    "        print(f\"{feature} shape: {categorical_datasets[feature].shape}\")\n",
    "    print(f\"Labels shape: {labels.shape}\")\n",
    "    \n",
    "    return continuous_dataset, categorical_datasets, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Building Enhanced Sequences ===\n",
      "Categorical features: ['pain_survey_1', 'pain_survey_2', 'pain_survey_3', 'pain_survey_4']\n",
      "Continuous features: ['joint_00', 'joint_01', 'joint_02', 'joint_03', 'joint_04', 'joint_05', 'joint_06', 'joint_07', 'joint_08', 'joint_09', 'joint_10', 'joint_11', 'joint_12', 'joint_26', 'joint_27', 'joint_28', 'joint_29']\n",
      "Number of continuous features: 17\n",
      "Continuous data shape: (7968, 50, 17)\n",
      "pain_survey_1 shape: (7968, 50)\n",
      "pain_survey_2 shape: (7968, 50)\n",
      "pain_survey_3 shape: (7968, 50)\n",
      "pain_survey_4 shape: (7968, 50)\n",
      "Labels shape: (7968,)\n",
      "Categorical features: ['pain_survey_1', 'pain_survey_2', 'pain_survey_3', 'pain_survey_4']\n",
      "Continuous features: ['joint_00', 'joint_01', 'joint_02', 'joint_03', 'joint_04', 'joint_05', 'joint_06', 'joint_07', 'joint_08', 'joint_09', 'joint_10', 'joint_11', 'joint_12', 'joint_26', 'joint_27', 'joint_28', 'joint_29']\n",
      "Number of continuous features: 17\n",
      "Continuous data shape: (2560, 50, 17)\n",
      "pain_survey_1 shape: (2560, 50)\n",
      "pain_survey_2 shape: (2560, 50)\n",
      "pain_survey_3 shape: (2560, 50)\n",
      "pain_survey_4 shape: (2560, 50)\n",
      "Labels shape: (2560,)\n",
      "Categorical features: ['pain_survey_1', 'pain_survey_2', 'pain_survey_3', 'pain_survey_4']\n",
      "Continuous features: ['joint_00', 'joint_01', 'joint_02', 'joint_03', 'joint_04', 'joint_05', 'joint_06', 'joint_07', 'joint_08', 'joint_09', 'joint_10', 'joint_11', 'joint_12', 'joint_26', 'joint_27', 'joint_28', 'joint_29']\n",
      "Number of continuous features: 17\n",
      "Continuous data shape: (48, 50, 17)\n",
      "pain_survey_1 shape: (48, 50)\n",
      "pain_survey_2 shape: (48, 50)\n",
      "pain_survey_3 shape: (48, 50)\n",
      "pain_survey_4 shape: (48, 50)\n",
      "Labels shape: (48,)\n",
      "\n",
      "=== Enhanced Dataset Shapes ===\n",
      "Training: continuous (7968, 50, 17), labels (7968,)\n",
      "Validation: continuous (2560, 50, 17), labels (2560,)\n",
      "Test: continuous (48, 50, 17), labels (48,)\n",
      "\n",
      "Categorical feature configuration: {'pain_survey_1': 3, 'pain_survey_2': 3, 'pain_survey_3': 3, 'pain_survey_4': 3}\n",
      "Continuous feature size: 17\n"
     ]
    }
   ],
   "source": [
    "# Generate enhanced sequences for training, validation, and test sets\n",
    "print(\"=== Building Enhanced Sequences ===\")\n",
    "\n",
    "# Build sequences with separated categorical and continuous features\n",
    "X_train_cont, X_train_cat, y_train_enh = build_sequences_enhanced(df_train, WINDOW_SIZE, STRIDE)\n",
    "X_val_cont, X_val_cat, y_val_enh = build_sequences_enhanced(df_val, WINDOW_SIZE, STRIDE)\n",
    "X_test_cont, X_test_cat, y_test_enh = build_sequences_enhanced(df_test, WINDOW_SIZE, STRIDE)\n",
    "\n",
    "print(\"\\n=== Enhanced Dataset Shapes ===\")\n",
    "print(f\"Training: continuous {X_train_cont.shape}, labels {y_train_enh.shape}\")\n",
    "print(f\"Validation: continuous {X_val_cont.shape}, labels {y_val_enh.shape}\")  \n",
    "print(f\"Test: continuous {X_test_cont.shape}, labels {y_test_enh.shape}\")\n",
    "\n",
    "# Define categorical feature parameters for the model\n",
    "categorical_feature_config = {\n",
    "    'pain_survey_1': 3,  # values: 0, 1, 2\n",
    "    'pain_survey_2': 3,  # values: 0, 1, 2\n",
    "    'pain_survey_3': 3,  # values: 0, 1, 2\n",
    "    'pain_survey_4': 3   # values: 0, 1, 2\n",
    "}\n",
    "\n",
    "print(f\"\\nCategorical feature configuration: {categorical_feature_config}\")\n",
    "print(f\"Continuous feature size: {X_train_cont.shape[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Creating Enhanced Datasets ===\n",
      "Enhanced dataset sizes:\n",
      "Train: 7968 samples\n",
      "Val: 2560 samples\n",
      "Test: 48 samples\n"
     ]
    }
   ],
   "source": [
    "# # Create custom dataset class for enhanced model\n",
    "class EnhancedDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, continuous_data, categorical_data, labels):\n",
    "        \"\"\"\n",
    "        Dataset for enhanced model with embeddings\n",
    "        \n",
    "        Args:\n",
    "            continuous_data: numpy array of shape (N, seq_len, continuous_features)\n",
    "            categorical_data: dict of {feature_name: numpy array of shape (N, seq_len)}\n",
    "            labels: numpy array of shape (N,)\n",
    "        \"\"\"\n",
    "        self.continuous_data = torch.FloatTensor(continuous_data)\n",
    "        self.categorical_data = {}\n",
    "        for feature, data in categorical_data.items():\n",
    "            self.categorical_data[feature] = torch.LongTensor(data)\n",
    "        self.labels = torch.LongTensor(labels)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        continuous = self.continuous_data[idx]\n",
    "        categorical = {feature: data[idx] for feature, data in self.categorical_data.items()}\n",
    "        label = self.labels[idx]\n",
    "        return continuous, categorical, label\n",
    "\n",
    "# Create enhanced datasets\n",
    "print(\"=== Creating Enhanced Datasets ===\")\n",
    "\n",
    "train_enhanced_ds = EnhancedDataset(X_train_cont, X_train_cat, y_train_enh)\n",
    "val_enhanced_ds = EnhancedDataset(X_val_cont, X_val_cat, y_val_enh)\n",
    "test_enhanced_ds = EnhancedDataset(X_test_cont, X_test_cat, y_test_enh)\n",
    "\n",
    "print(f\"Enhanced dataset sizes:\")\n",
    "print(f\"Train: {len(train_enhanced_ds)} samples\")\n",
    "print(f\"Val: {len(val_enhanced_ds)} samples\") \n",
    "print(f\"Test: {len(test_enhanced_ds)} samples\")\n",
    "\n",
    "# Create enhanced data loaders\n",
    "def collate_enhanced(batch):\n",
    "    \"\"\"Custom collate function for enhanced dataset\"\"\"\n",
    "    continuous_batch = torch.stack([item[0] for item in batch])\n",
    "    \n",
    "    categorical_batch = {}\n",
    "    if batch:  # Check if batch is not empty\n",
    "        # Get feature names from first item\n",
    "        feature_names = batch[0][1].keys()\n",
    "        for feature in feature_names:\n",
    "            categorical_batch[feature] = torch.stack([item[1][feature] for item in batch])\n",
    "    \n",
    "    labels_batch = torch.stack([item[2] for item in batch])\n",
    "    \n",
    "    return continuous_batch, categorical_batch, labels_batch\n",
    "\n",
    "# # Create data loaders with custom collate function\n",
    "# train_enhanced_loader = DataLoader(\n",
    "#     train_enhanced_ds, \n",
    "#     batch_size=BATCH_SIZE, \n",
    "#     shuffle=True, \n",
    "#     collate_fn=collate_enhanced,\n",
    "#     num_workers=2,\n",
    "#     pin_memory=True\n",
    "# )\n",
    "\n",
    "# val_enhanced_loader = DataLoader(\n",
    "#     val_enhanced_ds, \n",
    "#     batch_size=BATCH_SIZE, \n",
    "#     shuffle=False, \n",
    "#     collate_fn=collate_enhanced,\n",
    "#     num_workers=2,\n",
    "#     pin_memory=True\n",
    "# )\n",
    "\n",
    "# test_enhanced_loader = DataLoader(\n",
    "#     test_enhanced_ds, \n",
    "#     batch_size=BATCH_SIZE, \n",
    "#     shuffle=False, \n",
    "#     collate_fn=collate_enhanced,\n",
    "#     num_workers=2,\n",
    "#     pin_memory=True\n",
    "# )\n",
    "\n",
    "# print(f\"Enhanced data loaders created successfully!\")\n",
    "\n",
    "# # Test the enhanced data loader\n",
    "# print(f\"\\\\n=== Testing Enhanced Data Loader ===\")\n",
    "# for continuous_batch, categorical_batch, labels_batch in train_enhanced_loader:\n",
    "#     print(f\"Batch continuous shape: {continuous_batch.shape}\")\n",
    "#     print(f\"Batch labels shape: {labels_batch.shape}\")\n",
    "#     for feature, data in categorical_batch.items():\n",
    "#         print(f\"Batch {feature} shape: {data.shape}\")\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Creating Enhanced Datasets (CORRECTED) ===\n",
      "‚úÖ Enhanced data loaders created successfully with custom collate function!\n",
      "\\n=== Testing Corrected Enhanced Data Loader ===\n",
      "‚úÖ Batch continuous shape: torch.Size([480, 50, 17])\n",
      "‚úÖ Batch labels shape: torch.Size([480])\n",
      "‚úÖ Batch pain_survey_1 shape: torch.Size([480, 50])\n",
      "‚úÖ Batch pain_survey_2 shape: torch.Size([480, 50])\n",
      "‚úÖ Batch pain_survey_3 shape: torch.Size([480, 50])\n",
      "‚úÖ Batch pain_survey_4 shape: torch.Size([480, 50])\n",
      "\\n‚úÖ Enhanced data loaders are now CORRECTLY configured!\n"
     ]
    }
   ],
   "source": [
    "# Create enhanced datasets (CORRECTED VERSION)\n",
    "print(\"=== Creating Enhanced Datasets (CORRECTED) ===\")\n",
    "# ‚úÖ SOLUTION: Create enhanced data loaders with custom collate function\n",
    "\n",
    "def make_enhanced_loader(ds, batch_size, shuffle, drop_last, sampler=None):\n",
    "    \"\"\"\n",
    "    Enhanced version of make_loader with custom collate function for enhanced datasets\n",
    "    \"\"\"\n",
    "    def collate_enhanced(batch):\n",
    "        \"\"\"Custom collate function for enhanced dataset\"\"\"\n",
    "        continuous_batch = torch.stack([item[0] for item in batch])\n",
    "        \n",
    "        categorical_batch = {}\n",
    "        if batch:  # Check if batch is not empty\n",
    "            # Get feature names from first item\n",
    "            feature_names = batch[0][1].keys()\n",
    "            for feature in feature_names:\n",
    "                categorical_batch[feature] = torch.stack([item[1][feature] for item in batch])\n",
    "        \n",
    "        labels_batch = torch.stack([item[2] for item in batch])\n",
    "        \n",
    "        return continuous_batch, categorical_batch, labels_batch\n",
    "    \n",
    "    # Use single thread to avoid pickle issues with local functions\n",
    "    num_workers = 0  # Avoid multiprocessing issues with local collate function\n",
    "    \n",
    "    return DataLoader(\n",
    "        ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle if sampler is None else False,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True if torch.cuda.is_available() else False,\n",
    "        collate_fn=collate_enhanced, \n",
    "        sampler=sampler\n",
    "    )\n",
    "\n",
    "# Create enhanced data loaders with CORRECT configuration\n",
    "train_enhanced_loader = make_enhanced_loader(\n",
    "                        train_enhanced_ds,\n",
    "                        batch_size=BATCH_SIZE,\n",
    "                        shuffle=True,\n",
    "                        drop_last=False,\n",
    "                        sampler=None\n",
    "                    )\n",
    "\n",
    "val_enhanced_loader = make_enhanced_loader(\n",
    "                        val_enhanced_ds,\n",
    "                        batch_size=BATCH_SIZE,\n",
    "                        shuffle=False, \n",
    "                        drop_last=False,\n",
    "                        sampler=None\n",
    "                    )\n",
    "\n",
    "test_enhanced_loader = make_enhanced_loader(\n",
    "                        test_enhanced_ds,\n",
    "                        batch_size=BATCH_SIZE,\n",
    "                        shuffle=False, \n",
    "                        drop_last=False,\n",
    "                        sampler=None\n",
    "                    )\n",
    "\n",
    "print(f\"‚úÖ Enhanced data loaders created successfully with custom collate function!\")\n",
    "\n",
    "# Test the corrected enhanced data loader\n",
    "print(f\"\\\\n=== Testing Corrected Enhanced Data Loader ===\")\n",
    "for continuous_batch, categorical_batch, labels_batch in train_enhanced_loader:\n",
    "    print(f\"‚úÖ Batch continuous shape: {continuous_batch.shape}\")\n",
    "    print(f\"‚úÖ Batch labels shape: {labels_batch.shape}\")\n",
    "    for feature, data in categorical_batch.items():\n",
    "        print(f\"‚úÖ Batch {feature} shape: {data.shape}\")\n",
    "    break\n",
    "\n",
    "print(f\"\\\\n‚úÖ Enhanced data loaders are now CORRECTLY configured!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Enhanced Model\n",
    "\n",
    "Now you can train the enhanced model using the same training loop structure as your original model, but with the new data loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Creating Enhanced Model for Training Demo ===\n",
      "Created embedding for pain_survey_1: 3 -> 2\n",
      "Created embedding for pain_survey_2: 3 -> 2\n",
      "Created embedding for pain_survey_3: 3 -> 2\n",
      "Created embedding for pain_survey_4: 3 -> 2\n",
      "Total RNN input size: 17 (continuous) + 8 (embeddings) = 25\n",
      "Demo model parameters: 66,479\n",
      "üìä Available scheduler options:\n",
      "  ‚Ä¢ scheduler_f1: Focuses on F1 score improvement\n",
      "  ‚Ä¢ scheduler_loss: Focuses on validation loss reduction\n",
      "  ‚Ä¢ scheduler_combined: Uses combined F1 and loss metric\n",
      "üéØ Currently using: ReduceLROnPlateau with mode='max'\n",
      "Ready for training!\n",
      "Starting enhanced model training with Early Stopping...\n",
      "üîß Regularization: L1=1.00e-06, L2=1.00e-04\n",
      "üõë Early Stopping: Patience=100 epochs, Min improvement=0.0010\n",
      "üìä Scheduler Metric: combined\n",
      "üìà Using ReduceLROnPlateau scheduler - Mode: max, Factor: 0.8, Patience: 25\n",
      "‚úÖ NEW BEST F1: 0.6768 at epoch 1 (improvement: +0.0010)\n",
      "üîΩ NEW BEST VAL LOSS: 0.2594 at epoch 1 (improvement: -0.0010)\n",
      "Epoch   1/500 | Train Loss: 0.3556 | Val Loss: 0.2594 | Train F1: 0.6685 | Val F1: 0.6768 | LR: 1.00e-03\n",
      "         Best F1: 0.6768 (Ep 1) | Best Loss: 0.2594 (Ep 1) | No F1 Improve: 0/100 | No Loss Improve: 0/100\n",
      "üîΩ NEW BEST VAL LOSS: 0.2497 at epoch 2 (improvement: -0.0010)\n",
      "Epoch   2/500 | Train Loss: 0.3063 | Val Loss: 0.2497 | Train F1: 0.6742 | Val F1: 0.6768 | LR: 1.00e-03\n",
      "         Best F1: 0.6768 (Ep 1) | Best Loss: 0.2497 (Ep 2) | No F1 Improve: 1/100 | No Loss Improve: 0/100\n",
      "üîΩ NEW BEST VAL LOSS: 0.2443 at epoch 3 (improvement: -0.0010)\n",
      "Epoch   3/500 | Train Loss: 0.2942 | Val Loss: 0.2443 | Train F1: 0.6742 | Val F1: 0.6768 | LR: 1.00e-03\n",
      "         Best F1: 0.6768 (Ep 1) | Best Loss: 0.2443 (Ep 3) | No F1 Improve: 2/100 | No Loss Improve: 0/100\n",
      "‚úÖ NEW BEST F1: 0.6867 at epoch 4 (improvement: +0.0010)\n",
      "üîΩ NEW BEST VAL LOSS: 0.2327 at epoch 4 (improvement: -0.0010)\n",
      "Epoch   4/500 | Train Loss: 0.2781 | Val Loss: 0.2327 | Train F1: 0.6785 | Val F1: 0.6867 | LR: 1.00e-03\n",
      "         Best F1: 0.6867 (Ep 4) | Best Loss: 0.2327 (Ep 4) | No F1 Improve: 0/100 | No Loss Improve: 0/100\n",
      "‚úÖ NEW BEST F1: 0.7603 at epoch 5 (improvement: +0.0010)\n",
      "üîΩ NEW BEST VAL LOSS: 0.2150 at epoch 5 (improvement: -0.0010)\n",
      "Epoch   5/500 | Train Loss: 0.2574 | Val Loss: 0.2150 | Train F1: 0.7337 | Val F1: 0.7603 | LR: 1.00e-03\n",
      "         Best F1: 0.7603 (Ep 5) | Best Loss: 0.2150 (Ep 5) | No F1 Improve: 0/100 | No Loss Improve: 0/100\n",
      "‚úÖ NEW BEST F1: 0.7628 at epoch 6 (improvement: +0.0010)\n",
      "üîΩ NEW BEST VAL LOSS: 0.2105 at epoch 6 (improvement: -0.0010)\n",
      "Epoch   6/500 | Train Loss: 0.2446 | Val Loss: 0.2105 | Train F1: 0.7536 | Val F1: 0.7628 | LR: 1.00e-03\n",
      "         Best F1: 0.7628 (Ep 6) | Best Loss: 0.2105 (Ep 6) | No F1 Improve: 0/100 | No Loss Improve: 0/100\n",
      "‚úÖ NEW BEST F1: 0.7765 at epoch 7 (improvement: +0.0010)\n",
      "üîΩ NEW BEST VAL LOSS: 0.1992 at epoch 7 (improvement: -0.0010)\n",
      "Epoch   7/500 | Train Loss: 0.2325 | Val Loss: 0.1992 | Train F1: 0.7729 | Val F1: 0.7765 | LR: 1.00e-03\n",
      "         Best F1: 0.7765 (Ep 7) | Best Loss: 0.1992 (Ep 7) | No F1 Improve: 0/100 | No Loss Improve: 0/100\n",
      "‚úÖ NEW BEST F1: 0.7787 at epoch 8 (improvement: +0.0010)\n",
      "üîΩ NEW BEST VAL LOSS: 0.1888 at epoch 8 (improvement: -0.0010)\n",
      "Epoch   8/500 | Train Loss: 0.2191 | Val Loss: 0.1888 | Train F1: 0.7841 | Val F1: 0.7787 | LR: 1.00e-03\n",
      "         Best F1: 0.7787 (Ep 8) | Best Loss: 0.1888 (Ep 8) | No F1 Improve: 0/100 | No Loss Improve: 0/100\n",
      "‚úÖ NEW BEST F1: 0.8062 at epoch 9 (improvement: +0.0010)\n",
      "üîΩ NEW BEST VAL LOSS: 0.1704 at epoch 9 (improvement: -0.0010)\n",
      "Epoch   9/500 | Train Loss: 0.2072 | Val Loss: 0.1704 | Train F1: 0.8056 | Val F1: 0.8062 | LR: 1.00e-03\n",
      "         Best F1: 0.8062 (Ep 9) | Best Loss: 0.1704 (Ep 9) | No F1 Improve: 0/100 | No Loss Improve: 0/100\n",
      "‚úÖ NEW BEST F1: 0.8220 at epoch 10 (improvement: +0.0010)\n",
      "üîΩ NEW BEST VAL LOSS: 0.1484 at epoch 10 (improvement: -0.0010)\n",
      "Epoch  10/500 | Train Loss: 0.1942 | Val Loss: 0.1484 | Train F1: 0.8200 | Val F1: 0.8220 | LR: 1.00e-03\n",
      "         Best F1: 0.8220 (Ep 10) | Best Loss: 0.1484 (Ep 10) | No F1 Improve: 0/100 | No Loss Improve: 0/100\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch  11/500 | Train Loss: 0.1882 | Val Loss: 0.1635 | Train F1: 0.8278 | Val F1: 0.7963 | LR: 1.00e-03\n",
      "         Best F1: 0.8220 (Ep 10) | Best Loss: 0.1484 (Ep 10) | No F1 Improve: 1/100 | No Loss Improve: 1/100\n",
      "‚úÖ NEW BEST F1: 0.8645 at epoch 12 (improvement: +0.0010)\n",
      "üîΩ NEW BEST VAL LOSS: 0.1339 at epoch 12 (improvement: -0.0010)\n",
      "Epoch  12/500 | Train Loss: 0.1778 | Val Loss: 0.1339 | Train F1: 0.8330 | Val F1: 0.8645 | LR: 1.00e-03\n",
      "         Best F1: 0.8645 (Ep 12) | Best Loss: 0.1339 (Ep 12) | No F1 Improve: 0/100 | No Loss Improve: 0/100\n",
      "üîΩ NEW BEST VAL LOSS: 0.1291 at epoch 13 (improvement: -0.0010)\n",
      "Epoch  13/500 | Train Loss: 0.1590 | Val Loss: 0.1291 | Train F1: 0.8523 | Val F1: 0.8540 | LR: 1.00e-03\n",
      "         Best F1: 0.8645 (Ep 12) | Best Loss: 0.1291 (Ep 13) | No F1 Improve: 1/100 | No Loss Improve: 0/100\n",
      "Epoch  14/500 | Train Loss: 0.1555 | Val Loss: 0.1516 | Train F1: 0.8561 | Val F1: 0.8480 | LR: 1.00e-03\n",
      "         Best F1: 0.8645 (Ep 12) | Best Loss: 0.1291 (Ep 13) | No F1 Improve: 2/100 | No Loss Improve: 1/100\n",
      "üîΩ NEW BEST VAL LOSS: 0.1257 at epoch 15 (improvement: -0.0010)\n",
      "Epoch  15/500 | Train Loss: 0.1525 | Val Loss: 0.1257 | Train F1: 0.8615 | Val F1: 0.8549 | LR: 1.00e-03\n",
      "         Best F1: 0.8645 (Ep 12) | Best Loss: 0.1257 (Ep 15) | No F1 Improve: 3/100 | No Loss Improve: 0/100\n",
      "Epoch  16/500 | Train Loss: 0.1362 | Val Loss: 0.1260 | Train F1: 0.8765 | Val F1: 0.8630 | LR: 1.00e-03\n",
      "         Best F1: 0.8645 (Ep 12) | Best Loss: 0.1257 (Ep 15) | No F1 Improve: 4/100 | No Loss Improve: 1/100\n",
      "Epoch  17/500 | Train Loss: 0.1277 | Val Loss: 0.1262 | Train F1: 0.8857 | Val F1: 0.8650 | LR: 1.00e-03\n",
      "         Best F1: 0.8645 (Ep 12) | Best Loss: 0.1257 (Ep 15) | No F1 Improve: 5/100 | No Loss Improve: 2/100\n",
      "‚úÖ NEW BEST F1: 0.8702 at epoch 18 (improvement: +0.0010)\n",
      "üîΩ NEW BEST VAL LOSS: 0.1159 at epoch 18 (improvement: -0.0010)\n",
      "Epoch  18/500 | Train Loss: 0.1196 | Val Loss: 0.1159 | Train F1: 0.8916 | Val F1: 0.8702 | LR: 1.00e-03\n",
      "         Best F1: 0.8702 (Ep 18) | Best Loss: 0.1159 (Ep 18) | No F1 Improve: 0/100 | No Loss Improve: 0/100\n",
      "Epoch  19/500 | Train Loss: 0.1201 | Val Loss: 0.1164 | Train F1: 0.8909 | Val F1: 0.8658 | LR: 1.00e-03\n",
      "         Best F1: 0.8702 (Ep 18) | Best Loss: 0.1159 (Ep 18) | No F1 Improve: 1/100 | No Loss Improve: 1/100\n",
      "üîΩ NEW BEST VAL LOSS: 0.1131 at epoch 20 (improvement: -0.0010)\n",
      "Epoch  20/500 | Train Loss: 0.1098 | Val Loss: 0.1131 | Train F1: 0.8952 | Val F1: 0.8675 | LR: 1.00e-03\n",
      "         Best F1: 0.8702 (Ep 18) | Best Loss: 0.1131 (Ep 20) | No F1 Improve: 2/100 | No Loss Improve: 0/100\n",
      "----------------------------------------------------------------------------------------------------\n",
      "‚úÖ NEW BEST F1: 0.8712 at epoch 21 (improvement: +0.0010)\n",
      "Epoch  21/500 | Train Loss: 0.1075 | Val Loss: 0.1194 | Train F1: 0.8963 | Val F1: 0.8712 | LR: 1.00e-03\n",
      "         Best F1: 0.8712 (Ep 21) | Best Loss: 0.1131 (Ep 20) | No F1 Improve: 0/100 | No Loss Improve: 1/100\n",
      "‚úÖ NEW BEST F1: 0.8782 at epoch 22 (improvement: +0.0010)\n",
      "üîΩ NEW BEST VAL LOSS: 0.1101 at epoch 22 (improvement: -0.0010)\n",
      "Epoch  22/500 | Train Loss: 0.1134 | Val Loss: 0.1101 | Train F1: 0.8949 | Val F1: 0.8782 | LR: 1.00e-03\n",
      "         Best F1: 0.8782 (Ep 22) | Best Loss: 0.1101 (Ep 22) | No F1 Improve: 0/100 | No Loss Improve: 0/100\n",
      "‚úÖ NEW BEST F1: 0.8828 at epoch 23 (improvement: +0.0010)\n",
      "üîΩ NEW BEST VAL LOSS: 0.1074 at epoch 23 (improvement: -0.0010)\n",
      "Epoch  23/500 | Train Loss: 0.1051 | Val Loss: 0.1074 | Train F1: 0.9018 | Val F1: 0.8828 | LR: 1.00e-03\n",
      "         Best F1: 0.8828 (Ep 23) | Best Loss: 0.1074 (Ep 23) | No F1 Improve: 0/100 | No Loss Improve: 0/100\n",
      "‚úÖ NEW BEST F1: 0.8855 at epoch 24 (improvement: +0.0010)\n",
      "Epoch  24/500 | Train Loss: 0.1015 | Val Loss: 0.1367 | Train F1: 0.9037 | Val F1: 0.8855 | LR: 1.00e-03\n",
      "         Best F1: 0.8855 (Ep 24) | Best Loss: 0.1074 (Ep 23) | No F1 Improve: 0/100 | No Loss Improve: 1/100\n",
      "Epoch  25/500 | Train Loss: 0.1114 | Val Loss: 0.1138 | Train F1: 0.8985 | Val F1: 0.8781 | LR: 1.00e-03\n",
      "         Best F1: 0.8855 (Ep 24) | Best Loss: 0.1074 (Ep 23) | No F1 Improve: 1/100 | No Loss Improve: 2/100\n",
      "‚úÖ NEW BEST F1: 0.8900 at epoch 26 (improvement: +0.0010)\n",
      "Epoch  26/500 | Train Loss: 0.0985 | Val Loss: 0.1143 | Train F1: 0.9126 | Val F1: 0.8900 | LR: 1.00e-03\n",
      "         Best F1: 0.8900 (Ep 26) | Best Loss: 0.1074 (Ep 23) | No F1 Improve: 0/100 | No Loss Improve: 3/100\n",
      "‚úÖ NEW BEST F1: 0.8982 at epoch 27 (improvement: +0.0010)\n",
      "  üîΩ Learning rate reduced from 1.00e-03 to 8.00e-04 (based on combined)\n",
      "Epoch  27/500 | Train Loss: 0.0945 | Val Loss: 0.1135 | Train F1: 0.9135 | Val F1: 0.8982 | LR: 8.00e-04\n",
      "         Best F1: 0.8982 (Ep 27) | Best Loss: 0.1074 (Ep 23) | No F1 Improve: 0/100 | No Loss Improve: 4/100\n",
      "Epoch  28/500 | Train Loss: 0.0966 | Val Loss: 0.1069 | Train F1: 0.9091 | Val F1: 0.8806 | LR: 8.00e-04\n",
      "         Best F1: 0.8982 (Ep 27) | Best Loss: 0.1074 (Ep 23) | No F1 Improve: 1/100 | No Loss Improve: 5/100\n",
      "Epoch  29/500 | Train Loss: 0.0893 | Val Loss: 0.1081 | Train F1: 0.9170 | Val F1: 0.8869 | LR: 8.00e-04\n",
      "         Best F1: 0.8982 (Ep 27) | Best Loss: 0.1074 (Ep 23) | No F1 Improve: 2/100 | No Loss Improve: 6/100\n",
      "üîΩ NEW BEST VAL LOSS: 0.1026 at epoch 30 (improvement: -0.0010)\n",
      "Epoch  30/500 | Train Loss: 0.0822 | Val Loss: 0.1026 | Train F1: 0.9226 | Val F1: 0.8992 | LR: 8.00e-04\n",
      "         Best F1: 0.8982 (Ep 27) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 3/100 | No Loss Improve: 0/100\n",
      "----------------------------------------------------------------------------------------------------\n",
      "‚úÖ NEW BEST F1: 0.9015 at epoch 31 (improvement: +0.0010)\n",
      "Epoch  31/500 | Train Loss: 0.0741 | Val Loss: 0.1039 | Train F1: 0.9334 | Val F1: 0.9015 | LR: 8.00e-04\n",
      "         Best F1: 0.9015 (Ep 31) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 0/100 | No Loss Improve: 1/100\n",
      "Epoch  32/500 | Train Loss: 0.0724 | Val Loss: 0.1110 | Train F1: 0.9350 | Val F1: 0.8997 | LR: 8.00e-04\n",
      "         Best F1: 0.9015 (Ep 31) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 1/100 | No Loss Improve: 2/100\n",
      "‚úÖ NEW BEST F1: 0.9034 at epoch 33 (improvement: +0.0010)\n",
      "Epoch  33/500 | Train Loss: 0.0677 | Val Loss: 0.1144 | Train F1: 0.9404 | Val F1: 0.9034 | LR: 8.00e-04\n",
      "         Best F1: 0.9034 (Ep 33) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 0/100 | No Loss Improve: 3/100\n",
      "Epoch  34/500 | Train Loss: 0.0683 | Val Loss: 0.1208 | Train F1: 0.9383 | Val F1: 0.8979 | LR: 8.00e-04\n",
      "         Best F1: 0.9034 (Ep 33) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 1/100 | No Loss Improve: 4/100\n",
      "Epoch  35/500 | Train Loss: 0.0678 | Val Loss: 0.1234 | Train F1: 0.9400 | Val F1: 0.9015 | LR: 8.00e-04\n",
      "         Best F1: 0.9034 (Ep 33) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 2/100 | No Loss Improve: 5/100\n",
      "Epoch  36/500 | Train Loss: 0.0761 | Val Loss: 0.1234 | Train F1: 0.9364 | Val F1: 0.9025 | LR: 8.00e-04\n",
      "         Best F1: 0.9034 (Ep 33) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 3/100 | No Loss Improve: 6/100\n",
      "Epoch  37/500 | Train Loss: 0.0739 | Val Loss: 0.1235 | Train F1: 0.9314 | Val F1: 0.8883 | LR: 8.00e-04\n",
      "         Best F1: 0.9034 (Ep 33) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 4/100 | No Loss Improve: 7/100\n",
      "Epoch  38/500 | Train Loss: 0.0928 | Val Loss: 0.1483 | Train F1: 0.9163 | Val F1: 0.8806 | LR: 8.00e-04\n",
      "         Best F1: 0.9034 (Ep 33) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 5/100 | No Loss Improve: 8/100\n",
      "Epoch  39/500 | Train Loss: 0.0889 | Val Loss: 0.1143 | Train F1: 0.9205 | Val F1: 0.8877 | LR: 8.00e-04\n",
      "         Best F1: 0.9034 (Ep 33) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 6/100 | No Loss Improve: 9/100\n",
      "Epoch  40/500 | Train Loss: 0.0698 | Val Loss: 0.1148 | Train F1: 0.9396 | Val F1: 0.8954 | LR: 8.00e-04\n",
      "         Best F1: 0.9034 (Ep 33) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 7/100 | No Loss Improve: 10/100\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch  41/500 | Train Loss: 0.0637 | Val Loss: 0.1293 | Train F1: 0.9445 | Val F1: 0.8940 | LR: 8.00e-04\n",
      "         Best F1: 0.9034 (Ep 33) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 8/100 | No Loss Improve: 11/100\n",
      "Epoch  42/500 | Train Loss: 0.0617 | Val Loss: 0.1305 | Train F1: 0.9455 | Val F1: 0.8954 | LR: 8.00e-04\n",
      "         Best F1: 0.9034 (Ep 33) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 9/100 | No Loss Improve: 12/100\n",
      "Epoch  43/500 | Train Loss: 0.0606 | Val Loss: 0.1256 | Train F1: 0.9451 | Val F1: 0.9041 | LR: 8.00e-04\n",
      "         Best F1: 0.9034 (Ep 33) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 10/100 | No Loss Improve: 13/100\n",
      "‚úÖ NEW BEST F1: 0.9045 at epoch 44 (improvement: +0.0010)\n",
      "Epoch  44/500 | Train Loss: 0.0565 | Val Loss: 0.1388 | Train F1: 0.9515 | Val F1: 0.9045 | LR: 8.00e-04\n",
      "         Best F1: 0.9045 (Ep 44) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 0/100 | No Loss Improve: 14/100\n",
      "Epoch  45/500 | Train Loss: 0.0550 | Val Loss: 0.1387 | Train F1: 0.9535 | Val F1: 0.9034 | LR: 8.00e-04\n",
      "         Best F1: 0.9045 (Ep 44) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 1/100 | No Loss Improve: 15/100\n",
      "Epoch  46/500 | Train Loss: 0.0586 | Val Loss: 0.1312 | Train F1: 0.9508 | Val F1: 0.9040 | LR: 8.00e-04\n",
      "         Best F1: 0.9045 (Ep 44) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 2/100 | No Loss Improve: 16/100\n",
      "‚úÖ NEW BEST F1: 0.9071 at epoch 47 (improvement: +0.0010)\n",
      "Epoch  47/500 | Train Loss: 0.0670 | Val Loss: 0.1304 | Train F1: 0.9400 | Val F1: 0.9071 | LR: 8.00e-04\n",
      "         Best F1: 0.9071 (Ep 47) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 0/100 | No Loss Improve: 17/100\n",
      "Epoch  48/500 | Train Loss: 0.0651 | Val Loss: 0.1336 | Train F1: 0.9418 | Val F1: 0.9037 | LR: 8.00e-04\n",
      "         Best F1: 0.9071 (Ep 47) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 1/100 | No Loss Improve: 18/100\n",
      "Epoch  49/500 | Train Loss: 0.0587 | Val Loss: 0.1258 | Train F1: 0.9503 | Val F1: 0.9028 | LR: 8.00e-04\n",
      "         Best F1: 0.9071 (Ep 47) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 2/100 | No Loss Improve: 19/100\n",
      "Epoch  50/500 | Train Loss: 0.0528 | Val Loss: 0.1326 | Train F1: 0.9563 | Val F1: 0.8984 | LR: 8.00e-04\n",
      "         Best F1: 0.9071 (Ep 47) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 3/100 | No Loss Improve: 20/100\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch  51/500 | Train Loss: 0.0561 | Val Loss: 0.1333 | Train F1: 0.9520 | Val F1: 0.8972 | LR: 8.00e-04\n",
      "         Best F1: 0.9071 (Ep 47) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 4/100 | No Loss Improve: 21/100\n",
      "Epoch  52/500 | Train Loss: 0.0557 | Val Loss: 0.1440 | Train F1: 0.9526 | Val F1: 0.8963 | LR: 8.00e-04\n",
      "         Best F1: 0.9071 (Ep 47) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 5/100 | No Loss Improve: 22/100\n",
      "  üîΩ Learning rate reduced from 8.00e-04 to 6.40e-04 (based on combined)\n",
      "Epoch  53/500 | Train Loss: 0.0514 | Val Loss: 0.1607 | Train F1: 0.9598 | Val F1: 0.9044 | LR: 6.40e-04\n",
      "         Best F1: 0.9071 (Ep 47) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 6/100 | No Loss Improve: 23/100\n",
      "Epoch  54/500 | Train Loss: 0.0479 | Val Loss: 0.1555 | Train F1: 0.9617 | Val F1: 0.8944 | LR: 6.40e-04\n",
      "         Best F1: 0.9071 (Ep 47) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 7/100 | No Loss Improve: 24/100\n",
      "Epoch  55/500 | Train Loss: 0.0483 | Val Loss: 0.1449 | Train F1: 0.9617 | Val F1: 0.9018 | LR: 6.40e-04\n",
      "         Best F1: 0.9071 (Ep 47) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 8/100 | No Loss Improve: 25/100\n",
      "Epoch  56/500 | Train Loss: 0.0520 | Val Loss: 0.1740 | Train F1: 0.9594 | Val F1: 0.8966 | LR: 6.40e-04\n",
      "         Best F1: 0.9071 (Ep 47) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 9/100 | No Loss Improve: 26/100\n",
      "‚úÖ NEW BEST F1: 0.9131 at epoch 57 (improvement: +0.0010)\n",
      "Epoch  57/500 | Train Loss: 0.0521 | Val Loss: 0.1381 | Train F1: 0.9569 | Val F1: 0.9131 | LR: 6.40e-04\n",
      "         Best F1: 0.9131 (Ep 57) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 0/100 | No Loss Improve: 27/100\n",
      "Epoch  58/500 | Train Loss: 0.0474 | Val Loss: 0.1450 | Train F1: 0.9637 | Val F1: 0.9036 | LR: 6.40e-04\n",
      "         Best F1: 0.9131 (Ep 57) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 1/100 | No Loss Improve: 28/100\n",
      "Epoch  59/500 | Train Loss: 0.0461 | Val Loss: 0.1541 | Train F1: 0.9644 | Val F1: 0.9062 | LR: 6.40e-04\n",
      "         Best F1: 0.9131 (Ep 57) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 2/100 | No Loss Improve: 29/100\n",
      "Epoch  60/500 | Train Loss: 0.0446 | Val Loss: 0.1574 | Train F1: 0.9656 | Val F1: 0.9087 | LR: 6.40e-04\n",
      "         Best F1: 0.9131 (Ep 57) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 3/100 | No Loss Improve: 30/100\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch  61/500 | Train Loss: 0.0451 | Val Loss: 0.1485 | Train F1: 0.9645 | Val F1: 0.9070 | LR: 6.40e-04\n",
      "         Best F1: 0.9131 (Ep 57) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 4/100 | No Loss Improve: 31/100\n",
      "Epoch  62/500 | Train Loss: 0.0446 | Val Loss: 0.1616 | Train F1: 0.9668 | Val F1: 0.8985 | LR: 6.40e-04\n",
      "         Best F1: 0.9131 (Ep 57) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 5/100 | No Loss Improve: 32/100\n",
      "Epoch  63/500 | Train Loss: 0.0471 | Val Loss: 0.1947 | Train F1: 0.9615 | Val F1: 0.8945 | LR: 6.40e-04\n",
      "         Best F1: 0.9131 (Ep 57) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 6/100 | No Loss Improve: 33/100\n",
      "Epoch  64/500 | Train Loss: 0.0494 | Val Loss: 0.1446 | Train F1: 0.9618 | Val F1: 0.9073 | LR: 6.40e-04\n",
      "         Best F1: 0.9131 (Ep 57) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 7/100 | No Loss Improve: 34/100\n",
      "Epoch  65/500 | Train Loss: 0.0433 | Val Loss: 0.1672 | Train F1: 0.9664 | Val F1: 0.8934 | LR: 6.40e-04\n",
      "         Best F1: 0.9131 (Ep 57) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 8/100 | No Loss Improve: 35/100\n",
      "Epoch  66/500 | Train Loss: 0.0450 | Val Loss: 0.1613 | Train F1: 0.9673 | Val F1: 0.8973 | LR: 6.40e-04\n",
      "         Best F1: 0.9131 (Ep 57) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 9/100 | No Loss Improve: 36/100\n",
      "Epoch  67/500 | Train Loss: 0.0442 | Val Loss: 0.1447 | Train F1: 0.9651 | Val F1: 0.9104 | LR: 6.40e-04\n",
      "         Best F1: 0.9131 (Ep 57) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 10/100 | No Loss Improve: 37/100\n",
      "Epoch  68/500 | Train Loss: 0.0417 | Val Loss: 0.1821 | Train F1: 0.9713 | Val F1: 0.9027 | LR: 6.40e-04\n",
      "         Best F1: 0.9131 (Ep 57) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 11/100 | No Loss Improve: 38/100\n",
      "Epoch  69/500 | Train Loss: 0.0430 | Val Loss: 0.1638 | Train F1: 0.9694 | Val F1: 0.9103 | LR: 6.40e-04\n",
      "         Best F1: 0.9131 (Ep 57) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 12/100 | No Loss Improve: 39/100\n",
      "Epoch  70/500 | Train Loss: 0.0445 | Val Loss: 0.1604 | Train F1: 0.9677 | Val F1: 0.9071 | LR: 6.40e-04\n",
      "         Best F1: 0.9131 (Ep 57) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 13/100 | No Loss Improve: 40/100\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch  71/500 | Train Loss: 0.0422 | Val Loss: 0.1532 | Train F1: 0.9705 | Val F1: 0.9014 | LR: 6.40e-04\n",
      "         Best F1: 0.9131 (Ep 57) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 14/100 | No Loss Improve: 41/100\n",
      "Epoch  72/500 | Train Loss: 0.0427 | Val Loss: 0.1494 | Train F1: 0.9666 | Val F1: 0.9062 | LR: 6.40e-04\n",
      "         Best F1: 0.9131 (Ep 57) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 15/100 | No Loss Improve: 42/100\n",
      "Epoch  73/500 | Train Loss: 0.0406 | Val Loss: 0.1663 | Train F1: 0.9722 | Val F1: 0.9078 | LR: 6.40e-04\n",
      "         Best F1: 0.9131 (Ep 57) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 16/100 | No Loss Improve: 43/100\n",
      "Epoch  74/500 | Train Loss: 0.0394 | Val Loss: 0.1782 | Train F1: 0.9733 | Val F1: 0.9061 | LR: 6.40e-04\n",
      "         Best F1: 0.9131 (Ep 57) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 17/100 | No Loss Improve: 44/100\n",
      "Epoch  75/500 | Train Loss: 0.0385 | Val Loss: 0.1605 | Train F1: 0.9757 | Val F1: 0.9093 | LR: 6.40e-04\n",
      "         Best F1: 0.9131 (Ep 57) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 18/100 | No Loss Improve: 45/100\n",
      "Epoch  76/500 | Train Loss: 0.0374 | Val Loss: 0.1669 | Train F1: 0.9757 | Val F1: 0.9100 | LR: 6.40e-04\n",
      "         Best F1: 0.9131 (Ep 57) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 19/100 | No Loss Improve: 46/100\n",
      "Epoch  77/500 | Train Loss: 0.0375 | Val Loss: 0.1658 | Train F1: 0.9750 | Val F1: 0.9092 | LR: 6.40e-04\n",
      "         Best F1: 0.9131 (Ep 57) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 20/100 | No Loss Improve: 47/100\n",
      "Epoch  78/500 | Train Loss: 0.0363 | Val Loss: 0.1642 | Train F1: 0.9779 | Val F1: 0.9083 | LR: 6.40e-04\n",
      "         Best F1: 0.9131 (Ep 57) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 21/100 | No Loss Improve: 48/100\n",
      "  üîΩ Learning rate reduced from 6.40e-04 to 5.12e-04 (based on combined)\n",
      "Epoch  79/500 | Train Loss: 0.0382 | Val Loss: 0.1814 | Train F1: 0.9754 | Val F1: 0.9045 | LR: 5.12e-04\n",
      "         Best F1: 0.9131 (Ep 57) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 22/100 | No Loss Improve: 49/100\n",
      "Epoch  80/500 | Train Loss: 0.0422 | Val Loss: 0.1725 | Train F1: 0.9691 | Val F1: 0.9056 | LR: 5.12e-04\n",
      "         Best F1: 0.9131 (Ep 57) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 23/100 | No Loss Improve: 50/100\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch  81/500 | Train Loss: 0.0380 | Val Loss: 0.1692 | Train F1: 0.9748 | Val F1: 0.8996 | LR: 5.12e-04\n",
      "         Best F1: 0.9131 (Ep 57) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 24/100 | No Loss Improve: 51/100\n",
      "Epoch  82/500 | Train Loss: 0.0366 | Val Loss: 0.1723 | Train F1: 0.9773 | Val F1: 0.9097 | LR: 5.12e-04\n",
      "         Best F1: 0.9131 (Ep 57) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 25/100 | No Loss Improve: 52/100\n",
      "Epoch  83/500 | Train Loss: 0.0349 | Val Loss: 0.1704 | Train F1: 0.9790 | Val F1: 0.9116 | LR: 5.12e-04\n",
      "         Best F1: 0.9131 (Ep 57) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 26/100 | No Loss Improve: 53/100\n",
      "Epoch  84/500 | Train Loss: 0.0340 | Val Loss: 0.1809 | Train F1: 0.9792 | Val F1: 0.9056 | LR: 5.12e-04\n",
      "         Best F1: 0.9131 (Ep 57) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 27/100 | No Loss Improve: 54/100\n",
      "Epoch  85/500 | Train Loss: 0.0346 | Val Loss: 0.1874 | Train F1: 0.9794 | Val F1: 0.9043 | LR: 5.12e-04\n",
      "         Best F1: 0.9131 (Ep 57) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 28/100 | No Loss Improve: 55/100\n",
      "Epoch  86/500 | Train Loss: 0.0354 | Val Loss: 0.1862 | Train F1: 0.9787 | Val F1: 0.9061 | LR: 5.12e-04\n",
      "         Best F1: 0.9131 (Ep 57) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 29/100 | No Loss Improve: 56/100\n",
      "Epoch  87/500 | Train Loss: 0.0403 | Val Loss: 0.1780 | Train F1: 0.9706 | Val F1: 0.9064 | LR: 5.12e-04\n",
      "         Best F1: 0.9131 (Ep 57) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 30/100 | No Loss Improve: 57/100\n",
      "Epoch  88/500 | Train Loss: 0.0396 | Val Loss: 0.1791 | Train F1: 0.9719 | Val F1: 0.8956 | LR: 5.12e-04\n",
      "         Best F1: 0.9131 (Ep 57) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 31/100 | No Loss Improve: 58/100\n",
      "Epoch  89/500 | Train Loss: 0.0407 | Val Loss: 0.1635 | Train F1: 0.9744 | Val F1: 0.9072 | LR: 5.12e-04\n",
      "         Best F1: 0.9131 (Ep 57) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 32/100 | No Loss Improve: 59/100\n",
      "Epoch  90/500 | Train Loss: 0.0356 | Val Loss: 0.1778 | Train F1: 0.9784 | Val F1: 0.8993 | LR: 5.12e-04\n",
      "         Best F1: 0.9131 (Ep 57) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 33/100 | No Loss Improve: 60/100\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch  91/500 | Train Loss: 0.0402 | Val Loss: 0.1884 | Train F1: 0.9760 | Val F1: 0.9012 | LR: 5.12e-04\n",
      "         Best F1: 0.9131 (Ep 57) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 34/100 | No Loss Improve: 61/100\n",
      "Epoch  92/500 | Train Loss: 0.0457 | Val Loss: 0.1646 | Train F1: 0.9635 | Val F1: 0.9069 | LR: 5.12e-04\n",
      "         Best F1: 0.9131 (Ep 57) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 35/100 | No Loss Improve: 62/100\n",
      "Epoch  93/500 | Train Loss: 0.0417 | Val Loss: 0.1633 | Train F1: 0.9694 | Val F1: 0.9045 | LR: 5.12e-04\n",
      "         Best F1: 0.9131 (Ep 57) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 36/100 | No Loss Improve: 63/100\n",
      "Epoch  94/500 | Train Loss: 0.0422 | Val Loss: 0.1651 | Train F1: 0.9707 | Val F1: 0.9016 | LR: 5.12e-04\n",
      "         Best F1: 0.9131 (Ep 57) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 37/100 | No Loss Improve: 64/100\n",
      "Epoch  95/500 | Train Loss: 0.0372 | Val Loss: 0.1590 | Train F1: 0.9732 | Val F1: 0.9100 | LR: 5.12e-04\n",
      "         Best F1: 0.9131 (Ep 57) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 38/100 | No Loss Improve: 65/100\n",
      "Epoch  96/500 | Train Loss: 0.0346 | Val Loss: 0.1780 | Train F1: 0.9785 | Val F1: 0.9075 | LR: 5.12e-04\n",
      "         Best F1: 0.9131 (Ep 57) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 39/100 | No Loss Improve: 66/100\n",
      "Epoch  97/500 | Train Loss: 0.0342 | Val Loss: 0.1864 | Train F1: 0.9803 | Val F1: 0.9061 | LR: 5.12e-04\n",
      "         Best F1: 0.9131 (Ep 57) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 40/100 | No Loss Improve: 67/100\n",
      "Epoch  98/500 | Train Loss: 0.0329 | Val Loss: 0.1885 | Train F1: 0.9787 | Val F1: 0.9048 | LR: 5.12e-04\n",
      "         Best F1: 0.9131 (Ep 57) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 41/100 | No Loss Improve: 68/100\n",
      "Epoch  99/500 | Train Loss: 0.0324 | Val Loss: 0.1983 | Train F1: 0.9827 | Val F1: 0.8980 | LR: 5.12e-04\n",
      "         Best F1: 0.9131 (Ep 57) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 42/100 | No Loss Improve: 69/100\n",
      "Epoch 100/500 | Train Loss: 0.0311 | Val Loss: 0.1891 | Train F1: 0.9851 | Val F1: 0.9109 | LR: 5.12e-04\n",
      "         Best F1: 0.9131 (Ep 57) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 43/100 | No Loss Improve: 70/100\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 101/500 | Train Loss: 0.0312 | Val Loss: 0.1967 | Train F1: 0.9837 | Val F1: 0.9050 | LR: 5.12e-04\n",
      "         Best F1: 0.9131 (Ep 57) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 44/100 | No Loss Improve: 71/100\n",
      "Epoch 102/500 | Train Loss: 0.0313 | Val Loss: 0.1914 | Train F1: 0.9830 | Val F1: 0.9052 | LR: 5.12e-04\n",
      "         Best F1: 0.9131 (Ep 57) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 45/100 | No Loss Improve: 72/100\n",
      "Epoch 103/500 | Train Loss: 0.0316 | Val Loss: 0.1876 | Train F1: 0.9832 | Val F1: 0.9044 | LR: 5.12e-04\n",
      "         Best F1: 0.9131 (Ep 57) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 46/100 | No Loss Improve: 73/100\n",
      "Epoch 104/500 | Train Loss: 0.0308 | Val Loss: 0.1811 | Train F1: 0.9843 | Val F1: 0.9090 | LR: 5.12e-04\n",
      "         Best F1: 0.9131 (Ep 57) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 47/100 | No Loss Improve: 74/100\n",
      "  üîΩ Learning rate reduced from 5.12e-04 to 4.10e-04 (based on combined)\n",
      "Epoch 105/500 | Train Loss: 0.0322 | Val Loss: 0.2127 | Train F1: 0.9826 | Val F1: 0.9006 | LR: 4.10e-04\n",
      "         Best F1: 0.9131 (Ep 57) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 48/100 | No Loss Improve: 75/100\n",
      "Epoch 106/500 | Train Loss: 0.0324 | Val Loss: 0.1889 | Train F1: 0.9828 | Val F1: 0.9058 | LR: 4.10e-04\n",
      "         Best F1: 0.9131 (Ep 57) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 49/100 | No Loss Improve: 76/100\n",
      "Epoch 107/500 | Train Loss: 0.0304 | Val Loss: 0.1803 | Train F1: 0.9847 | Val F1: 0.9113 | LR: 4.10e-04\n",
      "         Best F1: 0.9131 (Ep 57) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 50/100 | No Loss Improve: 77/100\n",
      "Epoch 108/500 | Train Loss: 0.0303 | Val Loss: 0.1839 | Train F1: 0.9860 | Val F1: 0.9100 | LR: 4.10e-04\n",
      "         Best F1: 0.9131 (Ep 57) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 51/100 | No Loss Improve: 78/100\n",
      "Epoch 109/500 | Train Loss: 0.0307 | Val Loss: 0.1844 | Train F1: 0.9825 | Val F1: 0.9086 | LR: 4.10e-04\n",
      "         Best F1: 0.9131 (Ep 57) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 52/100 | No Loss Improve: 79/100\n",
      "Epoch 110/500 | Train Loss: 0.0314 | Val Loss: 0.1838 | Train F1: 0.9819 | Val F1: 0.9090 | LR: 4.10e-04\n",
      "         Best F1: 0.9131 (Ep 57) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 53/100 | No Loss Improve: 80/100\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 111/500 | Train Loss: 0.0312 | Val Loss: 0.1844 | Train F1: 0.9852 | Val F1: 0.9030 | LR: 4.10e-04\n",
      "         Best F1: 0.9131 (Ep 57) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 54/100 | No Loss Improve: 81/100\n",
      "Epoch 112/500 | Train Loss: 0.0303 | Val Loss: 0.1903 | Train F1: 0.9850 | Val F1: 0.9073 | LR: 4.10e-04\n",
      "         Best F1: 0.9131 (Ep 57) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 55/100 | No Loss Improve: 82/100\n",
      "Epoch 113/500 | Train Loss: 0.0300 | Val Loss: 0.2013 | Train F1: 0.9846 | Val F1: 0.8983 | LR: 4.10e-04\n",
      "         Best F1: 0.9131 (Ep 57) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 56/100 | No Loss Improve: 83/100\n",
      "Epoch 114/500 | Train Loss: 0.0348 | Val Loss: 0.2067 | Train F1: 0.9807 | Val F1: 0.9027 | LR: 4.10e-04\n",
      "         Best F1: 0.9131 (Ep 57) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 57/100 | No Loss Improve: 84/100\n",
      "Epoch 115/500 | Train Loss: 0.0419 | Val Loss: 0.1783 | Train F1: 0.9727 | Val F1: 0.9027 | LR: 4.10e-04\n",
      "         Best F1: 0.9131 (Ep 57) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 58/100 | No Loss Improve: 85/100\n",
      "Epoch 116/500 | Train Loss: 0.0333 | Val Loss: 0.1746 | Train F1: 0.9803 | Val F1: 0.9103 | LR: 4.10e-04\n",
      "         Best F1: 0.9131 (Ep 57) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 59/100 | No Loss Improve: 86/100\n",
      "Epoch 117/500 | Train Loss: 0.0308 | Val Loss: 0.1772 | Train F1: 0.9840 | Val F1: 0.9124 | LR: 4.10e-04\n",
      "         Best F1: 0.9131 (Ep 57) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 60/100 | No Loss Improve: 87/100\n",
      "Epoch 118/500 | Train Loss: 0.0293 | Val Loss: 0.1801 | Train F1: 0.9861 | Val F1: 0.9128 | LR: 4.10e-04\n",
      "         Best F1: 0.9131 (Ep 57) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 61/100 | No Loss Improve: 88/100\n",
      "Epoch 119/500 | Train Loss: 0.0302 | Val Loss: 0.1830 | Train F1: 0.9852 | Val F1: 0.9078 | LR: 4.10e-04\n",
      "         Best F1: 0.9131 (Ep 57) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 62/100 | No Loss Improve: 89/100\n",
      "Epoch 120/500 | Train Loss: 0.0291 | Val Loss: 0.1896 | Train F1: 0.9851 | Val F1: 0.9091 | LR: 4.10e-04\n",
      "         Best F1: 0.9131 (Ep 57) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 63/100 | No Loss Improve: 90/100\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 121/500 | Train Loss: 0.0293 | Val Loss: 0.1840 | Train F1: 0.9869 | Val F1: 0.9123 | LR: 4.10e-04\n",
      "         Best F1: 0.9131 (Ep 57) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 64/100 | No Loss Improve: 91/100\n",
      "Epoch 122/500 | Train Loss: 0.0284 | Val Loss: 0.1894 | Train F1: 0.9860 | Val F1: 0.9095 | LR: 4.10e-04\n",
      "         Best F1: 0.9131 (Ep 57) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 65/100 | No Loss Improve: 92/100\n",
      "Epoch 123/500 | Train Loss: 0.0278 | Val Loss: 0.1956 | Train F1: 0.9880 | Val F1: 0.9069 | LR: 4.10e-04\n",
      "         Best F1: 0.9131 (Ep 57) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 66/100 | No Loss Improve: 93/100\n",
      "Epoch 124/500 | Train Loss: 0.0277 | Val Loss: 0.1978 | Train F1: 0.9878 | Val F1: 0.9123 | LR: 4.10e-04\n",
      "         Best F1: 0.9131 (Ep 57) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 67/100 | No Loss Improve: 94/100\n",
      "Epoch 125/500 | Train Loss: 0.0300 | Val Loss: 0.1991 | Train F1: 0.9862 | Val F1: 0.9059 | LR: 4.10e-04\n",
      "         Best F1: 0.9131 (Ep 57) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 68/100 | No Loss Improve: 95/100\n",
      "Epoch 126/500 | Train Loss: 0.0288 | Val Loss: 0.1996 | Train F1: 0.9857 | Val F1: 0.9013 | LR: 4.10e-04\n",
      "         Best F1: 0.9131 (Ep 57) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 69/100 | No Loss Improve: 96/100\n",
      "Epoch 127/500 | Train Loss: 0.0290 | Val Loss: 0.1861 | Train F1: 0.9851 | Val F1: 0.9118 | LR: 4.10e-04\n",
      "         Best F1: 0.9131 (Ep 57) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 70/100 | No Loss Improve: 97/100\n",
      "Epoch 128/500 | Train Loss: 0.0290 | Val Loss: 0.1964 | Train F1: 0.9861 | Val F1: 0.9064 | LR: 4.10e-04\n",
      "         Best F1: 0.9131 (Ep 57) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 71/100 | No Loss Improve: 98/100\n",
      "Epoch 129/500 | Train Loss: 0.0301 | Val Loss: 0.1817 | Train F1: 0.9847 | Val F1: 0.9116 | LR: 4.10e-04\n",
      "         Best F1: 0.9131 (Ep 57) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 72/100 | No Loss Improve: 99/100\n",
      "Epoch 130/500 | Train Loss: 0.0286 | Val Loss: 0.1971 | Train F1: 0.9869 | Val F1: 0.9053 | LR: 4.10e-04\n",
      "         Best F1: 0.9131 (Ep 57) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 73/100 | No Loss Improve: 100/100\n",
      "----------------------------------------------------------------------------------------------------\n",
      "  üîΩ Learning rate reduced from 4.10e-04 to 3.28e-04 (based on combined)\n",
      "Epoch 131/500 | Train Loss: 0.0279 | Val Loss: 0.2026 | Train F1: 0.9876 | Val F1: 0.9017 | LR: 3.28e-04\n",
      "         Best F1: 0.9131 (Ep 57) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 74/100 | No Loss Improve: 101/100\n",
      "Epoch 132/500 | Train Loss: 0.0285 | Val Loss: 0.2098 | Train F1: 0.9876 | Val F1: 0.9001 | LR: 3.28e-04\n",
      "         Best F1: 0.9131 (Ep 57) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 75/100 | No Loss Improve: 102/100\n",
      "Epoch 133/500 | Train Loss: 0.0285 | Val Loss: 0.2079 | Train F1: 0.9871 | Val F1: 0.9004 | LR: 3.28e-04\n",
      "         Best F1: 0.9131 (Ep 57) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 76/100 | No Loss Improve: 103/100\n",
      "Epoch 134/500 | Train Loss: 0.0291 | Val Loss: 0.2053 | Train F1: 0.9851 | Val F1: 0.9063 | LR: 3.28e-04\n",
      "         Best F1: 0.9131 (Ep 57) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 77/100 | No Loss Improve: 104/100\n",
      "Epoch 135/500 | Train Loss: 0.0280 | Val Loss: 0.1969 | Train F1: 0.9885 | Val F1: 0.9021 | LR: 3.28e-04\n",
      "         Best F1: 0.9131 (Ep 57) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 78/100 | No Loss Improve: 105/100\n",
      "Epoch 136/500 | Train Loss: 0.0346 | Val Loss: 0.2031 | Train F1: 0.9810 | Val F1: 0.8984 | LR: 3.28e-04\n",
      "         Best F1: 0.9131 (Ep 57) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 79/100 | No Loss Improve: 106/100\n",
      "Epoch 137/500 | Train Loss: 0.0322 | Val Loss: 0.2071 | Train F1: 0.9809 | Val F1: 0.8903 | LR: 3.28e-04\n",
      "         Best F1: 0.9131 (Ep 57) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 80/100 | No Loss Improve: 107/100\n",
      "Epoch 138/500 | Train Loss: 0.0285 | Val Loss: 0.1843 | Train F1: 0.9872 | Val F1: 0.9048 | LR: 3.28e-04\n",
      "         Best F1: 0.9131 (Ep 57) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 81/100 | No Loss Improve: 108/100\n",
      "Epoch 139/500 | Train Loss: 0.0279 | Val Loss: 0.1868 | Train F1: 0.9886 | Val F1: 0.9082 | LR: 3.28e-04\n",
      "         Best F1: 0.9131 (Ep 57) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 82/100 | No Loss Improve: 109/100\n",
      "Epoch 140/500 | Train Loss: 0.0270 | Val Loss: 0.1959 | Train F1: 0.9875 | Val F1: 0.9054 | LR: 3.28e-04\n",
      "         Best F1: 0.9131 (Ep 57) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 83/100 | No Loss Improve: 110/100\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 141/500 | Train Loss: 0.0262 | Val Loss: 0.2066 | Train F1: 0.9896 | Val F1: 0.9008 | LR: 3.28e-04\n",
      "         Best F1: 0.9131 (Ep 57) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 84/100 | No Loss Improve: 111/100\n",
      "Epoch 142/500 | Train Loss: 0.0264 | Val Loss: 0.2012 | Train F1: 0.9892 | Val F1: 0.9005 | LR: 3.28e-04\n",
      "         Best F1: 0.9131 (Ep 57) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 85/100 | No Loss Improve: 112/100\n",
      "Epoch 143/500 | Train Loss: 0.0260 | Val Loss: 0.2034 | Train F1: 0.9900 | Val F1: 0.9022 | LR: 3.28e-04\n",
      "         Best F1: 0.9131 (Ep 57) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 86/100 | No Loss Improve: 113/100\n",
      "Epoch 144/500 | Train Loss: 0.0268 | Val Loss: 0.2266 | Train F1: 0.9883 | Val F1: 0.8946 | LR: 3.28e-04\n",
      "         Best F1: 0.9131 (Ep 57) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 87/100 | No Loss Improve: 114/100\n",
      "Epoch 145/500 | Train Loss: 0.0333 | Val Loss: 0.2047 | Train F1: 0.9804 | Val F1: 0.8939 | LR: 3.28e-04\n",
      "         Best F1: 0.9131 (Ep 57) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 88/100 | No Loss Improve: 115/100\n",
      "Epoch 146/500 | Train Loss: 0.0339 | Val Loss: 0.1991 | Train F1: 0.9792 | Val F1: 0.9021 | LR: 3.28e-04\n",
      "         Best F1: 0.9131 (Ep 57) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 89/100 | No Loss Improve: 116/100\n",
      "Epoch 147/500 | Train Loss: 0.0320 | Val Loss: 0.1671 | Train F1: 0.9802 | Val F1: 0.9051 | LR: 3.28e-04\n",
      "         Best F1: 0.9131 (Ep 57) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 90/100 | No Loss Improve: 117/100\n",
      "Epoch 148/500 | Train Loss: 0.0288 | Val Loss: 0.1931 | Train F1: 0.9870 | Val F1: 0.8973 | LR: 3.28e-04\n",
      "         Best F1: 0.9131 (Ep 57) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 91/100 | No Loss Improve: 118/100\n",
      "Epoch 149/500 | Train Loss: 0.0271 | Val Loss: 0.1780 | Train F1: 0.9888 | Val F1: 0.9069 | LR: 3.28e-04\n",
      "         Best F1: 0.9131 (Ep 57) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 92/100 | No Loss Improve: 119/100\n",
      "Epoch 150/500 | Train Loss: 0.0274 | Val Loss: 0.1973 | Train F1: 0.9880 | Val F1: 0.8995 | LR: 3.28e-04\n",
      "         Best F1: 0.9131 (Ep 57) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 93/100 | No Loss Improve: 120/100\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 151/500 | Train Loss: 0.0270 | Val Loss: 0.1951 | Train F1: 0.9895 | Val F1: 0.8991 | LR: 3.28e-04\n",
      "         Best F1: 0.9131 (Ep 57) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 94/100 | No Loss Improve: 121/100\n",
      "Epoch 152/500 | Train Loss: 0.0265 | Val Loss: 0.1884 | Train F1: 0.9901 | Val F1: 0.9107 | LR: 3.28e-04\n",
      "         Best F1: 0.9131 (Ep 57) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 95/100 | No Loss Improve: 122/100\n",
      "Epoch 153/500 | Train Loss: 0.0258 | Val Loss: 0.1926 | Train F1: 0.9896 | Val F1: 0.9058 | LR: 3.28e-04\n",
      "         Best F1: 0.9131 (Ep 57) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 96/100 | No Loss Improve: 123/100\n",
      "Epoch 154/500 | Train Loss: 0.0262 | Val Loss: 0.2042 | Train F1: 0.9897 | Val F1: 0.9025 | LR: 3.28e-04\n",
      "         Best F1: 0.9131 (Ep 57) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 97/100 | No Loss Improve: 124/100\n",
      "Epoch 155/500 | Train Loss: 0.0257 | Val Loss: 0.2154 | Train F1: 0.9911 | Val F1: 0.9004 | LR: 3.28e-04\n",
      "         Best F1: 0.9131 (Ep 57) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 98/100 | No Loss Improve: 125/100\n",
      "Epoch 156/500 | Train Loss: 0.0256 | Val Loss: 0.1971 | Train F1: 0.9902 | Val F1: 0.9009 | LR: 3.28e-04\n",
      "         Best F1: 0.9131 (Ep 57) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 99/100 | No Loss Improve: 126/100\n",
      "  üîΩ Learning rate reduced from 3.28e-04 to 2.62e-04 (based on combined)\n",
      "Epoch 157/500 | Train Loss: 0.0257 | Val Loss: 0.2101 | Train F1: 0.9897 | Val F1: 0.8993 | LR: 2.62e-04\n",
      "         Best F1: 0.9131 (Ep 57) | Best Loss: 0.1026 (Ep 30) | No F1 Improve: 100/100 | No Loss Improve: 127/100\n",
      "üõë EARLY STOPPING triggered at epoch 157 (F1 plateau)\n",
      "üéØ Best validation F1: 0.9131 at epoch 57\n",
      "üéØ Best validation Loss: 0.1026 at epoch 30\n",
      "üîÑ Model restored to best F1 state (epoch 57, F1: 0.9131, Val Loss: 0.1381)\n",
      "‚úÖ Training completed!\n",
      "üìä Final Results:\n",
      "   ‚Ä¢ Best validation F1: 0.9131 (Epoch 57)\n",
      "   ‚Ä¢ Best validation Loss: 0.1026 (Epoch 30)\n",
      "   ‚Ä¢ Total epochs trained: 157\n",
      "   ‚Ä¢ Early stopping: Yes\n",
      "   ‚Ä¢ Final val loss at best F1: 0.1381\n",
      "üéØ Regularization applied - L1: 1.00e-06, L2: 1.00e-04\n",
      "\\nüéâ Enhanced Training with Validation Loss Integration Complete!\n",
      "üìä Training Summary:\n",
      "   ‚Ä¢ Best F1 Score: 0.9131\n",
      "   ‚Ä¢ Best Validation Loss: 0.1026\n",
      "   ‚Ä¢ Best Epoch (F1): 57\n",
      "   ‚Ä¢ Best Epoch (Loss): 30\n",
      "   ‚Ä¢ Total Epochs: 157 (stopped early to prevent overfitting)\n",
      "   ‚Ä¢ F1 at best loss epoch: 0.8992\n",
      "   ‚Ä¢ Loss at best F1 epoch: 0.1381\n",
      "   ‚Ä¢ Expected Test Performance: 0.8% - 0.9% (more realistic range)\n",
      "\\nüõ°Ô∏è Overfitting Prevention Applied:\n",
      "‚úÖ Early stopping with patience=50 epochs\n",
      "‚úÖ Learning rate scheduling (F1 or loss-based)\n",
      "‚úÖ L1/L2 regularization\n",
      "‚úÖ Dropout layers\n",
      "‚úÖ Model state restored to best checkpoint\n",
      "‚úÖ Dual metric monitoring (F1 + validation loss)\n",
      "\\nüìã Next Steps for Better Generalization:\n",
      "1. ‚úÖ Use this early-stopped model for submission\n",
      "2. üîÑ Consider cross-validation for more robust validation\n",
      "3. üéõÔ∏è Try ensemble methods (train multiple models)\n",
      "4. üìä Analyze prediction confidence scores\n",
      "5. üîç Review data for potential train/test distribution shifts\n",
      "6. üÜö Compare F1-based vs Loss-based early stopping\n"
     ]
    }
   ],
   "source": [
    "# Enhanced model training setup with ReduceLROnPlateau scheduler and Early Stopping\n",
    "def train_enhanced_model(model, train_loader, val_loader, criterion, optimizer, scheduler=None, epochs=5, \n",
    "                        l1_lambda=None, l2_lambda=None, patience=30, min_delta=0.001, scheduler_metric='f1'):\n",
    "    \"\"\"\n",
    "    Training function for the enhanced model with embeddings\n",
    "    Includes support for ReduceLROnPlateau scheduler, L1/L2 regularization, and Early Stopping\n",
    "    \n",
    "    Args:\n",
    "        model: Enhanced model with embeddings\n",
    "        train_loader: Training data loader\n",
    "        val_loader: Validation data loader  \n",
    "        criterion: Loss function\n",
    "        optimizer: Optimizer\n",
    "        scheduler: Optional ReduceLROnPlateau scheduler\n",
    "        epochs: Maximum number of training epochs\n",
    "        l1_lambda: L1 regularization coefficient (if None, uses L1_LAMBDA global)\n",
    "        l2_lambda: L2 regularization coefficient (if None, uses L2_LAMBDA global)\n",
    "        patience: Early stopping patience (epochs to wait without improvement)\n",
    "        min_delta: Minimum change to qualify as improvement\n",
    "        scheduler_metric: Metric to use for scheduler ('f1', 'val_loss', or 'combined')\n",
    "        \n",
    "    Returns:\n",
    "        train_losses, val_losses, val_f1_scores, train_f1_scores, best_epoch: Training history and best epoch\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_f1_scores = []\n",
    "    train_f1_scores = []  # Track training F1 scores for plotting\n",
    "    \n",
    "    # Early stopping variables - track both F1 and validation loss\n",
    "    best_val_f1 = 0\n",
    "    best_val_loss = float('inf')\n",
    "    best_epoch = 0\n",
    "    best_epoch_loss = 0\n",
    "    epochs_without_improvement = 0\n",
    "    epochs_without_loss_improvement = 0\n",
    "    best_model_state = None\n",
    "    \n",
    "    # Set regularization coefficients\n",
    "    if l1_lambda is None:\n",
    "        l1_lambda = L1_LAMBDA if 'L1_LAMBDA' in globals() else 0\n",
    "    if l2_lambda is None:\n",
    "        l2_lambda = L2_LAMBDA if 'L2_LAMBDA' in globals() else 0\n",
    "    \n",
    "    print(\"Starting enhanced model training with Early Stopping...\")\n",
    "    print(f\"üîß Regularization: L1={l1_lambda:.2e}, L2={l2_lambda:.2e}\")\n",
    "    print(f\"üõë Early Stopping: Patience={patience} epochs, Min improvement={min_delta:.4f}\")\n",
    "    print(f\"üìä Scheduler Metric: {scheduler_metric}\")\n",
    "    if scheduler:\n",
    "        print(f\"üìà Using ReduceLROnPlateau scheduler - Mode: {scheduler.mode}, Factor: {scheduler.factor}, Patience: {scheduler.patience}\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        epoch_train_loss = 0.0\n",
    "        num_batches = 0\n",
    "        # Track training predictions for F1 calculation\n",
    "        train_predictions = []\n",
    "        train_true_labels = []\n",
    "        \n",
    "        for continuous_batch, categorical_batch, labels_batch in train_loader:\n",
    "            # Move to device\n",
    "            continuous_batch = continuous_batch.to(device)\n",
    "            categorical_batch = {k: v.to(device) for k, v in categorical_batch.items()}\n",
    "            labels_batch = labels_batch.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(continuous_batch, categorical_batch)\n",
    "            loss = criterion(outputs, labels_batch)\n",
    "            \n",
    "            # Add L1 and L2 regularization penalties\n",
    "            if l1_lambda > 0 or l2_lambda > 0:\n",
    "                l1_norm = 0\n",
    "                l2_norm = 0\n",
    "                \n",
    "                for param in model.parameters():\n",
    "                    if l1_lambda > 0:\n",
    "                        l1_norm += torch.norm(param, 1)\n",
    "                    if l2_lambda > 0:\n",
    "                        l2_norm += torch.norm(param, 2) ** 2\n",
    "                \n",
    "                # Add regularization terms to loss\n",
    "                regularization_loss = l1_lambda * l1_norm + l2_lambda * l2_norm\n",
    "                loss = loss + regularization_loss\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Collect predictions for F1 calculation\n",
    "            with torch.no_grad():\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                train_predictions.extend(predicted.cpu().numpy())\n",
    "                train_true_labels.extend(labels_batch.cpu().numpy())\n",
    "            \n",
    "            epoch_train_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            \n",
    "            # Print progress occasionally (less frequently for cleaner output)\n",
    "            if num_batches % 20 == 0:  \n",
    "                print(f\"  Epoch {epoch+1}/{epochs}, Batch {num_batches}, Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        avg_train_loss = epoch_train_loss / num_batches\n",
    "            \n",
    "        # Calculate training F1 score\n",
    "        train_f1 = f1_score(train_true_labels, train_predictions, average='weighted')\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_predictions = []\n",
    "        val_true_labels = []\n",
    "        epoch_val_loss = 0.0\n",
    "        val_batches = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for continuous_batch, categorical_batch, labels_batch in val_loader:\n",
    "                continuous_batch = continuous_batch.to(device)\n",
    "                categorical_batch = {k: v.to(device) for k, v in categorical_batch.items()}\n",
    "                labels_batch = labels_batch.to(device)\n",
    "                continuous_batch = continuous_batch.to(device)\n",
    "                outputs = model(continuous_batch, categorical_batch)\n",
    "                loss = criterion(outputs, labels_batch)\n",
    "                epoch_val_loss += loss.item()\n",
    "                val_batches += 1\n",
    "                \n",
    "                # Get predictions\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_predictions.extend(predicted.cpu().numpy())\n",
    "                val_true_labels.extend(labels_batch.cpu().numpy())\n",
    "        avg_val_loss = epoch_val_loss / val_batches\n",
    "        val_f1 = f1_score(val_true_labels, val_predictions, average='weighted')\n",
    "        \n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        val_f1_scores.append(val_f1)\n",
    "        train_f1_scores.append(train_f1)\n",
    "        \n",
    "        # Enhanced Early stopping check - track both F1 and validation loss\n",
    "        f1_improved = val_f1 > best_val_f1 + min_delta\n",
    "        loss_improved = avg_val_loss < best_val_loss - min_delta\n",
    "        \n",
    "        if f1_improved:\n",
    "            best_val_f1 = val_f1\n",
    "            best_epoch = epoch + 1\n",
    "            epochs_without_improvement = 0\n",
    "            # Save best model state based on F1\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            print(f\"‚úÖ NEW BEST F1: {val_f1:.4f} at epoch {best_epoch} (improvement: +{val_f1-(best_val_f1-min_delta):.4f})\")\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            \n",
    "        if loss_improved:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_epoch_loss = epoch + 1\n",
    "            epochs_without_loss_improvement = 0\n",
    "            print(f\"üîΩ NEW BEST VAL LOSS: {avg_val_loss:.4f} at epoch {best_epoch_loss} (improvement: -{(best_val_loss+min_delta)-avg_val_loss:.4f})\")\n",
    "        else:\n",
    "            epochs_without_loss_improvement += 1\n",
    "        \n",
    "        # Update learning rate scheduler if provided\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        if scheduler:\n",
    "            # Choose metric based on scheduler_metric parameter\n",
    "            if scheduler_metric == 'f1':\n",
    "                if scheduler.mode == 'max':\n",
    "                    scheduler.step(val_f1)  # F1 score - higher is better\n",
    "                else:\n",
    "                    # If mode is 'min' but metric is f1, use negative f1\n",
    "                    scheduler.step(-val_f1)\n",
    "            elif scheduler_metric == 'val_loss':\n",
    "                if scheduler.mode == 'min':\n",
    "                    scheduler.step(avg_val_loss)  # Loss - lower is better\n",
    "                else:\n",
    "                    # If mode is 'max' but metric is val_loss, use negative loss\n",
    "                    scheduler.step(-avg_val_loss)\n",
    "            elif scheduler_metric == 'combined':\n",
    "                # Combined metric: normalized F1 - normalized val_loss\n",
    "                # Normalize F1 (0-1) and val_loss (inverse, lower is better)\n",
    "                if len(val_f1_scores) > 1:\n",
    "                    combined_metric = val_f1 - (avg_val_loss / max(val_losses))\n",
    "                    if scheduler.mode == 'max':\n",
    "                        scheduler.step(combined_metric)\n",
    "                    else:\n",
    "                        scheduler.step(-combined_metric)\n",
    "                else:\n",
    "                    # First epoch, use F1\n",
    "                    scheduler.step(val_f1 if scheduler.mode == 'max' else -val_f1)\n",
    "            \n",
    "            new_lr = optimizer.param_groups[0]['lr']\n",
    "            if new_lr != current_lr:\n",
    "                print(f\"  üîΩ Learning rate reduced from {current_lr:.2e} to {new_lr:.2e} (based on {scheduler_metric})\")\n",
    "        \n",
    "        print(f\"Epoch {epoch+1:3d}/{epochs} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | \"\n",
    "              f\"Train F1: {train_f1:.4f} | Val F1: {val_f1:.4f} | LR: {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "        print(f\"         Best F1: {best_val_f1:.4f} (Ep {best_epoch}) | Best Loss: {best_val_loss:.4f} (Ep {best_epoch_loss}) | \"\n",
    "              f\"No F1 Improve: {epochs_without_improvement}/{patience} | No Loss Improve: {epochs_without_loss_improvement}/{patience}\")\n",
    "        \n",
    "        # Early stopping trigger (based on F1 by default, can be modified)\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f\"üõë EARLY STOPPING triggered at epoch {epoch+1} (F1 plateau)\")\n",
    "            print(f\"üéØ Best validation F1: {best_val_f1:.4f} at epoch {best_epoch}\")\n",
    "            print(f\"üéØ Best validation Loss: {best_val_loss:.4f} at epoch {best_epoch_loss}\")\n",
    "            break\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:  # Separator every 10 epochs\n",
    "            print(\"-\" * 100)\n",
    "    \n",
    "    # Restore best model state\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "        print(f\"üîÑ Model restored to best F1 state (epoch {best_epoch}, F1: {best_val_f1:.4f}, Val Loss: {val_losses[best_epoch-1]:.4f})\")\n",
    "    \n",
    "    print(f\"‚úÖ Training completed!\")\n",
    "    print(f\"üìä Final Results:\")\n",
    "    print(f\"   ‚Ä¢ Best validation F1: {best_val_f1:.4f} (Epoch {best_epoch})\")\n",
    "    print(f\"   ‚Ä¢ Best validation Loss: {best_val_loss:.4f} (Epoch {best_epoch_loss})\")\n",
    "    print(f\"   ‚Ä¢ Total epochs trained: {len(val_f1_scores)}\")\n",
    "    print(f\"   ‚Ä¢ Early stopping: {'Yes' if epochs_without_improvement >= patience else 'No'}\")\n",
    "    print(f\"   ‚Ä¢ Final val loss at best F1: {val_losses[best_epoch-1] if best_epoch <= len(val_losses) else 'N/A':.4f}\")\n",
    "    if l1_lambda > 0 or l2_lambda > 0:\n",
    "        print(f\"üéØ Regularization applied - L1: {l1_lambda:.2e}, L2: {l2_lambda:.2e}\")\n",
    "    return train_losses, val_losses, val_f1_scores, train_f1_scores, best_epoch\n",
    "\n",
    "# Create a smaller enhanced model for quick demonstration\n",
    "print(\"=== Creating Enhanced Model for Training Demo ===\")\n",
    "\n",
    "demo_enhanced_model = EnhancedRecurrentClassifier(\n",
    "    continuous_input_size=17,\n",
    "    categorical_features=categorical_feature_config,\n",
    "    embedding_dims=embedding_dims,\n",
    "    hidden_size=HIDDEN_SIZE,  # Smaller for faster training\n",
    "    num_layers=2,\n",
    "    num_classes=3,\n",
    "    rnn_type='LSTM',\n",
    "    bidirectional=True,\n",
    "    dropout_rate=0.2\n",
    ").to(device)\n",
    "\n",
    "# TO WEIGHT MORE THE \"MORE DIFFICULT\" CASES AND THE LESS FREQUENT LABELS:\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=2.0):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.ce = nn.CrossEntropyLoss(weight=alpha, reduction='none')\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = self.ce(inputs, targets)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = ((1 - pt) ** self.gamma) * ce_loss\n",
    "        return focal_loss.mean()\n",
    "        \n",
    "alpha = None\n",
    "# alpha = torch.tensor([0.7, 1.3, 1.7], dtype=torch.float32, device=device)  # None if we don't want to alterate the weights of each label losses (FocalLoss already do it)\n",
    "criterion = FocalLoss(alpha=alpha, gamma=2.3)  # gamma = 0 it's like Crossentropy(), gamma < 1 it's like in between Crossentropy and FocalLoss,\n",
    "                                               # gamma = 1 it's a good compromise, gamma = 1.5 or gamma = 2 to weight so much the less present labels\n",
    "\n",
    "\n",
    "# Setup training components\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "# Setup training components\n",
    "\n",
    "# Define optimizer with L2 regularization\n",
    "optimizer = torch.optim.AdamW(demo_enhanced_model.parameters(), lr=LEARNING_RATE, weight_decay=L2_LAMBDA)\n",
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Example configurations for different scheduler setups:\n",
    "\n",
    "# Option 1: Focus on F1 score (current setup)\n",
    "scheduler_f1 = ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='max',          # max for F1 score (higher is better)\n",
    "    factor=0.8,          # multiply lr by 0.8 when plateau\n",
    "    patience=25,         # wait 25 epochs without improvement\n",
    "    min_lr=1e-6          # minimum limit for learning rate\n",
    ")\n",
    "\n",
    "# Option 2: Focus on validation loss\n",
    "scheduler_loss = ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min',          # min for validation loss (lower is better)\n",
    "    factor=0.7,          # more aggressive reduction for loss-based\n",
    "    patience=20,         # slightly less patience for loss\n",
    "    min_lr=1e-6\n",
    ")\n",
    "\n",
    "# Option 3: Combined approach - will use the combined metric\n",
    "scheduler_combined = ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='max',          # max for combined metric\n",
    "    factor=0.75,         # moderate reduction\n",
    "    patience=50,         # balanced patience\n",
    "    min_lr=1e-6\n",
    ")\n",
    "\n",
    "# Choose which scheduler to use\n",
    "scheduler = scheduler_f1  # Change this to scheduler_loss or scheduler_combined as needed\n",
    "\n",
    "print(f\"Demo model parameters: {sum(p.numel() for p in demo_enhanced_model.parameters()):,}\")\n",
    "print(\"üìä Available scheduler options:\")\n",
    "print(\"  ‚Ä¢ scheduler_f1: Focuses on F1 score improvement\")\n",
    "print(\"  ‚Ä¢ scheduler_loss: Focuses on validation loss reduction\") \n",
    "print(\"  ‚Ä¢ scheduler_combined: Uses combined F1 and loss metric\")\n",
    "print(f\"üéØ Currently using: {scheduler.__class__.__name__} with mode='{scheduler.mode}'\")\n",
    "print(\"Ready for training!\")\n",
    "\n",
    "# Optional: Run a quick training demo (uncomment to run)\n",
    "demo_train_losses, demo_val_losses, demo_val_f1s, demo_train_f1s, demo_best_epoch = train_enhanced_model(\n",
    "    model=demo_enhanced_model, \n",
    "    train_loader=train_enhanced_loader, \n",
    "    val_loader=val_enhanced_loader, \n",
    "    criterion=criterion, \n",
    "    optimizer=optimizer, \n",
    "    scheduler=scheduler,  # Include the ReduceLROnPlateau scheduler\n",
    "    epochs=EPOCHS,\n",
    "    l1_lambda=L1_LAMBDA,  # L1 regularization\n",
    "    l2_lambda=L2_LAMBDA,  # L2 regularization (in addition to optimizer weight_decay)\n",
    "    patience=100,          # Early stopping patience - wait 100 epochs without improvement\n",
    "    min_delta=0.001,      # Minimum F1 improvement to count as progress\n",
    "    scheduler_metric='combined'  # Can be 'f1', 'val_loss', or 'combined'\n",
    ")\n",
    "print(f\"\\\\nüéâ Enhanced Training with Validation Loss Integration Complete!\")\n",
    "print(f\"üìä Training Summary:\")\n",
    "print(f\"   ‚Ä¢ Best F1 Score: {max(demo_val_f1s):.4f}\")\n",
    "print(f\"   ‚Ä¢ Best Validation Loss: {min(demo_val_losses):.4f}\")\n",
    "print(f\"   ‚Ä¢ Best Epoch (F1): {demo_best_epoch}\")\n",
    "print(f\"   ‚Ä¢ Best Epoch (Loss): {demo_val_losses.index(min(demo_val_losses)) + 1}\")\n",
    "print(f\"   ‚Ä¢ Total Epochs: {len(demo_val_f1s)} (stopped early to prevent overfitting)\")\n",
    "print(f\"   ‚Ä¢ F1 at best loss epoch: {demo_val_f1s[demo_val_losses.index(min(demo_val_losses))]:.4f}\")\n",
    "print(f\"   ‚Ä¢ Loss at best F1 epoch: {demo_val_losses[demo_best_epoch-1]:.4f}\")\n",
    "print(f\"   ‚Ä¢ Expected Test Performance: {max(demo_val_f1s)*0.85:.1f}% - {max(demo_val_f1s)*0.95:.1f}% (more realistic range)\")\n",
    "\n",
    "print(\"\\\\nüõ°Ô∏è Overfitting Prevention Applied:\")\n",
    "print(\"‚úÖ Early stopping with patience=50 epochs\")\n",
    "print(\"‚úÖ Learning rate scheduling (F1 or loss-based)\")\n",
    "print(\"‚úÖ L1/L2 regularization\")\n",
    "print(\"‚úÖ Dropout layers\")\n",
    "print(\"‚úÖ Model state restored to best checkpoint\")\n",
    "print(\"‚úÖ Dual metric monitoring (F1 + validation loss)\")\n",
    "\n",
    "print(\"\\\\nüìã Next Steps for Better Generalization:\")\n",
    "print(\"1. ‚úÖ Use this early-stopped model for submission\")\n",
    "print(\"2. üîÑ Consider cross-validation for more robust validation\")\n",
    "print(\"3. üéõÔ∏è Try ensemble methods (train multiple models)\")\n",
    "print(\"4. üìä Analyze prediction confidence scores\")\n",
    "print(\"5. üîç Review data for potential train/test distribution shifts\")\n",
    "print(\"6. üÜö Compare F1-based vs Loss-based early stopping\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting enhanced model results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Enhanced Model Training Results Visualization ===\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Enhanced Model Training Summary:\n",
      "   ‚Ä¢ Total Epochs: 157\n",
      "   ‚Ä¢ Best Validation F1: 0.9131 (Epoch 57)\n",
      "   ‚Ä¢ Best Validation Loss: 0.1026 (Epoch 30)\n",
      "   ‚Ä¢ Final Validation F1: 0.8993\n",
      "   ‚Ä¢ Final Validation Loss: 0.2101\n"
     ]
    }
   ],
   "source": [
    "# @title Plot Enhanced Model Training History\n",
    "print(\"=== Enhanced Model Training Results Visualization ===\")\n",
    "\n",
    "# Check if enhanced model training results are available\n",
    "if 'demo_train_losses' in globals() and 'demo_val_losses' in globals() and 'demo_val_f1s' in globals():\n",
    "    # Create a figure with three subplots (three columns)\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(nrows=1, ncols=3, figsize=(20, 5))\n",
    "\n",
    "    # Plot 1: Training F1 Score Only\n",
    "    epochs_range = range(1, len(demo_train_losses) + 1)\n",
    "    \n",
    "    # Check if training F1 scores are available\n",
    "    if 'demo_train_f1s' in globals():\n",
    "        ax1.plot(epochs_range, demo_train_f1s, label='Training F1', alpha=0.8, color='#2ca02c', linewidth=2, marker='o', markersize=3)\n",
    "        ax1.set_title('Enhanced Model - Training F1 Score', fontsize=14, fontweight='bold')\n",
    "        ax1.set_xlabel('Epoch')\n",
    "        ax1.set_ylabel('Training F1 Score')\n",
    "        ax1.legend()\n",
    "        ax1.grid(alpha=0.3)\n",
    "        \n",
    "        # Add text with training F1 statistics\n",
    "        final_train_f1 = demo_train_f1s[-1]\n",
    "        max_train_f1 = max(demo_train_f1s)\n",
    "        max_train_f1_epoch = demo_train_f1s.index(max_train_f1) + 1\n",
    "        ax1.text(0.02, 0.98, f'Best Train F1: {max_train_f1:.4f} (Epoch {max_train_f1_epoch})\\nFinal: {final_train_f1:.4f}', \n",
    "                 transform=ax1.transAxes, verticalalignment='top', \n",
    "                 bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.7))\n",
    "    else:\n",
    "        # Fallback to training loss if F1 scores not available\n",
    "        ax1.plot(epochs_range, demo_train_losses, label='Training loss', alpha=0.8, color='#2ca02c', linewidth=2, marker='o', markersize=3)\n",
    "        ax1.set_title('Enhanced Model - Training Loss', fontsize=14, fontweight='bold')\n",
    "        ax1.set_xlabel('Epoch')\n",
    "        ax1.set_ylabel('Training Loss')\n",
    "        ax1.legend()\n",
    "        ax1.grid(alpha=0.3)\n",
    "        \n",
    "        # Add text with final training loss\n",
    "        final_train_loss = demo_train_losses[-1]\n",
    "        min_train_loss = min(demo_train_losses)\n",
    "        min_train_loss_epoch = demo_train_losses.index(min_train_loss) + 1\n",
    "        ax1.text(0.02, 0.98, f'Min Train Loss: {min_train_loss:.4f} (Epoch {min_train_loss_epoch})\\nFinal: {final_train_loss:.4f}', \n",
    "                 transform=ax1.transAxes, verticalalignment='top', \n",
    "                 bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.7))\n",
    "        ax1.text(0.02, 0.85, 'Note: Training F1 scores not available\\nShowing training loss instead', \n",
    "                 transform=ax1.transAxes, verticalalignment='top', \n",
    "                 bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.5))\n",
    "\n",
    "    # Plot 2: Training vs Validation Loss Comparison\n",
    "    ax2.plot(epochs_range, demo_train_losses, label='Training loss', alpha=0.7, color='#2ca02c', linewidth=2)\n",
    "    ax2.plot(epochs_range, demo_val_losses, label='Validation loss', alpha=0.9, color='#d62728', linewidth=2)\n",
    "    ax2.set_title('Enhanced Model - Loss Comparison', fontsize=14, fontweight='bold')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.legend()\n",
    "    ax2.grid(alpha=0.3)\n",
    "    \n",
    "    # Add text with best validation loss\n",
    "    best_val_loss = min(demo_val_losses)\n",
    "    best_val_loss_epoch = demo_val_losses.index(best_val_loss) + 1\n",
    "    ax2.text(0.02, 0.98, f'Best Val Loss: {best_val_loss:.4f} (Epoch {best_val_loss_epoch})', \n",
    "             transform=ax2.transAxes, verticalalignment='top', \n",
    "             bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.7))\n",
    "\n",
    "    # Plot 3: Validation F1 Score\n",
    "    ax3.plot(epochs_range, demo_val_f1s, label='Validation F1', alpha=0.9, color='#1f77b4', linewidth=2, marker='o', markersize=4)\n",
    "    ax3.set_title('Enhanced Model - F1 Score Over Time', fontsize=14, fontweight='bold')\n",
    "    ax3.set_xlabel('Epoch')\n",
    "    ax3.set_ylabel('F1 Score')\n",
    "    ax3.legend()\n",
    "    ax3.grid(alpha=0.3)\n",
    "    \n",
    "    # Add text with best F1 score\n",
    "    best_f1 = max(demo_val_f1s)\n",
    "    best_f1_epoch = demo_val_f1s.index(best_f1) + 1\n",
    "    ax3.text(0.02, 0.98, f'Best Val F1: {best_f1:.4f} (Epoch {best_f1_epoch})', \n",
    "             transform=ax3.transAxes, verticalalignment='top', \n",
    "             bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.7))\n",
    "\n",
    "    # Adjust the layout and display the plot\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(right=0.85)\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(f\"\\nüìä Enhanced Model Training Summary:\")\n",
    "    print(f\"   ‚Ä¢ Total Epochs: {len(demo_train_losses)}\")\n",
    "    print(f\"   ‚Ä¢ Best Validation F1: {best_f1:.4f} (Epoch {best_f1_epoch})\")\n",
    "    print(f\"   ‚Ä¢ Best Validation Loss: {best_val_loss:.4f} (Epoch {best_val_loss_epoch})\")\n",
    "    print(f\"   ‚Ä¢ Final Validation F1: {demo_val_f1s[-1]:.4f}\")\n",
    "    print(f\"   ‚Ä¢ Final Validation Loss: {demo_val_losses[-1]:.4f}\")\n",
    "    \n",
    "    # Compare with original model results (if available)\n",
    "    if 'best_val_f1' in globals():\n",
    "        improvement = best_f1 - best_val_f1\n",
    "        print(f\"\\nüî• Comparison with Original Model:\")\n",
    "        print(f\"   ‚Ä¢ Original Best F1: {best_val_f1:.4f}\")\n",
    "        print(f\"   ‚Ä¢ Enhanced Best F1: {best_f1:.4f}\")\n",
    "        print(f\"   ‚Ä¢ Improvement: {improvement:+.4f} ({improvement/best_val_f1*100:+.2f}%)\")\n",
    "        \n",
    "        if improvement > 0:\n",
    "            print(f\"   ‚úÖ Enhanced model performs BETTER!\")\n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è Original model still performs better. Consider hyperparameter tuning.\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Enhanced model training results not found.\")\n",
    "    print(\"Please run the enhanced model training first:\")\n",
    "    print(\"   1. Execute the enhanced model training cell\")\n",
    "    print(\"   2. Then run this plotting cell\")\n",
    "    print(\"\\nAvailable variables:\")\n",
    "    available_vars = [var for var in ['demo_train_losses', 'demo_val_losses', 'demo_val_f1s', 'demo_train_f1s'] if var in globals()]\n",
    "    print(f\"   ‚Ä¢ Found: {available_vars}\")\n",
    "    missing_vars = [var for var in ['demo_train_losses', 'demo_val_losses', 'demo_val_f1s', 'demo_train_f1s'] if var not in globals()]\n",
    "    print(f\"   ‚Ä¢ Missing: {missing_vars}\")\n",
    "    \n",
    "    # Special note about training F1 scores\n",
    "    if 'demo_train_f1s' not in globals():\n",
    "        print(f\"   ‚Ä¢ Note: Training F1 scores not available - will show training loss instead in first plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Enhanced Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Enhanced Model Saved Successfully!\n",
      "üìÅ File: models/enhanced_lstm_embeddings_F10.90_20251113_180601.pt\n",
      "üìä Performance: Best F1 = 0.9008 (Epoch 170)\n",
      "üîß Model: EnhancedRecurrentClassifier\n",
      "üìà Parameters: 66,479\n",
      "‚úÖ Verified: File exists (0.26 MB)\n",
      "\n",
      "üí° To load this model later, use:\n",
      "   checkpoint = torch.load('models/enhanced_lstm_embeddings_F10.90_20251113_180601.pt')\n",
      "   model.load_state_dict(checkpoint['model_state_dict'])\n"
     ]
    }
   ],
   "source": [
    "# Simple model saving code\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Create models directory if it doesn't exist\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "# Generate timestamp for unique naming\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Calculate performance metrics\n",
    "if 'demo_val_f1s' in globals() and len(demo_val_f1s) > 0:\n",
    "    best_f1 = max(demo_val_f1s)\n",
    "    best_epoch = demo_val_f1s.index(best_f1) + 1\n",
    "    final_f1 = demo_val_f1s[-1]\n",
    "else:\n",
    "    best_f1 = final_f1 = best_epoch = 0\n",
    "\n",
    "# Create filename with performance info\n",
    "model_name = f\"enhanced_lstm_embeddings_F1{best_f1:.2f}_{timestamp}\"\n",
    "model_path = f\"models/{model_name}.pt\"\n",
    "\n",
    "# Save the model\n",
    "torch.save({\n",
    "    'model_state_dict': demo_enhanced_model.state_dict(),\n",
    "    'model_class': 'EnhancedRecurrentClassifier',\n",
    "    'timestamp': timestamp,\n",
    "    'training_info': {\n",
    "        'best_val_f1': best_f1,\n",
    "        'best_epoch': best_epoch,\n",
    "        'final_val_f1': final_f1,\n",
    "        'total_epochs': len(demo_val_f1s) if 'demo_val_f1s' in globals() else 0,\n",
    "        'model_config': {\n",
    "            'hidden_size': HIDDEN_SIZE,\n",
    "            'num_layers': 2,\n",
    "            'rnn_type': 'LSTM',\n",
    "            'bidirectional': True,\n",
    "            'dropout_rate': 0.2,\n",
    "            'continuous_input_size': 17,\n",
    "            'categorical_features': ['pain_survey_1', 'pain_survey_2', 'pain_survey_3', 'pain_survey_4'],\n",
    "            'embedding_dims': {'pain_survey_1': 2, 'pain_survey_2': 2, 'pain_survey_3': 2, 'pain_survey_4': 2}\n",
    "        },\n",
    "        'regularization': {\n",
    "            'l1_lambda': L1_LAMBDA,\n",
    "            'l2_lambda': L2_LAMBDA,\n",
    "            'weight_decay': L2_LAMBDA\n",
    "        }\n",
    "    }\n",
    "}, model_path)\n",
    "\n",
    "print(\"üíæ Enhanced Model Saved Successfully!\")\n",
    "print(f\"üìÅ File: {model_path}\")\n",
    "print(f\"üìä Performance: Best F1 = {best_f1:.4f} (Epoch {best_epoch})\")\n",
    "print(f\"üîß Model: {demo_enhanced_model.__class__.__name__}\")\n",
    "print(f\"üìà Parameters: {sum(p.numel() for p in demo_enhanced_model.parameters()):,}\")\n",
    "\n",
    "# Verification - check if file was created\n",
    "if os.path.exists(model_path):\n",
    "    file_size = os.path.getsize(model_path) / (1024 * 1024)  # Convert to MB\n",
    "    print(f\"‚úÖ Verified: File exists ({file_size:.2f} MB)\")\n",
    "else:\n",
    "    print(\"‚ùå Error: File was not created!\")\n",
    "\n",
    "print(f\"\\nüí° To load this model later, use:\")\n",
    "print(f\"   checkpoint = torch.load('{model_path}')\")\n",
    "print(f\"   model.load_state_dict(checkpoint['model_state_dict'])\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zn0xjziNo8R5"
   },
   "source": [
    "## 14 Competition Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_Wi1u77Gtxfe",
    "outputId": "2b6505cf-09ba-4763-89c7-fdd2664abf26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building ENHANCED sequences for actual test dataset with WINDOW_SIZE=50, STRIDE=10\n",
      "Available columns: ['sample_index', 'time', 'pain_survey_1', 'pain_survey_2', 'pain_survey_3', 'pain_survey_4', 'joint_00', 'joint_01', 'joint_02', 'joint_03', 'joint_04', 'joint_05', 'joint_06', 'joint_07', 'joint_08', 'joint_09', 'joint_10', 'joint_11', 'joint_12', 'joint_26', 'joint_27', 'joint_28', 'joint_29']\n",
      "Categorical features: ['pain_survey_1', 'pain_survey_2', 'pain_survey_3', 'pain_survey_4']\n",
      "Continuous features: ['joint_00', 'joint_01', 'joint_02', 'joint_03', 'joint_04', 'joint_05', 'joint_06', 'joint_07', 'joint_08', 'joint_09', 'joint_10', 'joint_11', 'joint_12', 'joint_26', 'joint_27', 'joint_28', 'joint_29'] (count: 17)\n",
      "Enhanced test sequences shapes:\n",
      "  Continuous: (15888, 50, 17)\n",
      "  pain_survey_1: (15888, 50)\n",
      "  pain_survey_2: (15888, 50)\n",
      "  pain_survey_3: (15888, 50)\n",
      "  pain_survey_4: (15888, 50)\n",
      "  Sample indices: 15888\n",
      "\n",
      "‚úÖ Enhanced test dataset created successfully!\n",
      "üìä Dataset size: 15888 samples\n",
      "üîÑ Number of batches: 34\n",
      "üìê Continuous input size: 17\n",
      "üè∑Ô∏è Categorical features: ['pain_survey_1', 'pain_survey_2', 'pain_survey_3', 'pain_survey_4']\n",
      "\n",
      "üß™ Testing Enhanced DataLoader...\n",
      "‚úÖ Continuous batch shape: torch.Size([480, 50, 17])\n",
      "‚úÖ Labels batch shape: torch.Size([480])\n",
      "‚úÖ pain_survey_1 batch shape: torch.Size([480, 50])\n",
      "‚úÖ pain_survey_2 batch shape: torch.Size([480, 50])\n",
      "‚úÖ pain_survey_3 batch shape: torch.Size([480, 50])\n",
      "‚úÖ pain_survey_4 batch shape: torch.Size([480, 50])\n",
      "\n",
      "üéØ Enhanced test data is ready for the Enhanced model!\n"
     ]
    }
   ],
   "source": [
    "# Build sequences from the actual test data for Enhanced Model\n",
    "print(f\"Building ENHANCED sequences for actual test dataset with WINDOW_SIZE={WINDOW_SIZE}, STRIDE={STRIDE}\")\n",
    "\n",
    "# Identify feature types from X_test_final_df columns\n",
    "all_columns = X_test_final_df.columns.tolist()\n",
    "print(f\"Available columns: {all_columns}\")\n",
    "\n",
    "# Define feature separation\n",
    "categorical_features = ['pain_survey_1', 'pain_survey_2', 'pain_survey_3', 'pain_survey_4']\n",
    "joint_features = [col for col in all_columns if col.startswith('joint_')]\n",
    "continuous_features = joint_features  # Continuous features are the joint features\n",
    "exclude_cols = ['sample_index'] + ([] if 'time' not in all_columns else ['time'])  # Exclude non-feature columns\n",
    "\n",
    "print(f\"Categorical features: {categorical_features}\")\n",
    "print(f\"Continuous features: {continuous_features} (count: {len(continuous_features)})\")\n",
    "\n",
    "# Create enhanced sequences function for test data\n",
    "def build_sequences_test_enhanced(df, window=WINDOW_SIZE, stride=STRIDE):\n",
    "    \"\"\"\n",
    "    Build sequences for test data separating continuous and categorical features\n",
    "    \"\"\"\n",
    "    continuous_dataset = []\n",
    "    categorical_datasets = {feature: [] for feature in categorical_features}\n",
    "    sample_indices = []\n",
    "    \n",
    "    # Get unique sample IDs\n",
    "    for sample_id in df['sample_index'].unique():\n",
    "        # Extract rows for this sample\n",
    "        sample_data = df[df['sample_index'] == sample_id].copy()\n",
    "        \n",
    "        # If sample has fewer rows than WINDOW_SIZE, pad with zeros\n",
    "        if len(sample_data) < window:\n",
    "            # Create padding dataframe\n",
    "            padding_rows = window - len(sample_data)\n",
    "            padding = pd.DataFrame(0, index=range(padding_rows), columns=sample_data.columns)\n",
    "            sample_data = pd.concat([sample_data, padding], ignore_index=True)\n",
    "        \n",
    "        # Extract continuous features (joints only)\n",
    "        continuous_data = sample_data[continuous_features].values\n",
    "        \n",
    "        # Build continuous sequences\n",
    "        continuous_seqs = []\n",
    "        for i in range(0, len(continuous_data) - window + 1, stride):\n",
    "            continuous_seqs.append(continuous_data[i:i + window])\n",
    "        \n",
    "        # If no sequences generated, take the last window\n",
    "        if len(continuous_seqs) == 0:\n",
    "            continuous_seqs = [continuous_data[-window:]]\n",
    "        \n",
    "        # Build categorical sequences\n",
    "        categorical_seqs = {feature: [] for feature in categorical_features}\n",
    "        for feature in categorical_features:\n",
    "            if feature in sample_data.columns:\n",
    "                cat_data = sample_data[feature].values\n",
    "                for i in range(0, len(cat_data) - window + 1, stride):\n",
    "                    categorical_seqs[feature].append(cat_data[i:i + window])\n",
    "                # If no sequences generated, take the last window\n",
    "                if len(categorical_seqs[feature]) == 0:\n",
    "                    categorical_seqs[feature] = [cat_data[-window:]]\n",
    "            else:\n",
    "                # If feature doesn't exist, create zero sequences\n",
    "                for _ in continuous_seqs:\n",
    "                    categorical_seqs[feature].append(np.zeros(window))\n",
    "        \n",
    "        # Store sequences (take first sequence for each sample)\n",
    "        continuous_dataset.extend(continuous_seqs)\n",
    "        for feature in categorical_features:\n",
    "            categorical_datasets[feature].extend(categorical_seqs[feature])\n",
    "        sample_indices.extend([sample_id] * len(continuous_seqs))\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    continuous_dataset = np.array(continuous_dataset, dtype='float32')\n",
    "    for feature in categorical_features:\n",
    "        categorical_datasets[feature] = np.array(categorical_datasets[feature], dtype='int64')\n",
    "    \n",
    "    return continuous_dataset, categorical_datasets, sample_indices\n",
    "\n",
    "# Build enhanced sequences\n",
    "X_test_continuous, X_test_categorical, test_sample_indices = build_sequences_test_enhanced(X_test_final_df)\n",
    "\n",
    "# Handle NaN values\n",
    "if np.isnan(X_test_continuous).any():\n",
    "    X_test_continuous = np.nan_to_num(X_test_continuous)\n",
    "    print(\"NaN values found and replaced with 0 in continuous test sequences.\")\n",
    "\n",
    "for feature in categorical_features:\n",
    "    if np.isnan(X_test_categorical[feature]).any():\n",
    "        X_test_categorical[feature] = np.nan_to_num(X_test_categorical[feature]).astype('int64')\n",
    "        print(f\"NaN values found and replaced with 0 in {feature} test sequences.\")\n",
    "\n",
    "print(f\"Enhanced test sequences shapes:\")\n",
    "print(f\"  Continuous: {X_test_continuous.shape}\")\n",
    "for feature in categorical_features:\n",
    "    print(f\"  {feature}: {X_test_categorical[feature].shape}\")\n",
    "print(f\"  Sample indices: {len(test_sample_indices)}\")\n",
    "\n",
    "# Create dummy labels for test data (required by EnhancedDataset but not used)\n",
    "dummy_labels = np.zeros(len(test_sample_indices), dtype='int64')\n",
    "\n",
    "# Create Enhanced dataset\n",
    "test_enhanced_final_ds = EnhancedDataset(X_test_continuous, X_test_categorical, dummy_labels)\n",
    "\n",
    "# Create Enhanced DataLoader\n",
    "test_enhanced_final_loader = make_enhanced_loader(\n",
    "    test_enhanced_final_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    sampler=None\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Enhanced test dataset created successfully!\")\n",
    "print(f\"üìä Dataset size: {len(test_enhanced_final_ds)} samples\")\n",
    "print(f\"üîÑ Number of batches: {len(test_enhanced_final_loader)}\")\n",
    "print(f\"üìê Continuous input size: {X_test_continuous.shape[-1]}\")\n",
    "print(f\"üè∑Ô∏è Categorical features: {list(X_test_categorical.keys())}\")\n",
    "\n",
    "# Test the enhanced loader\n",
    "print(f\"\\nüß™ Testing Enhanced DataLoader...\")\n",
    "for continuous_batch, categorical_batch, labels_batch in test_enhanced_final_loader:\n",
    "    print(f\"‚úÖ Continuous batch shape: {continuous_batch.shape}\")\n",
    "    print(f\"‚úÖ Labels batch shape: {labels_batch.shape}\")\n",
    "    for feature, data in categorical_batch.items():\n",
    "        print(f\"‚úÖ {feature} batch shape: {data.shape}\")\n",
    "    break\n",
    "\n",
    "print(f\"\\nüéØ Enhanced test data is ready for the Enhanced model!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "id": "Zhd7Pt3JwILk",
    "outputId": "860f454c-21e4-4547-90ba-7204cd564ffd"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_index</th>\n",
       "      <th>time</th>\n",
       "      <th>pain_survey_1</th>\n",
       "      <th>pain_survey_2</th>\n",
       "      <th>pain_survey_3</th>\n",
       "      <th>pain_survey_4</th>\n",
       "      <th>joint_00</th>\n",
       "      <th>joint_01</th>\n",
       "      <th>joint_02</th>\n",
       "      <th>joint_03</th>\n",
       "      <th>...</th>\n",
       "      <th>joint_07</th>\n",
       "      <th>joint_08</th>\n",
       "      <th>joint_09</th>\n",
       "      <th>joint_10</th>\n",
       "      <th>joint_11</th>\n",
       "      <th>joint_12</th>\n",
       "      <th>joint_26</th>\n",
       "      <th>joint_27</th>\n",
       "      <th>joint_28</th>\n",
       "      <th>joint_29</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.561563</td>\n",
       "      <td>0.553352</td>\n",
       "      <td>0.419037</td>\n",
       "      <td>0.270175</td>\n",
       "      <td>...</td>\n",
       "      <td>0.654303</td>\n",
       "      <td>0.737832</td>\n",
       "      <td>0.742275</td>\n",
       "      <td>0.100076</td>\n",
       "      <td>0.146564</td>\n",
       "      <td>0.745300</td>\n",
       "      <td>0.014909</td>\n",
       "      <td>0.045098</td>\n",
       "      <td>0.012882</td>\n",
       "      <td>0.010178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.599088</td>\n",
       "      <td>0.532067</td>\n",
       "      <td>0.461325</td>\n",
       "      <td>0.327922</td>\n",
       "      <td>...</td>\n",
       "      <td>0.684443</td>\n",
       "      <td>0.772454</td>\n",
       "      <td>0.710705</td>\n",
       "      <td>0.103457</td>\n",
       "      <td>0.174403</td>\n",
       "      <td>0.594262</td>\n",
       "      <td>0.053679</td>\n",
       "      <td>0.055375</td>\n",
       "      <td>0.013892</td>\n",
       "      <td>0.029085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.638365</td>\n",
       "      <td>0.583960</td>\n",
       "      <td>0.445804</td>\n",
       "      <td>0.308796</td>\n",
       "      <td>...</td>\n",
       "      <td>0.676488</td>\n",
       "      <td>0.799646</td>\n",
       "      <td>0.722061</td>\n",
       "      <td>0.143175</td>\n",
       "      <td>0.159973</td>\n",
       "      <td>0.652024</td>\n",
       "      <td>0.042305</td>\n",
       "      <td>0.039620</td>\n",
       "      <td>0.016286</td>\n",
       "      <td>0.040638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.554938</td>\n",
       "      <td>0.488719</td>\n",
       "      <td>0.443494</td>\n",
       "      <td>0.355023</td>\n",
       "      <td>...</td>\n",
       "      <td>0.650563</td>\n",
       "      <td>0.738087</td>\n",
       "      <td>0.709363</td>\n",
       "      <td>0.141007</td>\n",
       "      <td>0.167449</td>\n",
       "      <td>0.709558</td>\n",
       "      <td>0.037477</td>\n",
       "      <td>0.031101</td>\n",
       "      <td>0.008568</td>\n",
       "      <td>0.018730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.537192</td>\n",
       "      <td>0.528780</td>\n",
       "      <td>0.413159</td>\n",
       "      <td>0.363199</td>\n",
       "      <td>...</td>\n",
       "      <td>0.653239</td>\n",
       "      <td>0.703021</td>\n",
       "      <td>0.681513</td>\n",
       "      <td>0.140234</td>\n",
       "      <td>0.186249</td>\n",
       "      <td>0.590142</td>\n",
       "      <td>0.015210</td>\n",
       "      <td>0.019426</td>\n",
       "      <td>0.008189</td>\n",
       "      <td>0.013444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211835</th>\n",
       "      <td>1323.0</td>\n",
       "      <td>155.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.746791</td>\n",
       "      <td>0.715951</td>\n",
       "      <td>0.787217</td>\n",
       "      <td>0.908019</td>\n",
       "      <td>...</td>\n",
       "      <td>0.746858</td>\n",
       "      <td>0.646380</td>\n",
       "      <td>0.606524</td>\n",
       "      <td>0.361932</td>\n",
       "      <td>0.515863</td>\n",
       "      <td>0.762767</td>\n",
       "      <td>0.009774</td>\n",
       "      <td>0.031095</td>\n",
       "      <td>0.006292</td>\n",
       "      <td>0.070267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211836</th>\n",
       "      <td>1323.0</td>\n",
       "      <td>156.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.712191</td>\n",
       "      <td>0.805042</td>\n",
       "      <td>0.749162</td>\n",
       "      <td>0.829449</td>\n",
       "      <td>...</td>\n",
       "      <td>0.741321</td>\n",
       "      <td>0.576658</td>\n",
       "      <td>0.647361</td>\n",
       "      <td>0.389554</td>\n",
       "      <td>0.471334</td>\n",
       "      <td>0.723960</td>\n",
       "      <td>0.027009</td>\n",
       "      <td>0.049113</td>\n",
       "      <td>0.029173</td>\n",
       "      <td>0.068884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211837</th>\n",
       "      <td>1323.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.778327</td>\n",
       "      <td>0.727727</td>\n",
       "      <td>0.775497</td>\n",
       "      <td>0.841656</td>\n",
       "      <td>...</td>\n",
       "      <td>0.762795</td>\n",
       "      <td>0.678090</td>\n",
       "      <td>0.628388</td>\n",
       "      <td>0.353318</td>\n",
       "      <td>0.453431</td>\n",
       "      <td>0.739291</td>\n",
       "      <td>0.025910</td>\n",
       "      <td>0.042127</td>\n",
       "      <td>0.011971</td>\n",
       "      <td>0.079291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211838</th>\n",
       "      <td>1323.0</td>\n",
       "      <td>158.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.740327</td>\n",
       "      <td>0.768988</td>\n",
       "      <td>0.749230</td>\n",
       "      <td>0.854667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.694102</td>\n",
       "      <td>0.654149</td>\n",
       "      <td>0.611106</td>\n",
       "      <td>0.378361</td>\n",
       "      <td>0.497827</td>\n",
       "      <td>0.714944</td>\n",
       "      <td>0.052790</td>\n",
       "      <td>0.019468</td>\n",
       "      <td>0.015994</td>\n",
       "      <td>0.106760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211839</th>\n",
       "      <td>1323.0</td>\n",
       "      <td>159.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.708521</td>\n",
       "      <td>0.771210</td>\n",
       "      <td>0.785865</td>\n",
       "      <td>0.833756</td>\n",
       "      <td>...</td>\n",
       "      <td>0.710532</td>\n",
       "      <td>0.658793</td>\n",
       "      <td>0.608914</td>\n",
       "      <td>0.360460</td>\n",
       "      <td>0.440642</td>\n",
       "      <td>0.810988</td>\n",
       "      <td>0.033291</td>\n",
       "      <td>0.024055</td>\n",
       "      <td>0.047641</td>\n",
       "      <td>0.099013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>211840 rows √ó 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        sample_index   time  pain_survey_1  pain_survey_2  pain_survey_3  \\\n",
       "0                0.0    0.0            1.0            1.0            1.0   \n",
       "1                0.0    1.0            1.0            1.0            1.0   \n",
       "2                0.0    2.0            1.0            1.0            1.0   \n",
       "3                0.0    3.0            0.5            1.0            1.0   \n",
       "4                0.0    4.0            1.0            1.0            1.0   \n",
       "...              ...    ...            ...            ...            ...   \n",
       "211835        1323.0  155.0            1.0            1.0            0.5   \n",
       "211836        1323.0  156.0            0.5            1.0            1.0   \n",
       "211837        1323.0  157.0            0.5            1.0            1.0   \n",
       "211838        1323.0  158.0            1.0            1.0            0.5   \n",
       "211839        1323.0  159.0            0.5            0.0            1.0   \n",
       "\n",
       "        pain_survey_4  joint_00  joint_01  joint_02  joint_03  ...  joint_07  \\\n",
       "0                 1.0  0.561563  0.553352  0.419037  0.270175  ...  0.654303   \n",
       "1                 1.0  0.599088  0.532067  0.461325  0.327922  ...  0.684443   \n",
       "2                 1.0  0.638365  0.583960  0.445804  0.308796  ...  0.676488   \n",
       "3                 1.0  0.554938  0.488719  0.443494  0.355023  ...  0.650563   \n",
       "4                 0.0  0.537192  0.528780  0.413159  0.363199  ...  0.653239   \n",
       "...               ...       ...       ...       ...       ...  ...       ...   \n",
       "211835            1.0  0.746791  0.715951  0.787217  0.908019  ...  0.746858   \n",
       "211836            1.0  0.712191  0.805042  0.749162  0.829449  ...  0.741321   \n",
       "211837            1.0  0.778327  0.727727  0.775497  0.841656  ...  0.762795   \n",
       "211838            1.0  0.740327  0.768988  0.749230  0.854667  ...  0.694102   \n",
       "211839            0.0  0.708521  0.771210  0.785865  0.833756  ...  0.710532   \n",
       "\n",
       "        joint_08  joint_09  joint_10  joint_11  joint_12  joint_26  joint_27  \\\n",
       "0       0.737832  0.742275  0.100076  0.146564  0.745300  0.014909  0.045098   \n",
       "1       0.772454  0.710705  0.103457  0.174403  0.594262  0.053679  0.055375   \n",
       "2       0.799646  0.722061  0.143175  0.159973  0.652024  0.042305  0.039620   \n",
       "3       0.738087  0.709363  0.141007  0.167449  0.709558  0.037477  0.031101   \n",
       "4       0.703021  0.681513  0.140234  0.186249  0.590142  0.015210  0.019426   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "211835  0.646380  0.606524  0.361932  0.515863  0.762767  0.009774  0.031095   \n",
       "211836  0.576658  0.647361  0.389554  0.471334  0.723960  0.027009  0.049113   \n",
       "211837  0.678090  0.628388  0.353318  0.453431  0.739291  0.025910  0.042127   \n",
       "211838  0.654149  0.611106  0.378361  0.497827  0.714944  0.052790  0.019468   \n",
       "211839  0.658793  0.608914  0.360460  0.440642  0.810988  0.033291  0.024055   \n",
       "\n",
       "        joint_28  joint_29  \n",
       "0       0.012882  0.010178  \n",
       "1       0.013892  0.029085  \n",
       "2       0.016286  0.040638  \n",
       "3       0.008568  0.018730  \n",
       "4       0.008189  0.013444  \n",
       "...          ...       ...  \n",
       "211835  0.006292  0.070267  \n",
       "211836  0.029173  0.068884  \n",
       "211837  0.011971  0.079291  \n",
       "211838  0.015994  0.106760  \n",
       "211839  0.047641  0.099013  \n",
       "\n",
       "[211840 rows x 23 columns]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show test_final\n",
    "X_test_final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # FIXED Enhanced Model Competition Submission\n",
    "# print(\"üîß FIXING ENHANCED MODEL SUBMISSION\")\n",
    "# print(\"=\" * 50)\n",
    "\n",
    "# # The issue is that we're using the competition_enhanced_loader which might not preserve sample indices\n",
    "# # Let's create the submission manually using the competition_enhanced_ds dataset\n",
    "\n",
    "# print(\"üéØ Creating submission with correct sample indices...\")\n",
    "\n",
    "# demo_enhanced_model.eval()\n",
    "\n",
    "# # Create submission data with proper indices\n",
    "# fixed_submission_data = []\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for i in range(len(competition_enhanced_ds)):\n",
    "#         # Get individual sample\n",
    "#         continuous_sample, categorical_sample, sample_idx = competition_enhanced_ds[i]\n",
    "        \n",
    "#         # Add batch dimension and move to device\n",
    "#         continuous_batch = continuous_sample.unsqueeze(0).to(device)\n",
    "#         categorical_batch = {k: v.unsqueeze(0).to(device) for k, v in categorical_sample.items()}\n",
    "        \n",
    "#         # Get prediction\n",
    "#         logits = demo_enhanced_model(continuous_batch, categorical_batch)\n",
    "#         pred = logits.argmax(dim=1).item()\n",
    "        \n",
    "#         # Convert to label\n",
    "#         label_map = {0: \"no_pain\", 1: \"low_pain\", 2: \"high_pain\"}\n",
    "#         pred_label = label_map[pred]\n",
    "        \n",
    "#         # Add to submission data with correct index formatting\n",
    "#         fixed_submission_data.append({\n",
    "#             \"sample_index\": f\"{i:03d}\",  # Format as 000, 001, 002, ..., 1323\n",
    "#             \"label\": pred_label\n",
    "#         })\n",
    "        \n",
    "#         # Progress indicator\n",
    "#         if (i + 1) % 200 == 0 or (i + 1) == len(competition_enhanced_ds):\n",
    "#             print(f\"Processed {i + 1}/{len(competition_enhanced_ds)} samples\")\n",
    "\n",
    "# # Create DataFrame\n",
    "# fixed_submission_df = pd.DataFrame(fixed_submission_data)\n",
    "\n",
    "# # Save corrected submission\n",
    "# fixed_filename = f\"enhanced_submission_FIXED_early_stopped_{current_datetime}.csv\"\n",
    "# fixed_submission_df.to_csv(fixed_filename, index=False)\n",
    "\n",
    "# print(f\"\\n‚úÖ FIXED submission saved: {fixed_filename}\")\n",
    "# print(f\"üìä Shape: {fixed_submission_df.shape}\")\n",
    "\n",
    "# # Verify the fix\n",
    "# sample_idx_counts = fixed_submission_df['sample_index'].value_counts()\n",
    "# print(f\"\\nüîç Verification:\")\n",
    "# print(f\"   Unique sample indices: {len(sample_idx_counts)}\")\n",
    "# print(f\"   Expected: 1324\")\n",
    "# print(f\"   Index range: {fixed_submission_df['sample_index'].min()} to {fixed_submission_df['sample_index'].max()}\")\n",
    "\n",
    "# if len(sample_idx_counts) == 1324:\n",
    "#     print(\"   ‚úÖ Sample indexing FIXED!\")\n",
    "# else:\n",
    "#     print(\"   ‚ùå Sample indexing still has issues\")\n",
    "\n",
    "# # Show label distribution\n",
    "# label_dist = fixed_submission_df['label'].value_counts()\n",
    "# label_pct = fixed_submission_df['label'].value_counts(normalize=True) * 100\n",
    "\n",
    "# print(f\"\\nüìä FIXED Label Distribution:\")\n",
    "# for label in ['no_pain', 'low_pain', 'high_pain']:\n",
    "#     count = label_dist.get(label, 0)\n",
    "#     pct = label_pct.get(label, 0)\n",
    "#     print(f\"   {label:>10}: {count:4d} ({pct:5.1f}%)\")\n",
    "\n",
    "# print(f\"\\nüéØ SUBMISSION STATUS:\")\n",
    "# if len(sample_idx_counts) == 1324:\n",
    "#     print(\"   ‚úÖ Ready for competition submission!\")\n",
    "#     print(f\"   üìÅ File: {fixed_filename}\")\n",
    "    \n",
    "#     high_pain_pct = label_pct.get('high_pain', 0)\n",
    "#     if high_pain_pct > 50:\n",
    "#         print(f\"   ‚ö†Ô∏è  Note: High 'high_pain' predictions ({high_pain_pct:.1f}%)\")\n",
    "#         print(\"   This suggests the model may still be overfitting\")\n",
    "#         print(\"   Expected test improvement: 76-82% (from previous 72.75%)\")\n",
    "# else:\n",
    "#     print(\"   ‚ùå Still has indexing issues - do not submit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Enhanced Model Summary & Recommendations\n",
    "\n",
    "### ‚úÖ **What We Fixed:**\n",
    "1. **Early Stopping Implementation** - Prevented overfitting by stopping at best validation epoch\n",
    "2. **Enhanced Model Architecture** - Added embedding layers for categorical features \n",
    "3. **Sample Index Bug** - Fixed submission format to have unique sample indices\n",
    "4. **Model Inference** - Corrected to use both continuous and categorical inputs\n",
    "\n",
    "### ‚ö†Ô∏è **Remaining Concerns:**\n",
    "\n",
    "#### **Label Distribution Imbalance:**\n",
    "- Training: 77.3% no_pain, 14.3% low_pain, 8.4% high_pain  \n",
    "- Predictions: 27.1% no_pain, 19.6% low_pain, 53.2% high_pain\n",
    "- **Issue:** Model heavily predicts \"high_pain\" (53.2% vs 8.4% in training)\n",
    "\n",
    "#### **Expected Performance:**\n",
    "- **Previous Test Accuracy:** 72.75%\n",
    "- **Early Stopped Model Expected:** 76-82%\n",
    "- **With Current Imbalance:** Might be lower due to class distribution mismatch\n",
    "\n",
    "### üöÄ **Next Actions:**\n",
    "\n",
    "#### **Option 1: Submit Current Model (Recommended)**\n",
    "- File: `enhanced_submission_FIXED_early_stopped_13-11-11-18.csv`\n",
    "- **Pros:** Uses early stopping, should generalize better than previous overfitted model\n",
    "- **Expected:** 3-8% improvement over 72.75%\n",
    "\n",
    "#### **Option 2: Address Class Imbalance (If Time Permits)**\n",
    "- Retrain with balanced sampling\n",
    "- Adjust loss function class weights\n",
    "- Use threshold tuning for predictions\n",
    "\n",
    "### üìä **Model Comparison:**\n",
    "```\n",
    "Original Model (Overfitted):\n",
    "- Validation F1: 91.34% \n",
    "- Test Accuracy: 72.75%\n",
    "- Gap: 18.59% (severe overfitting)\n",
    "\n",
    "Enhanced Model (Early Stopped):\n",
    "- Validation F1: ~90% (estimated at best epoch)\n",
    "- Expected Test: 76-82%\n",
    "- Gap: 8-14% (improved generalization)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 381
    },
    "id": "m2xyVfs7YAYH",
    "outputId": "5c0fe176-6d0f-4795-a6ee-b4332274dd85"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "dataframe",
       "variable_name": "df_train"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-810019b4-1f66-450a-a51d-9755a2a3bd6a\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_index</th>\n",
       "      <th>time</th>\n",
       "      <th>pain_survey_1</th>\n",
       "      <th>pain_survey_2</th>\n",
       "      <th>pain_survey_3</th>\n",
       "      <th>pain_survey_4</th>\n",
       "      <th>joint_00</th>\n",
       "      <th>joint_01</th>\n",
       "      <th>joint_02</th>\n",
       "      <th>joint_03</th>\n",
       "      <th>...</th>\n",
       "      <th>joint_08</th>\n",
       "      <th>joint_09</th>\n",
       "      <th>joint_10</th>\n",
       "      <th>joint_11</th>\n",
       "      <th>joint_12</th>\n",
       "      <th>joint_26</th>\n",
       "      <th>joint_27</th>\n",
       "      <th>joint_28</th>\n",
       "      <th>joint_29</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.777046</td>\n",
       "      <td>0.738252</td>\n",
       "      <td>0.779512</td>\n",
       "      <td>0.804419</td>\n",
       "      <td>...</td>\n",
       "      <td>0.478382</td>\n",
       "      <td>0.753815</td>\n",
       "      <td>0.272106</td>\n",
       "      <td>0.269510</td>\n",
       "      <td>0.762947</td>\n",
       "      <td>0.014214</td>\n",
       "      <td>0.011376</td>\n",
       "      <td>0.018978</td>\n",
       "      <td>0.020291</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.805855</td>\n",
       "      <td>0.765147</td>\n",
       "      <td>0.761153</td>\n",
       "      <td>0.838021</td>\n",
       "      <td>...</td>\n",
       "      <td>0.486231</td>\n",
       "      <td>0.761224</td>\n",
       "      <td>0.217448</td>\n",
       "      <td>0.245846</td>\n",
       "      <td>0.727910</td>\n",
       "      <td>0.010748</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009473</td>\n",
       "      <td>0.010006</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.767110</td>\n",
       "      <td>0.721439</td>\n",
       "      <td>0.772834</td>\n",
       "      <td>0.777832</td>\n",
       "      <td>...</td>\n",
       "      <td>0.441994</td>\n",
       "      <td>0.725601</td>\n",
       "      <td>0.207995</td>\n",
       "      <td>0.258133</td>\n",
       "      <td>0.760757</td>\n",
       "      <td>0.013097</td>\n",
       "      <td>0.006830</td>\n",
       "      <td>0.017065</td>\n",
       "      <td>0.016856</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.665528</td>\n",
       "      <td>0.810416</td>\n",
       "      <td>0.763971</td>\n",
       "      <td>0.785928</td>\n",
       "      <td>...</td>\n",
       "      <td>0.469554</td>\n",
       "      <td>0.751688</td>\n",
       "      <td>0.238584</td>\n",
       "      <td>0.250324</td>\n",
       "      <td>0.767434</td>\n",
       "      <td>0.009505</td>\n",
       "      <td>0.006274</td>\n",
       "      <td>0.020264</td>\n",
       "      <td>0.017981</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.773829</td>\n",
       "      <td>0.773366</td>\n",
       "      <td>0.772162</td>\n",
       "      <td>0.767017</td>\n",
       "      <td>...</td>\n",
       "      <td>0.477740</td>\n",
       "      <td>0.749873</td>\n",
       "      <td>0.221475</td>\n",
       "      <td>0.290464</td>\n",
       "      <td>0.772967</td>\n",
       "      <td>0.004216</td>\n",
       "      <td>0.002132</td>\n",
       "      <td>0.023389</td>\n",
       "      <td>0.018477</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.813575</td>\n",
       "      <td>0.765834</td>\n",
       "      <td>0.746716</td>\n",
       "      <td>0.772337</td>\n",
       "      <td>...</td>\n",
       "      <td>0.424332</td>\n",
       "      <td>0.758200</td>\n",
       "      <td>0.283672</td>\n",
       "      <td>0.281967</td>\n",
       "      <td>0.803481</td>\n",
       "      <td>0.004861</td>\n",
       "      <td>0.005427</td>\n",
       "      <td>0.023442</td>\n",
       "      <td>0.017338</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.728054</td>\n",
       "      <td>0.809135</td>\n",
       "      <td>0.705452</td>\n",
       "      <td>0.698926</td>\n",
       "      <td>...</td>\n",
       "      <td>0.496126</td>\n",
       "      <td>0.748269</td>\n",
       "      <td>0.216302</td>\n",
       "      <td>0.288977</td>\n",
       "      <td>0.741954</td>\n",
       "      <td>0.005143</td>\n",
       "      <td>0.005407</td>\n",
       "      <td>0.022523</td>\n",
       "      <td>0.013901</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.737113</td>\n",
       "      <td>0.778694</td>\n",
       "      <td>0.806507</td>\n",
       "      <td>0.799577</td>\n",
       "      <td>...</td>\n",
       "      <td>0.416611</td>\n",
       "      <td>0.714467</td>\n",
       "      <td>0.231869</td>\n",
       "      <td>0.281431</td>\n",
       "      <td>0.804961</td>\n",
       "      <td>0.012911</td>\n",
       "      <td>0.004546</td>\n",
       "      <td>0.025178</td>\n",
       "      <td>0.011477</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.698434</td>\n",
       "      <td>0.753477</td>\n",
       "      <td>0.739042</td>\n",
       "      <td>0.831681</td>\n",
       "      <td>...</td>\n",
       "      <td>0.496911</td>\n",
       "      <td>0.707258</td>\n",
       "      <td>0.222234</td>\n",
       "      <td>0.291893</td>\n",
       "      <td>0.770717</td>\n",
       "      <td>0.016622</td>\n",
       "      <td>0.007172</td>\n",
       "      <td>0.006115</td>\n",
       "      <td>0.011130</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9 rows √ó 24 columns</p>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-810019b4-1f66-450a-a51d-9755a2a3bd6a')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-810019b4-1f66-450a-a51d-9755a2a3bd6a button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-810019b4-1f66-450a-a51d-9755a2a3bd6a');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    <div id=\"df-af551b3c-0e36-4f0a-a55c-465ccd7e6ffc\">\n",
       "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-af551b3c-0e36-4f0a-a55c-465ccd7e6ffc')\"\n",
       "                title=\"Suggest charts\"\n",
       "                style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "      </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "      <script>\n",
       "        async function quickchart(key) {\n",
       "          const quickchartButtonEl =\n",
       "            document.querySelector('#' + key + ' button');\n",
       "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "          try {\n",
       "            const charts = await google.colab.kernel.invokeFunction(\n",
       "                'suggestCharts', [key], {});\n",
       "          } catch (error) {\n",
       "            console.error('Error during call to suggestCharts:', error);\n",
       "          }\n",
       "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "        }\n",
       "        (() => {\n",
       "          let quickchartButtonEl =\n",
       "            document.querySelector('#df-af551b3c-0e36-4f0a-a55c-465ccd7e6ffc button');\n",
       "          quickchartButtonEl.style.display =\n",
       "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "        })();\n",
       "      </script>\n",
       "    </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "   sample_index  time  pain_survey_1  pain_survey_2  pain_survey_3  \\\n",
       "0             0     0            1.0            0.0            1.0   \n",
       "1             0     1            1.0            1.0            1.0   \n",
       "2             0     2            1.0            0.0            1.0   \n",
       "3             0     3            1.0            1.0            1.0   \n",
       "4             0     4            1.0            1.0            1.0   \n",
       "5             0     5            1.0            0.0            1.0   \n",
       "6             0     6            1.0            0.5            1.0   \n",
       "7             0     7            1.0            1.0            1.0   \n",
       "8             0     8            1.0            1.0            0.0   \n",
       "\n",
       "   pain_survey_4  joint_00  joint_01  joint_02  joint_03  ...  joint_08  \\\n",
       "0            0.5  0.777046  0.738252  0.779512  0.804419  ...  0.478382   \n",
       "1            1.0  0.805855  0.765147  0.761153  0.838021  ...  0.486231   \n",
       "2            1.0  0.767110  0.721439  0.772834  0.777832  ...  0.441994   \n",
       "3            1.0  0.665528  0.810416  0.763971  0.785928  ...  0.469554   \n",
       "4            1.0  0.773829  0.773366  0.772162  0.767017  ...  0.477740   \n",
       "5            0.5  0.813575  0.765834  0.746716  0.772337  ...  0.424332   \n",
       "6            0.5  0.728054  0.809135  0.705452  0.698926  ...  0.496126   \n",
       "7            1.0  0.737113  0.778694  0.806507  0.799577  ...  0.416611   \n",
       "8            0.5  0.698434  0.753477  0.739042  0.831681  ...  0.496911   \n",
       "\n",
       "   joint_09  joint_10  joint_11  joint_12  joint_26  joint_27  joint_28  \\\n",
       "0  0.753815  0.272106  0.269510  0.762947  0.014214  0.011376  0.018978   \n",
       "1  0.761224  0.217448  0.245846  0.727910  0.010748  0.000000  0.009473   \n",
       "2  0.725601  0.207995  0.258133  0.760757  0.013097  0.006830  0.017065   \n",
       "3  0.751688  0.238584  0.250324  0.767434  0.009505  0.006274  0.020264   \n",
       "4  0.749873  0.221475  0.290464  0.772967  0.004216  0.002132  0.023389   \n",
       "5  0.758200  0.283672  0.281967  0.803481  0.004861  0.005427  0.023442   \n",
       "6  0.748269  0.216302  0.288977  0.741954  0.005143  0.005407  0.022523   \n",
       "7  0.714467  0.231869  0.281431  0.804961  0.012911  0.004546  0.025178   \n",
       "8  0.707258  0.222234  0.291893  0.770717  0.016622  0.007172  0.006115   \n",
       "\n",
       "   joint_29  label  \n",
       "0  0.020291      0  \n",
       "1  0.010006      0  \n",
       "2  0.016856      0  \n",
       "3  0.017981      0  \n",
       "4  0.018477      0  \n",
       "5  0.017338      0  \n",
       "6  0.013901      0  \n",
       "7  0.011477      0  \n",
       "8  0.011130      0  \n",
       "\n",
       "[9 rows x 24 columns]"
      ]
     },
     "execution_count": 922,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhanced Model Submission Section\n",
    "Modified version of the submission code to use the enhanced model with embeddings instead of the original RNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Model input size derived from enhanced features: {enhanced_input_size}\")\n",
    "\n",
    "# # Create enhanced model instance for submission (using the trained demo_enhanced_model)\n",
    "# enhanced_model_submission = demo_enhanced_model  # Use the already trained enhanced model\n",
    "\n",
    "# print(f\"Enhanced Model created with input_size={enhanced_input_size}, hidden_size={HIDDEN_SIZE}\")\n",
    "# # Model is already trained and loaded, so we just set to evaluation mode\n",
    "# enhanced_model_submission.eval()\n",
    "\n",
    "# print(\"Starting inference on actual test set using Enhanced Model...\")\n",
    "\n",
    "# # --- Enhanced Model Inference Pipeline ---\n",
    "# final_test_preds = []\n",
    "# final_test_probabilities = []\n",
    "# sample_indices = []\n",
    "\n",
    "# print(f\"Running enhanced inference on {len(test_enhanced_loader)} batches...\")\n",
    "\n",
    "# with torch.no_grad():  # Disable gradient computation for inference\n",
    "#     for batch_idx, (continuous_batch, categorical_batch, _) in enumerate(test_enhanced_loader):\n",
    "#         # Move batches to device\n",
    "#         continuous_batch = continuous_batch.to(device)\n",
    "#         for key in categorical_batch:\n",
    "#             categorical_batch[key] = categorical_batch[key].to(device)\n",
    "\n",
    "#         # Verify batch dimensions\n",
    "#         if batch_idx == 0:\n",
    "#             print(f\"Continuous batch shape: {continuous_batch.shape}\")\n",
    "#             print(f\"Categorical batch keys: {list(categorical_batch.keys())}\")\n",
    "\n",
    "#         # Get enhanced model predictions\n",
    "#         logits = enhanced_model_submission(continuous_batch, categorical_batch)\n",
    "#         preds = logits.argmax(dim=1).cpu().numpy()\n",
    "\n",
    "#         # Get probabilities for confidence analysis\n",
    "#         probabilities = torch.softmax(logits, dim=1).cpu().numpy()\n",
    "\n",
    "#         # Store batch results\n",
    "#         final_test_preds.append(preds)\n",
    "#         final_test_probabilities.append(probabilities)\n",
    "\n",
    "#         # Progress indicator\n",
    "#         if (batch_idx + 1) % 10 == 0 or (batch_idx + 1) == len(test_enhanced_loader):\n",
    "#             print(f\"Processed batch {batch_idx + 1}/{len(test_enhanced_loader)}\")\n",
    "\n",
    "\n",
    "# # Combine all batches into single arrays\n",
    "# final_test_preds = np.concatenate(final_test_preds)\n",
    "# final_test_probabilities = np.concatenate(final_test_probabilities)\n",
    "\n",
    "# print(f\"\\nInference on actual test set completed successfully!\")\n",
    "# print(f\"Total predictions: {len(final_test_preds)}\")\n",
    "# print(f\"Predictions shape: {final_test_preds.shape}\")\n",
    "# print(f\"Probabilities shape: {final_test_probabilities.shape}\")\n",
    "\n",
    "\n",
    "# # --- Create Enhanced Model Submission File ---\n",
    "# # Map numerical predictions back to original labels\n",
    "# label_map = {0: \"no_pain\", 1: \"low_pain\", 2: \"high_pain\"}\n",
    "# pred_labels = [label_map[p] for p in final_test_preds]\n",
    "\n",
    "# final_results = [] # Initialize final_results list\n",
    "\n",
    "# for sample_id in X_test_final_df['sample_index'].unique():\n",
    "#     # extract rows for this sample\n",
    "#     temp = X_test_final_df[X_test_final_df['sample_index'] == sample_id]\n",
    "\n",
    "#     # If the sample has fewer rows than WINDOW_SIZE, pad it with zeros\n",
    "#     if len(temp) < WINDOW_SIZE:\n",
    "#         padding = pd.DataFrame(0, index=np.arange(WINDOW_SIZE - len(temp)), columns=temp.columns)\n",
    "#         temp = pd.concat([temp, padding], ignore_index=True)\n",
    "\n",
    "#     # For enhanced model, we need to separate continuous and categorical features\n",
    "#     continuous_cols = [col for col in temp.columns if col not in pain_survey_cols and col != 'sample_index']\n",
    "#     temp_cont = temp[continuous_cols].values\n",
    "#     temp_cat = {}\n",
    "#     for col in pain_survey_cols:\n",
    "#         if col in temp.columns:\n",
    "#             temp_cat[col] = temp[col].values\n",
    "\n",
    "#     # build sequences for this sample (continuous part)\n",
    "#     seqs_cont = build_sequences_test(temp_cont, window=WINDOW_SIZE, stride=STRIDE)\n",
    "\n",
    "#     # build sequences for categorical features\n",
    "#     seqs_cat = {}\n",
    "#     for col in pain_survey_cols:\n",
    "#         if col in temp_cat:\n",
    "#             seqs_cat[col] = build_sequences_test(temp_cat[col].reshape(-1, 1), window=WINDOW_SIZE, stride=STRIDE).squeeze(-1)\n",
    "\n",
    "#     # sometimes build_sequences_test might still return zero sequences if len(temp) < stride\n",
    "#     if len(seqs_cont) == 0:\n",
    "#         # fallback: take the last WINDOW_SIZE rows\n",
    "#         seqs_cont = temp_cont[-WINDOW_SIZE:][np.newaxis, :, :]\n",
    "#         for col in pain_survey_cols:\n",
    "#             if col in temp_cat:\n",
    "#                 seqs_cat[col] = temp_cat[col][-WINDOW_SIZE:][np.newaxis, :]\n",
    "\n",
    "#     # Convert to tensors\n",
    "#     seqs_cont = torch.tensor(seqs_cont, dtype=torch.float32).to(device)\n",
    "#     for col in seqs_cat:\n",
    "#         seqs_cat[col] = torch.tensor(seqs_cat[col], dtype=torch.long).to(device)\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         logits = enhanced_model_submission(seqs_cont, seqs_cat)\n",
    "#         probs = torch.softmax(logits, dim=1).cpu().numpy()\n",
    "#         preds = np.argmax(probs, axis=1)\n",
    "\n",
    "#     # Final decision for this sample_index\n",
    "#     final_class = np.bincount(preds).argmax()  # majority vote\n",
    "#     final_results.append({\n",
    "#         \"sample_index\": sample_id,\n",
    "#         \"prediction\": label_map[final_class]\n",
    "#     })\n",
    "\n",
    "# # Create enhanced submission filename\n",
    "# enhanced_submission_filename = f\"enhanced_{SUBMISSION_FILENAME}\"\n",
    "# submission = pd.DataFrame(final_results)\n",
    "# submission.to_csv(enhanced_submission_filename, index=False)\n",
    "\n",
    "# print(submission.head())\n",
    "# print(f\"‚úÖ Saved enhanced submission with {len(submission)} rows should be 1324\")\n",
    "\n",
    "# # --- Analyze class distribution in final predictions ---\n",
    "# label_counts = submission['prediction'].value_counts(normalize=True) * 100\n",
    "\n",
    "# print(\"\\nüìä Distribution of predicted classes (Enhanced Model):\")\n",
    "# for label in ['no_pain', 'low_pain', 'high_pain']:\n",
    "#     pct = label_counts.get(label, 0.0)\n",
    "#     print(f\"   {label:10s}: {pct:6.2f}%\")\n",
    "\n",
    "# # Optional: quick sanity check for imbalance\n",
    "# majority_label = label_counts.idxmax()\n",
    "# print(f\"\\nüß≠ Most common predicted label: {majority_label} ({label_counts.max():.2f}%) \\n\")\n",
    "\n",
    "# # Optional: visualize as a bar chart\n",
    "# import matplotlib.pyplot as plt\n",
    "# desired_order = [\"no_pain\", \"low_pain\", \"high_pain\"]\n",
    "# label_counts_ordered = label_counts.reindex(desired_order)\n",
    "# plt.figure(figsize=(5,3))\n",
    "# (label_counts_ordered\n",
    "#  .plot(kind='bar', rot=0, title='Enhanced Model Predicted Class Distribution', ylabel='Percentage (%)'))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # === Enhanced Model Test Data Processing ===\n",
    "# # Note: We use the existing test_enhanced_loader that was created with proper preprocessing\n",
    "# # This loader already handles:\n",
    "# # 1. Categorical features (pain_survey_1, pain_survey_2, pain_survey_3, pain_survey_4) \n",
    "# # 2. Continuous features (joint features)\n",
    "# # 3. Proper normalization and sequence building\n",
    "\n",
    "# print(\"üîç Enhanced Model Test Data Info:\")\n",
    "# print(f\"üìä Enhanced input size: {enhanced_input_size}\")\n",
    "# print(f\"üîÑ Test enhanced loader batches: {len(test_enhanced_loader)}\")\n",
    "# print(f\"üìã Categorical features: {pain_survey_cols}\")\n",
    "\n",
    "# # Verify the enhanced loader structure\n",
    "# sample_batch = next(iter(test_enhanced_loader))\n",
    "# continuous_batch, categorical_batch, _ = sample_batch\n",
    "\n",
    "# print(f\"\\nüìê Batch Shapes:\")\n",
    "# print(f\"   Continuous: {continuous_batch.shape}\")\n",
    "# print(f\"   Categorical keys: {list(categorical_batch.keys())}\")\n",
    "# for key, tensor in categorical_batch.items():\n",
    "#     print(f\"   {key}: {tensor.shape}\")\n",
    "\n",
    "# # Set model to evaluation mode\n",
    "# demo_enhanced_model.eval()\n",
    "\n",
    "# print(f\"\\nüöÄ Model ready for inference using enhanced data pipeline!\")\n",
    "# print(f\"üéØ Will generate submission file: enhanced_{SUBMISSION_FILENAME}\")\n",
    "\n",
    "# # === Load Enhanced Model Weights ===\n",
    "# # Check if we need to load saved model weights\n",
    "# try:\n",
    "#     # You can specify your enhanced model path here if you have a saved model\n",
    "#     # model_path = \"models/your_enhanced_model.pt\"  \n",
    "#     # checkpoint = torch.load(model_path, map_location=device)\n",
    "#     # demo_enhanced_model.load_state_dict(checkpoint)\n",
    "#     # print(f\"‚úì Enhanced model loaded from {model_path}\")\n",
    "    \n",
    "#     # For now, using the current demo_enhanced_model that should already be trained\n",
    "#     total_params = sum(p.numel() for p in demo_enhanced_model.parameters())\n",
    "#     print(f\"‚úì Enhanced model ready with {total_params:,} parameters\")\n",
    "    \n",
    "# except Exception as e:\n",
    "#     print(f\"‚ÑπÔ∏è  Using current demo_enhanced_model (should be already trained)\")\n",
    "#     print(f\"   Model parameters: {sum(p.numel() for p in demo_enhanced_model.parameters()):,}\")\n",
    "\n",
    "# # === Enhanced Model Inference ===\n",
    "# print(\"\\nüöÄ Starting Enhanced Model Inference...\")\n",
    "\n",
    "# final_test_preds = []\n",
    "# final_test_probabilities = []\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for batch_idx, (continuous_batch, categorical_batch, _) in enumerate(test_enhanced_loader):\n",
    "#         # Move data to device\n",
    "#         continuous_batch = continuous_batch.to(device)\n",
    "#         for key in categorical_batch:\n",
    "#             categorical_batch[key] = categorical_batch[key].to(device)\n",
    "        \n",
    "#         # Forward pass with enhanced model\n",
    "#         logits = demo_enhanced_model(continuous_batch, categorical_batch)\n",
    "#         preds = logits.argmax(dim=1).cpu().numpy()\n",
    "#         probabilities = torch.softmax(logits, dim=1).cpu().numpy()\n",
    "        \n",
    "#         # Store results\n",
    "#         final_test_preds.append(preds)\n",
    "#         final_test_probabilities.append(probabilities)\n",
    "        \n",
    "#         # Progress indicator\n",
    "#         if (batch_idx + 1) % 10 == 0 or (batch_idx + 1) == len(test_enhanced_loader):\n",
    "#             print(f\"   Processed batch {batch_idx + 1}/{len(test_enhanced_loader)}\")\n",
    "\n",
    "# # Combine results\n",
    "# final_test_preds = np.concatenate(final_test_preds)\n",
    "# final_test_probabilities = np.concatenate(final_test_probabilities)\n",
    "\n",
    "# print(f\"\\n‚úÖ Inference completed!\")\n",
    "# print(f\"üìä Total predictions: {len(final_test_preds)}\")\n",
    "# print(f\"üìà Predictions shape: {final_test_preds.shape}\")\n",
    "# print(f\"üé≤ Probabilities shape: {final_test_probabilities.shape}\")\n",
    "\n",
    "# # === Create Enhanced Submission File ===\n",
    "# label_map = {0: \"no_pain\", 1: \"low_pain\", 2: \"high_pain\"}\n",
    "\n",
    "# # Create submission data - the enhanced loader handles sequences automatically\n",
    "# submission_data = []\n",
    "# for i, prediction in enumerate(final_test_preds):\n",
    "#     submission_data.append({\n",
    "#         'sample_index': i,\n",
    "#         'pain_level': int(prediction)\n",
    "#     })\n",
    "\n",
    "# submission_df = pd.DataFrame(submission_data)\n",
    "\n",
    "# # Save enhanced submission\n",
    "# enhanced_submission_filename = f\"enhanced_{SUBMISSION_FILENAME}\"\n",
    "# submission_df.to_csv(enhanced_submission_filename, index=False)\n",
    "\n",
    "# print(f\"\\nüíæ Enhanced submission saved: {enhanced_submission_filename}\")\n",
    "# print(f\"üìä Submission shape: {submission_df.shape}\")\n",
    "\n",
    "# # Display sample and distribution\n",
    "# print(f\"\\nüìã Sample submission data:\")\n",
    "# print(submission_df.head(10))\n",
    "\n",
    "# print(f\"\\nüìà Prediction distribution:\")\n",
    "# distribution = submission_df['pain_level'].value_counts().sort_index()\n",
    "# for level, count in distribution.items():\n",
    "#     label = label_map[level]\n",
    "#     percentage = (count / len(submission_df)) * 100\n",
    "#     print(f\"   {label}: {count} ({percentage:.2f}%)\")\n",
    "\n",
    "# print(f\"\\nüéØ Enhanced model submission ready for competition!\")\n",
    "# print(f\"üìÅ File: {enhanced_submission_filename}\")\n",
    "# print(f\"üìä Total samples: {len(submission_df)}\")\n",
    "\n",
    "# # Optional: Comparison with expected format\n",
    "# try:\n",
    "#     sample_submission = pd.read_csv('an2dl2526c1/sample_submission.csv')\n",
    "#     print(f\"\\nüîç Format validation:\")\n",
    "#     print(f\"   Expected samples: {len(sample_submission)}\")\n",
    "#     print(f\"   Our samples: {len(submission_df)}\")\n",
    "#     print(f\"   Expected columns: {list(sample_submission.columns)}\")\n",
    "#     print(f\"   Our columns: {list(submission_df.columns)}\")\n",
    "    \n",
    "#     if len(submission_df) == len(sample_submission):\n",
    "#         print(\"   ‚úÖ Sample count matches!\")\n",
    "#     else:\n",
    "#         print(\"   ‚ö†Ô∏è  Sample count mismatch!\")\n",
    "        \n",
    "# except:\n",
    "#     print(\"   ‚ÑπÔ∏è  Could not load sample_submission.csv for validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Competition Submission - Enhanced Model\n",
    "\n",
    "This section creates the final submission CSV using the trained enhanced model with the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÅ Starting Competition Submission Process\n",
      "============================================================\n",
      "üíª Device: cuda\n",
      "üìÇ Loading model from: models/enhanced_lstm_embeddings_F10.90_20251113_180601.pt\n",
      "üìã Using saved model configuration: {'hidden_size': 42, 'num_layers': 2, 'rnn_type': 'LSTM', 'bidirectional': True, 'dropout_rate': 0.2, 'continuous_input_size': 17, 'categorical_features': ['pain_survey_1', 'pain_survey_2', 'pain_survey_3', 'pain_survey_4'], 'embedding_dims': {'pain_survey_1': 2, 'pain_survey_2': 2, 'pain_survey_3': 2, 'pain_survey_4': 2}}\n",
      "‚úÖ Using exact checkpoint configuration:\n",
      "   Categorical feature config: {'pain_survey_1': 3, 'pain_survey_2': 3, 'pain_survey_3': 3, 'pain_survey_4': 3} (will create embeddings of size [4, 4, 4, 4])\n",
      "   Embedding dims: {'pain_survey_1': 2, 'pain_survey_2': 2, 'pain_survey_3': 2, 'pain_survey_4': 2}\n",
      "   Hidden size: 42\n",
      "Created embedding for pain_survey_1: 3 -> 2\n",
      "Created embedding for pain_survey_2: 3 -> 2\n",
      "Created embedding for pain_survey_3: 3 -> 2\n",
      "Created embedding for pain_survey_4: 3 -> 2\n",
      "Total RNN input size: 17 (continuous) + 8 (embeddings) = 25\n",
      "‚úÖ Model loaded successfully!\n",
      "üìä Best F1 Score: 0.9008\n",
      "üéØ Model ready for inference\n",
      "üè∑Ô∏è Label mapping: {0: 'no_pain', 1: 'low_pain', 2: 'high_pain'}\n",
      "üìà Model parameters: 66,479\n"
     ]
    }
   ],
   "source": [
    "# Competition Submission - Load Enhanced Model and Generate Predictions\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"üèÅ Starting Competition Submission Process\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üíª Device: {device}\")\n",
    "\n",
    "# Load the saved enhanced model\n",
    "model_paths = [\n",
    "    \"models/enhanced_lstm_embeddings_F10.90_20251113_180601.pt\"   # F1 0.91\n",
    "]\n",
    "\n",
    "# Find the best model file\n",
    "best_model_path = None\n",
    "for path in model_paths:\n",
    "    if os.path.exists(path):\n",
    "        best_model_path = path\n",
    "        break\n",
    "\n",
    "if not best_model_path:\n",
    "    print(\"‚ùå No saved model found! Using current demo_enhanced_model instead.\")\n",
    "    submission_model = demo_enhanced_model\n",
    "else:\n",
    "    print(f\"üìÇ Loading model from: {best_model_path}\")\n",
    "    \n",
    "    # Load model checkpoint\n",
    "    checkpoint = torch.load(best_model_path, map_location=device)\n",
    "    \n",
    "    # Get the correct configuration from the saved checkpoint\n",
    "    saved_config = checkpoint['training_info']['model_config']\n",
    "    print(f\"üìã Using saved model configuration: {saved_config}\")\n",
    "    \n",
    "    # Use exact configuration to match the saved model (based on error analysis)\n",
    "    # The saved model embedding expects 4 slots, so we need num_values=3 (since embedding uses num_values+1)\n",
    "    categorical_feature_config = {'pain_survey_1': 3, 'pain_survey_2': 3, 'pain_survey_3': 3, 'pain_survey_4': 3}\n",
    "    embedding_dims = {'pain_survey_1': 2, 'pain_survey_2': 2, 'pain_survey_3': 2, 'pain_survey_4': 2}\n",
    "    \n",
    "    print(f\"‚úÖ Using exact checkpoint configuration:\")\n",
    "    print(f\"   Categorical feature config: {categorical_feature_config} (will create embeddings of size {[v+1 for v in categorical_feature_config.values()]})\")\n",
    "    print(f\"   Embedding dims: {embedding_dims}\")\n",
    "    print(f\"   Hidden size: {saved_config.get('hidden_size', 42)}\")\n",
    "    \n",
    "    submission_model = EnhancedRecurrentClassifier(\n",
    "        continuous_input_size=17,  # Always 17 joint features\n",
    "        categorical_features=categorical_feature_config,\n",
    "        embedding_dims=embedding_dims,\n",
    "        hidden_size=saved_config.get('hidden_size', 42),  # Use saved hidden size\n",
    "        num_layers=2,\n",
    "        num_classes=3,\n",
    "        rnn_type='LSTM',\n",
    "        bidirectional=True,\n",
    "        dropout_rate=0.2\n",
    "    ).to(device)\n",
    "    \n",
    "    # Load the trained weights\n",
    "    submission_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"‚úÖ Model loaded successfully!\")\n",
    "    print(f\"üìä Best F1 Score: {checkpoint['training_info']['best_val_f1']:.4f}\")\n",
    "\n",
    "# Set model to evaluation mode\n",
    "submission_model.eval()\n",
    "print(f\"üéØ Model ready for inference\")\n",
    "\n",
    "# Label mapping for predictions\n",
    "label_map = {0: 'no_pain', 1: 'low_pain', 2: 'high_pain'}\n",
    "print(f\"üè∑Ô∏è Label mapping: {label_map}\")\n",
    "\n",
    "print(f\"üìà Model parameters: {sum(p.numel() for p in submission_model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Starting Inference on Test Data\n",
      "==================================================\n",
      "üìä Using test_enhanced_loader with 1 batches\n",
      "üìã Total test samples: 48\n",
      "‚úÖ Inference completed!\n",
      "üìä Generated 48 predictions\n",
      "\n",
      "üìã Prediction Summary:\n",
      "    high_pain:     8 ( 16.67%)\n",
      "     low_pain:    24 ( 50.00%)\n",
      "      no_pain:    16 ( 33.33%)\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions using the prepared test dataset\n",
    "print(\"\\nüöÄ Starting Inference on Test Data\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check if test_enhanced_final_loader exists and use it, otherwise use test_enhanced_loader\n",
    "if 'test_enhanced_final_loader' in globals():\n",
    "    test_loader = test_enhanced_final_loader\n",
    "    sample_indices = test_sample_indices\n",
    "    print(f\"üìä Using test_enhanced_final_loader with {len(test_loader)} batches\")\n",
    "elif 'test_enhanced_loader' in globals():\n",
    "    test_loader = test_enhanced_loader\n",
    "    print(f\"üìä Using test_enhanced_loader with {len(test_loader)} batches\")\n",
    "    # Create sample indices for the regular test loader\n",
    "    sample_indices = list(range(len(test_loader.dataset)))\n",
    "else:\n",
    "    raise ValueError(\"‚ùå No test data loader found! Please run the data preparation cells first.\")\n",
    "\n",
    "print(f\"üìã Total test samples: {len(test_loader.dataset)}\")\n",
    "\n",
    "# Perform inference\n",
    "all_predictions = []\n",
    "all_probabilities = []\n",
    "all_sample_indices = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (continuous_batch, categorical_batch, labels_batch) in enumerate(test_loader):\n",
    "        # Move data to device\n",
    "        continuous_batch = continuous_batch.to(device)\n",
    "        categorical_batch = {k: v.to(device) for k, v in categorical_batch.items()}\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = submission_model(continuous_batch, categorical_batch)\n",
    "        probabilities = torch.softmax(logits, dim=1)\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "        \n",
    "        # Store results\n",
    "        all_predictions.extend(predictions.cpu().numpy())\n",
    "        all_probabilities.extend(probabilities.cpu().numpy())\n",
    "        \n",
    "        # Calculate corresponding sample indices for this batch\n",
    "        batch_start = batch_idx * test_loader.batch_size\n",
    "        batch_end = min(batch_start + len(predictions), len(sample_indices) if 'test_sample_indices' in globals() else len(test_loader.dataset))\n",
    "        \n",
    "        if 'test_sample_indices' in globals():\n",
    "            batch_sample_indices = test_sample_indices[batch_start:batch_end]\n",
    "        else:\n",
    "            batch_sample_indices = list(range(batch_start, batch_end))\n",
    "            \n",
    "        all_sample_indices.extend(batch_sample_indices)\n",
    "        \n",
    "        if (batch_idx + 1) % 50 == 0:\n",
    "            print(f\"   Processed {batch_idx + 1}/{len(test_loader)} batches\")\n",
    "\n",
    "print(f\"‚úÖ Inference completed!\")\n",
    "print(f\"üìä Generated {len(all_predictions)} predictions\")\n",
    "\n",
    "# Convert predictions to label strings\n",
    "predicted_labels = [label_map[pred] for pred in all_predictions]\n",
    "\n",
    "print(f\"\\nüìã Prediction Summary:\")\n",
    "unique, counts = np.unique(predicted_labels, return_counts=True)\n",
    "for label, count in zip(unique, counts):\n",
    "    percentage = (count / len(predicted_labels)) * 100\n",
    "    print(f\"   {label:>10}: {count:>5} ({percentage:>6.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù Creating Submission File\n",
      "========================================\n",
      "‚ö†Ô∏è  Using sequential indices as fallback\n",
      "üìä Submission Shape: (1324, 2)\n",
      "üìã Sample indices range: 000 to 1323\n",
      "üè∑Ô∏è Unique labels: ['high_pain', 'low_pain', 'no_pain']\n",
      "\n",
      "üìÑ First 10 rows of submission:\n",
      "  sample_index      label\n",
      "0          000    no_pain\n",
      "1          001    no_pain\n",
      "2          002    no_pain\n",
      "3          003    no_pain\n",
      "4          004    no_pain\n",
      "5          005  high_pain\n",
      "6          006    no_pain\n",
      "7          007    no_pain\n",
      "8          008    no_pain\n",
      "9          009    no_pain\n",
      "\n",
      "üíæ Saving submission to: submission_enhanced_F10.901_20251113_184911.csv\n",
      "‚ùå Error: Failed to save submission file!\n",
      "\n",
      "üéâ Submission process completed!\n",
      "üìÅ File: submission_enhanced_F10.901_20251113_184911.csv\n",
      "üìä Predictions: 1324 samples\n",
      "üè∑Ô∏è Classes: ['high_pain', 'low_pain', 'no_pain']\n"
     ]
    }
   ],
   "source": [
    "# Create submission DataFrame following the sample format\n",
    "print(\"\\nüìù Creating Submission File\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Handle sample indices formatting (ensure 3-digit zero-padded format)\n",
    "if 'test_sample_indices' in globals() and len(test_sample_indices) > 0:\n",
    "    # Use the actual sample indices from the test data processing\n",
    "    formatted_indices = []\n",
    "    for idx in all_sample_indices:\n",
    "        if isinstance(idx, (int, np.integer)):\n",
    "            formatted_indices.append(f\"{idx:03d}\")\n",
    "        else:\n",
    "            # Try to extract numeric part if it's a string\n",
    "            try:\n",
    "                numeric_idx = int(str(idx).replace('sample_', '').replace('.0', ''))\n",
    "                formatted_indices.append(f\"{numeric_idx:03d}\")\n",
    "            except:\n",
    "                formatted_indices.append(f\"{idx:03d}\")\n",
    "else:\n",
    "    # Fallback: create indices 000 to (num_predictions-1)\n",
    "    print(\"‚ö†Ô∏è  Using sequential indices as fallback\")\n",
    "    formatted_indices = [f\"{i:03d}\" for i in range(len(all_predictions))]\n",
    "\n",
    "# Create submission DataFrame\n",
    "submission_df = pd.DataFrame({\n",
    "    'sample_index': formatted_indices,\n",
    "    'label': predicted_labels\n",
    "})\n",
    "\n",
    "# Verify the submission format matches the sample\n",
    "print(f\"üìä Submission Shape: {submission_df.shape}\")\n",
    "print(f\"üìã Sample indices range: {submission_df['sample_index'].iloc[0]} to {submission_df['sample_index'].iloc[-1]}\")\n",
    "print(f\"üè∑Ô∏è Unique labels: {sorted(submission_df['label'].unique())}\")\n",
    "\n",
    "# Show first few rows\n",
    "print(f\"\\nüìÑ First 10 rows of submission:\")\n",
    "print(submission_df.head(10))\n",
    "\n",
    "# Generate filename with timestamp and model info\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "if best_model_path:\n",
    "    model_f1 = checkpoint['training_info']['best_val_f1']\n",
    "    submission_filename = f\"submission_enhanced_F1{model_f1:.3f}_{timestamp}.csv\"\n",
    "else:\n",
    "    submission_filename = f\"submission_enhanced_current_{timestamp}.csv\"\n",
    "\n",
    "print(f\"\\nüíæ Saving submission to: {submission_filename}\")\n",
    "\n",
    "# Save the submission file\n",
    "# submission_df.to_csv(submission_filename, index=False)\n",
    "\n",
    "if os.path.exists(submission_filename):\n",
    "    #file_size = os.path.getsize(submission_filename) / 1024  # KB\n",
    "    #print(f\"‚úÖ Submission saved successfully! ({file_size:.1f} KB)\")\n",
    "    \n",
    "    # Quick validation check\n",
    "    validation_df = pd.read_csv(submission_filename)\n",
    "    print(f\"‚úÖ Validation: Read back {len(validation_df)} rows\")\n",
    "    \n",
    "    # Check if we have the expected number of test samples (should be 1324 based on sample_submission.csv)\n",
    "    expected_samples = 1324\n",
    "    if len(validation_df) == expected_samples:\n",
    "        print(f\"‚úÖ Perfect! Submission has exactly {expected_samples} samples as expected\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  Warning: Submission has {len(validation_df)} samples, expected {expected_samples}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Error: Failed to save submission file!\")\n",
    "\n",
    "print(f\"\\nüéâ Submission process completed!\")\n",
    "print(f\"üìÅ File: {submission_filename}\")\n",
    "print(f\"üìä Predictions: {len(submission_df)} samples\")\n",
    "print(f\"üè∑Ô∏è Classes: {sorted(submission_df['label'].unique())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Loading Complete Test Data for Submission\n",
      "============================================================\n",
      "üìä X_test_final_df shape: (211840, 23)\n",
      "üìã Unique samples in X_test_final_df: 1324\n",
      "‚úÖ Test data has correct number of samples (1324)\n",
      "üîÑ Rebuilding test sequences for all 1324 samples...\n",
      "Building ENHANCED sequences for actual test dataset with WINDOW_SIZE=50, STRIDE=10\n",
      "Available columns: 23 columns\n",
      "Categorical features: ['pain_survey_1', 'pain_survey_2', 'pain_survey_3', 'pain_survey_4']\n",
      "Continuous features: 17 joint features\n",
      "   Processing 1324 unique samples...\n",
      "‚úÖ Generated sequences for 1324 samples\n",
      "   Continuous shape: (1324, 50, 17)\n",
      "   Sample indices range: 0.0 to 1323.0\n",
      "üéØ Ready to generate predictions for all 1324 samples!\n"
     ]
    }
   ],
   "source": [
    "# Fix: Load and process the complete test data for submission\n",
    "print(\"üîß Loading Complete Test Data for Submission\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check if the test CSV data is properly loaded\n",
    "if 'X_test_final_df' in globals():\n",
    "    print(f\"üìä X_test_final_df shape: {X_test_final_df.shape}\")\n",
    "    print(f\"üìã Unique samples in X_test_final_df: {X_test_final_df['sample_index'].nunique()}\")\n",
    "    \n",
    "    # If we have the right data but wrong processing, let's reprocess it\n",
    "    if X_test_final_df['sample_index'].nunique() == 1324:\n",
    "        print(\"‚úÖ Test data has correct number of samples (1324)\")\n",
    "        \n",
    "        # Rebuild test sequences properly \n",
    "        print(\"üîÑ Rebuilding test sequences for all 1324 samples...\")\n",
    "        \n",
    "        # Use the existing sequence building function from the notebook\n",
    "        print(f\"Building ENHANCED sequences for actual test dataset with WINDOW_SIZE={WINDOW_SIZE}, STRIDE={STRIDE}\")\n",
    "\n",
    "        # Identify feature types from X_test_final_df columns\n",
    "        all_columns = X_test_final_df.columns.tolist()\n",
    "        print(f\"Available columns: {len(all_columns)} columns\")\n",
    "\n",
    "        # Define feature separation\n",
    "        categorical_features = ['pain_survey_1', 'pain_survey_2', 'pain_survey_3', 'pain_survey_4']\n",
    "        joint_features = [col for col in all_columns if col.startswith('joint_')]\n",
    "        continuous_features = joint_features  # Continuous features are the joint features\n",
    "        exclude_cols = ['sample_index'] + ([] if 'time' not in all_columns else ['time'])  # Exclude non-feature columns\n",
    "\n",
    "        print(f\"Categorical features: {categorical_features}\")\n",
    "        print(f\"Continuous features: {len(continuous_features)} joint features\")\n",
    "\n",
    "        # Create enhanced sequences function for test data\n",
    "        def build_sequences_test_enhanced(df, window=WINDOW_SIZE, stride=STRIDE):\n",
    "            \"\"\"\n",
    "            Build sequences for test data separating continuous and categorical features\n",
    "            \"\"\"\n",
    "            continuous_dataset = []\n",
    "            categorical_datasets = {feature: [] for feature in categorical_features}\n",
    "            sample_indices = []\n",
    "            \n",
    "            # Get unique sample IDs\n",
    "            unique_samples = sorted(df['sample_index'].unique())\n",
    "            print(f\"   Processing {len(unique_samples)} unique samples...\")\n",
    "            \n",
    "            for sample_id in unique_samples:\n",
    "                # Extract rows for this sample\n",
    "                sample_data = df[df['sample_index'] == sample_id].copy()\n",
    "                \n",
    "                # If sample has fewer rows than WINDOW_SIZE, pad with zeros\n",
    "                if len(sample_data) < window:\n",
    "                    # Create padding dataframe\n",
    "                    padding_rows = window - len(sample_data)\n",
    "                    padding = pd.DataFrame(0, index=range(padding_rows), columns=sample_data.columns)\n",
    "                    sample_data = pd.concat([sample_data, padding], ignore_index=True)\n",
    "                \n",
    "                # Extract continuous features (joints only)\n",
    "                continuous_data = sample_data[continuous_features].values\n",
    "                \n",
    "                # Build continuous sequences\n",
    "                continuous_seqs = []\n",
    "                for i in range(0, len(continuous_data) - window + 1, stride):\n",
    "                    continuous_seqs.append(continuous_data[i:i + window])\n",
    "                \n",
    "                # If no sequences generated, take the last window\n",
    "                if len(continuous_seqs) == 0:\n",
    "                    continuous_seqs = [continuous_data[-window:]]\n",
    "                \n",
    "                # Build categorical sequences\n",
    "                categorical_seqs = {feature: [] for feature in categorical_features}\n",
    "                for feature in categorical_features:\n",
    "                    if feature in sample_data.columns:\n",
    "                        cat_data = sample_data[feature].values\n",
    "                        for i in range(0, len(cat_data) - window + 1, stride):\n",
    "                            categorical_seqs[feature].append(cat_data[i:i + window])\n",
    "                        # If no sequences generated, take the last window\n",
    "                        if len(categorical_seqs[feature]) == 0:\n",
    "                            categorical_seqs[feature] = [cat_data[-window:]]\n",
    "                    else:\n",
    "                        # If feature doesn't exist, create zero sequences\n",
    "                        for _ in continuous_seqs:\n",
    "                            categorical_seqs[feature].append(np.zeros(window))\n",
    "                \n",
    "                # Store ONE sequence per sample (use the first/last sequence)\n",
    "                continuous_dataset.append(continuous_seqs[-1])  # Take last sequence\n",
    "                for feature in categorical_features:\n",
    "                    categorical_datasets[feature].append(categorical_seqs[feature][-1])\n",
    "                sample_indices.append(sample_id)\n",
    "            \n",
    "            # Convert to numpy arrays\n",
    "            continuous_dataset = np.array(continuous_dataset, dtype='float32')\n",
    "            for feature in categorical_features:\n",
    "                categorical_datasets[feature] = np.array(categorical_datasets[feature], dtype='int64')\n",
    "            \n",
    "            return continuous_dataset, categorical_datasets, sample_indices\n",
    "\n",
    "        # Build enhanced sequences for all test samples\n",
    "        X_test_continuous_full, X_test_categorical_full, test_sample_indices_full = build_sequences_test_enhanced(X_test_final_df)\n",
    "\n",
    "        print(f\"‚úÖ Generated sequences for {len(test_sample_indices_full)} samples\")\n",
    "        print(f\"   Continuous shape: {X_test_continuous_full.shape}\")\n",
    "        print(f\"   Sample indices range: {min(test_sample_indices_full)} to {max(test_sample_indices_full)}\")\n",
    "        \n",
    "        # Handle NaN values\n",
    "        if np.isnan(X_test_continuous_full).any():\n",
    "            X_test_continuous_full = np.nan_to_num(X_test_continuous_full)\n",
    "            print(\"   ‚úì NaN values fixed in continuous features\")\n",
    "\n",
    "        for feature in categorical_features:\n",
    "            if np.isnan(X_test_categorical_full[feature]).any():\n",
    "                X_test_categorical_full[feature] = np.nan_to_num(X_test_categorical_full[feature]).astype('int64')\n",
    "                print(f\"   ‚úì NaN values fixed in {feature}\")\n",
    "        \n",
    "        # Update global variables\n",
    "        globals()['X_test_continuous_full'] = X_test_continuous_full\n",
    "        globals()['X_test_categorical_full'] = X_test_categorical_full  \n",
    "        globals()['test_sample_indices_full'] = test_sample_indices_full\n",
    "        \n",
    "        print(\"üéØ Ready to generate predictions for all 1324 samples!\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"‚ùå Test data has wrong number of samples: {X_test_final_df['sample_index'].nunique()}\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå X_test_final_df not found - need to load test data first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Generating Predictions for All Test Samples\n",
      "============================================================\n",
      "üìä Complete test dataset: 1324 samples\n",
      "üîÑ Data loader created: 3 batches\n",
      "‚úÖ Inference completed!\n",
      "üìä Generated 1324 predictions\n",
      "\n",
      "üìã Prediction Summary:\n",
      "    high_pain:   120 (  9.06%)\n",
      "     low_pain:   225 ( 16.99%)\n",
      "      no_pain:   979 ( 73.94%)\n",
      "\n",
      "üéØ Ready to create corrected submission file!\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions for all 1324 test samples\n",
    "print(\"üöÄ Generating Predictions for All Test Samples\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create dataset and loader for the complete test data\n",
    "dummy_labels_full = np.zeros(len(test_sample_indices_full), dtype='int64')\n",
    "test_enhanced_ds_full = EnhancedDataset(X_test_continuous_full, X_test_categorical_full, dummy_labels_full)\n",
    "\n",
    "print(f\"üìä Complete test dataset: {len(test_enhanced_ds_full)} samples\")\n",
    "\n",
    "# Create data loader for inference\n",
    "def collate_enhanced(batch):\n",
    "    \"\"\"Custom collate function for enhanced dataset\"\"\"\n",
    "    continuous_batch = torch.stack([item[0] for item in batch])\n",
    "    \n",
    "    categorical_batch = {}\n",
    "    if batch:  # Check if batch is not empty\n",
    "        # Get feature names from first item\n",
    "        feature_names = batch[0][1].keys()\n",
    "        for feature in feature_names:\n",
    "            categorical_batch[feature] = torch.stack([item[1][feature] for item in batch])\n",
    "    \n",
    "    labels_batch = torch.stack([item[2] for item in batch])\n",
    "    \n",
    "    return continuous_batch, categorical_batch, labels_batch\n",
    "\n",
    "test_loader_full = DataLoader(\n",
    "    test_enhanced_ds_full,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True if torch.cuda.is_available() else False,\n",
    "    collate_fn=collate_enhanced\n",
    ")\n",
    "\n",
    "print(f\"üîÑ Data loader created: {len(test_loader_full)} batches\")\n",
    "\n",
    "# Perform inference on all samples\n",
    "submission_model.eval()\n",
    "all_predictions_full = []\n",
    "all_probabilities_full = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (continuous_batch, categorical_batch, labels_batch) in enumerate(test_loader_full):\n",
    "        # Move data to device\n",
    "        continuous_batch = continuous_batch.to(device)\n",
    "        categorical_batch = {k: v.to(device) for k, v in categorical_batch.items()}\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = submission_model(continuous_batch, categorical_batch)\n",
    "        probabilities = torch.softmax(logits, dim=1)\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "        \n",
    "        # Store results\n",
    "        all_predictions_full.extend(predictions.cpu().numpy())\n",
    "        all_probabilities_full.extend(probabilities.cpu().numpy())\n",
    "        \n",
    "        if (batch_idx + 1) % 20 == 0:\n",
    "            print(f\"   Processed {batch_idx + 1}/{len(test_loader_full)} batches\")\n",
    "\n",
    "print(f\"‚úÖ Inference completed!\")\n",
    "print(f\"üìä Generated {len(all_predictions_full)} predictions\")\n",
    "\n",
    "# Convert predictions to label strings\n",
    "predicted_labels_full = [label_map[pred] for pred in all_predictions_full]\n",
    "\n",
    "print(f\"\\nüìã Prediction Summary:\")\n",
    "unique_full, counts_full = np.unique(predicted_labels_full, return_counts=True)\n",
    "for label, count in zip(unique_full, counts_full):\n",
    "    percentage = (count / len(predicted_labels_full)) * 100\n",
    "    print(f\"   {label:>10}: {count:>5} ({percentage:>6.2f}%)\")\n",
    "\n",
    "# Update global variables for the submission creation\n",
    "globals()['all_predictions'] = all_predictions_full\n",
    "globals()['all_sample_indices'] = test_sample_indices_full  \n",
    "globals()['predicted_labels'] = predicted_labels_full\n",
    "\n",
    "print(f\"\\nüéØ Ready to create corrected submission file!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Creating Final Corrected Submission File\n",
      "============================================================\n",
      "üìä Submission Shape: (1324, 2)\n",
      "üìã Sample indices range: 000 to 1323\n",
      "üè∑Ô∏è Unique labels: ['high_pain', 'low_pain', 'no_pain']\n",
      "\n",
      "üìÑ First 10 rows of corrected submission:\n",
      "  sample_index      label\n",
      "0          000    no_pain\n",
      "1          001    no_pain\n",
      "2          002    no_pain\n",
      "3          003    no_pain\n",
      "4          004    no_pain\n",
      "5          005  high_pain\n",
      "6          006    no_pain\n",
      "7          007    no_pain\n",
      "8          008    no_pain\n",
      "9          009    no_pain\n",
      "\n",
      "üíæ Saving corrected submission to: submission_enhanced_CORRECTED_F10.901_20251113_184830.csv\n",
      "‚úÖ Submission saved successfully! (17.6 KB)\n",
      "‚úÖ Validation: Read back 1324 rows\n",
      "‚úÖ Perfect! Submission has exactly 1324 samples as expected\n",
      "\n",
      "üîç Comparison with Sample Submission:\n",
      "   Sample submission shape: (1324, 2)\n",
      "   Our submission shape: (1324, 2)\n",
      "   ‚úÖ Row counts match perfectly!\n",
      "   ‚úÖ Column names match perfectly!\n",
      "\n",
      "üìà Final Prediction Distribution:\n",
      "      no_pain:  73.94%\n",
      "     low_pain:  16.99%\n",
      "    high_pain:   9.06%\n",
      "\n",
      "üéâ CORRECTED SUBMISSION COMPLETED!\n",
      "üìÅ File: submission_enhanced_CORRECTED_F10.901_20251113_184830.csv\n",
      "üìä Samples: 1324 (Expected: 1324)\n",
      "üéØ Enhanced LSTM model with embeddings\n",
      "üìà Model performance: F1 = 0.9008\n",
      "üè∑Ô∏è Ready for competition submission!\n"
     ]
    }
   ],
   "source": [
    "# Create Final Corrected Submission File\n",
    "print(\"üìù Creating Final Corrected Submission File\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Format sample indices correctly (3-digit zero-padded)\n",
    "formatted_indices_corrected = []\n",
    "for idx in all_sample_indices:\n",
    "    if isinstance(idx, (int, np.integer, float, np.floating)):\n",
    "        formatted_indices_corrected.append(f\"{int(idx):03d}\")\n",
    "    else:\n",
    "        # Try to extract numeric part if it's a string\n",
    "        try:\n",
    "            numeric_idx = int(str(idx).replace('sample_', '').replace('.0', ''))\n",
    "            formatted_indices_corrected.append(f\"{numeric_idx:03d}\")\n",
    "        except:\n",
    "            formatted_indices_corrected.append(f\"{idx:03d}\")\n",
    "\n",
    "# Create submission DataFrame\n",
    "submission_df_corrected = pd.DataFrame({\n",
    "    'sample_index': formatted_indices_corrected,\n",
    "    'label': predicted_labels\n",
    "})\n",
    "\n",
    "# Verify the submission format\n",
    "print(f\"üìä Submission Shape: {submission_df_corrected.shape}\")\n",
    "print(f\"üìã Sample indices range: {submission_df_corrected['sample_index'].iloc[0]} to {submission_df_corrected['sample_index'].iloc[-1]}\")\n",
    "print(f\"üè∑Ô∏è Unique labels: {sorted(submission_df_corrected['label'].unique())}\")\n",
    "\n",
    "# Show first few rows\n",
    "print(f\"\\nüìÑ First 10 rows of corrected submission:\")\n",
    "print(submission_df_corrected.head(10))\n",
    "\n",
    "# Generate filename with timestamp and model info\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "model_f1 = checkpoint['training_info']['best_val_f1']\n",
    "submission_filename_corrected = f\"submission_enhanced_F1{model_f1:.3f}_{timestamp}.csv\"\n",
    "\n",
    "print(f\"\\nüíæ Saving corrected submission to: {submission_filename_corrected}\")\n",
    "\n",
    "# Save the submission file with error handling\n",
    "try:\n",
    "    submission_df_corrected.to_csv(submission_filename_corrected, index=False)\n",
    "    \n",
    "    # Verify file was created and check size\n",
    "    if os.path.exists(submission_filename_corrected):\n",
    "        file_size = os.path.getsize(submission_filename_corrected) / 1024  # KB\n",
    "        print(f\"‚úÖ Submission saved successfully! ({file_size:.1f} KB)\")\n",
    "        \n",
    "        # Quick validation check\n",
    "        validation_df = pd.read_csv(submission_filename_corrected)\n",
    "        print(f\"‚úÖ Validation: Read back {len(validation_df)} rows\")\n",
    "        \n",
    "        # Check if we have the expected number of test samples\n",
    "        expected_samples = 1324\n",
    "        if len(validation_df) == expected_samples:\n",
    "            print(f\"‚úÖ Perfect! Submission has exactly {expected_samples} samples as expected\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  Warning: Submission has {len(validation_df)} samples, expected {expected_samples}\")\n",
    "        \n",
    "        # Compare with sample submission if available\n",
    "        try:\n",
    "            sample_submission = pd.read_csv(\"an2dl2526c1/sample_submission.csv\")\n",
    "            print(f\"\\nüîç Comparison with Sample Submission:\")\n",
    "            print(f\"   Sample submission shape: {sample_submission.shape}\")\n",
    "            print(f\"   Our submission shape: {submission_df_corrected.shape}\")\n",
    "            \n",
    "            if len(sample_submission) == len(submission_df_corrected):\n",
    "                print(\"   ‚úÖ Row counts match perfectly!\")\n",
    "            \n",
    "            if list(sample_submission.columns) == list(submission_df_corrected.columns):\n",
    "                print(\"   ‚úÖ Column names match perfectly!\")\n",
    "                \n",
    "        except FileNotFoundError:\n",
    "            print(\"   ‚ÑπÔ∏è  Could not find sample_submission.csv for comparison\")\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ùå Error: Failed to create submission file!\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error saving file: {e}\")\n",
    "    \n",
    "    # Try alternative filename if there's a permission issue\n",
    "    alt_filename = f\"submission_corrected_{timestamp}.csv\"\n",
    "    print(f\"üîÑ Trying alternative filename: {alt_filename}\")\n",
    "    try:\n",
    "        submission_df_corrected.to_csv(alt_filename, index=False)\n",
    "        submission_filename_corrected = alt_filename\n",
    "        print(f\"‚úÖ Saved with alternative filename!\")\n",
    "    except Exception as e2:\n",
    "        print(f\"‚ùå Alternative save also failed: {e2}\")\n",
    "\n",
    "print(f\"\\nüìà Final Prediction Distribution:\")\n",
    "distribution = submission_df_corrected['label'].value_counts(normalize=True) * 100\n",
    "for label in ['no_pain', 'low_pain', 'high_pain']:\n",
    "    if label in distribution:\n",
    "        print(f\"   {label:>10}: {distribution[label]:>6.2f}%\")\n",
    "\n",
    "print(f\"\\nüéâ CORRECTED SUBMISSION COMPLETED!\")\n",
    "print(f\"üìÅ File: {submission_filename_corrected}\")\n",
    "print(f\"üìä Samples: {len(submission_df_corrected)} (Expected: 1324)\")\n",
    "print(f\"üéØ Enhanced LSTM model with embeddings\")\n",
    "print(f\"üìà Model performance: F1 = {model_f1:.4f}\")\n",
    "print(f\"üè∑Ô∏è Ready for competition submission!\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
