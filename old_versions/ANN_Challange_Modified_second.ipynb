{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jy5OeJu9o8Rt"
      },
      "source": [
        "# üè¥‚Äç‚ò†Ô∏è Pirate Pain Classification Challenge\n",
        "\n",
        "> ‚öì *\"Even pirates feel pain ‚Äî let's teach the model to feel it too.\"*\n",
        "\n",
        "---\n",
        "\n",
        "## üìö Table of Contents\n",
        "0. [README](#readme)  \n",
        "1. [Setup & Configuration](#setup)  \n",
        "2. [Data Loading](#data-loading)  \n",
        "3. [Import Libraries](#import-libraries)  \n",
        "4. [Data Preprocessing](#data-preprocessing)  \n",
        "5. [Sequence Building](#sequence-building)  \n",
        "6. [DataLoaders](#dataloaders)  \n",
        "7. [Network Hyperparameters](#hyperparameters)\n",
        "8. [Model Architecture](#model-architecture)  \n",
        "9. [Training Functions](#training-functions)  \n",
        "10. [Model Training](#model-training)  \n",
        "11. [Evaluation & Metrics](#evaluation)  \n",
        "12. [Model Loading & Final Testing](#model-loading)  \n",
        "13. [Competition Submission](#submission)\n",
        "\n",
        "---\n",
        "\n",
        "### ‚öôÔ∏è Quick Configuration Map\n",
        "\n",
        "> üß≠ *\"If ye seek to tweak the code, here be where to look!\"*\n",
        "\n",
        "- üß∫ **Batch Size:** ‚Üí [DataLoaders](#dataloaders)  \n",
        "- ‚öóÔ∏è **Hyperparameters:** ‚Üí [Network Hyperparameters](#hyperparameters)  \n",
        "- ü™û **Window Size & Stride:** ‚Üí [Sequence Building](#sequence-building)  \n",
        "- ‚öôÔ∏è **Model Type:** ‚Üí [Setup & Configuration](#setup)  \n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### üí∞ Treasure Storage ‚Äî Models & Submissions\n",
        "> üè¥‚Äç‚ò†Ô∏è *\"A wise pirate always knows where his treasure be buried ‚Äî guard yer models and submissions well!\"*\n",
        "\n",
        "- üíæ **Model & Submission Save/Load Path:** ‚Üí [Setup & Configuration](#setup)  \n",
        "  - üóÇÔ∏è Models be saved in a **`models/`** folder with the name:\n",
        "    **`experiment_name_dd-mm-HH-MM.pt`** (day-month-hour-minute).\n",
        "  - üìú Submissions be saved in a **`submissions/`** folder with the filename format:  \n",
        "    **`experiment_name_dd-mm-HH-MM.csv`** .\n",
        "  - üî° All related model parameters are saved in **`models/`** folder with the  name **`experiment_name_dd-mm-HH-MM_config.json`** .\n",
        "\n",
        "  \n",
        "  *‚ùóThe experiment name is set as **`RnnType_Bi_dd-mm-HH-MM`** or **`RnnType_dd-mm-HH-MM`** depending on if it is bidirectional or not*\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4oU_xMZwJUrZ"
      },
      "source": [
        "<a id=\"readme\"></a>\n",
        "## 0. Info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mjy_NO-5HPw3"
      },
      "source": [
        "\n",
        "\n",
        "This section lists all the main parameters that can be modified to control data loading, model behavior, and training.\n",
        "\n",
        "---\n",
        "\n",
        "### üìÅ File Paths\n",
        "| Variable | Description | Default Value |\n",
        "|-----------|--------------|----------------|\n",
        "| `TRAIN_DATA_PATH` | Training features | `'pirate_pain_train.csv'` |\n",
        "| `TRAIN_LABELS_PATH` | Training labels | `'pirate_pain_train_labels.csv'` |\n",
        "| `TEST_DATA_PATH` | Test set for inference | `'pirate_pain_test.csv'` *(optional)* |\n",
        "| `MODEL_SAVE_PATH` | Output model file | `'pirate_model.pt'` |\n",
        "| `RESULTS_FILE` | CSV for predictions | `'results_<date-time>.csv'` |\n",
        "\n",
        "---\n",
        "\n",
        "### üß† Model & Architecture\n",
        "| Parameter | Description | Typical Values |\n",
        "|------------|--------------|----------------|\n",
        "| `model_type` | Choose model class | `'RNN'`, `'LSTM'`, `'GRU'`, `'ANN'` |\n",
        "| `input_size` | Number of features per time step | *auto-detected from data* |\n",
        "| `hidden_size` | Hidden layer size | `64`, `128`, `256` |\n",
        "| `num_layers` | Number of RNN layers | `1-4` |\n",
        "| `dropout` | Dropout probability | `0.2‚Äì0.5` |\n",
        "| `num_classes` | Output classes (pain levels) | *from label set* |\n",
        "\n",
        "---\n",
        "\n",
        "### üèãÔ∏è Training Hyperparameters\n",
        "| Parameter | Description | Default / Range |\n",
        "|------------|--------------|-----------------|\n",
        "| `batch_size` | Samples per batch | `512/2^n` |\n",
        "| `learning_rate` | Optimizer learning rate | `1e-3` |\n",
        "| `num_epochs` | Training iterations | `500` |\n",
        "| `optimizer` | Optimization algorithm | `'AdamW'` |\n",
        "| `criterion` | Loss function | `CrossEntropyLoss()` |\n",
        "| `seed` | Random seed for reproducibility | `42` |\n",
        "\n",
        "---\n",
        "\n",
        "### üì§ Inference\n",
        "| Parameter | Description |\n",
        "|------------|--------------|\n",
        "| `LOAD_MODEL_PATH` | Path to pretrained `.pt` model (optional) |\n",
        "| `save_results` | Whether to write output CSV | `True` |\n",
        "\n",
        "---\n",
        "\n",
        "> üí° *Tip:* Adjust hyperparameters in the ‚ÄúConfiguration‚Äù or ‚ÄúTraining Setup‚Äù cell before running the notebook.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZLBQ6tJrcBB"
      },
      "source": [
        "<a id=\"setup\"></a>\n",
        "## 1. Setup & Configuration\n",
        "\n",
        "*Optional: Connect to Google Drive (for Colab users)*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nig16xZNnmnz",
        "outputId": "9b3d9cc8-db1c-45ab-cc34-9e220571ee32"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount(\"/gdrive\")\n",
        "# current_dir = \"/gdrive/MyDrive/pirate_dataset\"\n",
        "# %cd $current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZL1iYHipaeMD"
      },
      "source": [
        "*Set Model Type*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "uWaTlLkTKgk5"
      },
      "outputs": [],
      "source": [
        "RNN_TYPE = 'LSTM'            # 'RNN', 'LSTM', or 'GRU'\n",
        "BIDIRECTIONAL = True        # True / False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "up7Qo6v-o8Ru"
      },
      "source": [
        "*Set Model Save Name*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wkBnTJHuo8Rv",
        "outputId": "1b7faf0a-4593-4c10-d860-f32ac1551f37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment name: LSTM_bi_12-11-19-11\n",
            "Submission filename: LSTM_bi_12-11-19-11.csv\n",
            "Model save path: models/LSTM_bi_12-11-19-11_model.pt\n",
            "Model load path: models/LSTM_bi_12-11-19-11_model.pt\n"
          ]
        }
      ],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "# Get current date and time for submission filename\n",
        "current_datetime = datetime.now().strftime(\"%d-%m-%H-%M\")\n",
        "\n",
        "if BIDIRECTIONAL:\n",
        "    EXPERIMENT_NAME = f\"{RNN_TYPE}_bi_{current_datetime}\"\n",
        "else:\n",
        "    EXPERIMENT_NAME = f\"{RNN_TYPE}_{current_datetime}\"\n",
        "\n",
        "SUBMISSION_FILENAME = f\"{EXPERIMENT_NAME}.csv\"\n",
        "\n",
        "\n",
        "# Directory configuration\n",
        "logs_dir = \"tensorboard\"\n",
        "models_dir = \"models\"\n",
        "\n",
        "# Model save/load paths\n",
        "MODEL_SAVE_PATH = f\"{models_dir}/{EXPERIMENT_NAME}_model.pt\"\n",
        "MODEL_LOAD_PATH = f\"{models_dir}/{EXPERIMENT_NAME}_model.pt\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(f\"Experiment name: {EXPERIMENT_NAME}\")\n",
        "print(f\"Submission filename: {SUBMISSION_FILENAME}\")\n",
        "print(f\"Model save path: {MODEL_SAVE_PATH}\")\n",
        "print(f\"Model load path: {MODEL_LOAD_PATH}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdaKCgHVvvHX"
      },
      "source": [
        "<a id=\"data-loading\"></a>\n",
        "## 2. Data Loading\n",
        "\n",
        "Load training and test datasets from CSV files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "oLyI938Jvn-J"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "X_train = pd.read_csv('an2dl2526c1/pirate_pain_train.csv')\n",
        "y_train = pd.read_csv('an2dl2526c1/pirate_pain_train_labels.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3Lre5NWwCyk"
      },
      "source": [
        "<a id=\"import-libraries\"></a>\n",
        "## 3. Import Libraries\n",
        "\n",
        "Set random seeds for reproducibility and import all necessary packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nt3AnE8SwJg1",
        "outputId": "8eefb176-0fd5-408a-a66e-ee37fd16bb84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'pkill' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch version: 2.10.0.dev20251109+cu128\n",
            "Device: cuda\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "A subdirectory or file -p already exists.\n",
            "Error occurred while processing: -p.\n",
            "A subdirectory or file models already exists.\n",
            "Error occurred while processing: models.\n"
          ]
        }
      ],
      "source": [
        "# Set seed for reproducibility\n",
        "SEED = 1122\n",
        "# Import necessary libraries\n",
        "import os\n",
        "\n",
        "# Set environment variables before importing modules\n",
        "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
        "os.environ['MPLCONFIGDIR'] = os.getcwd() + '/configs/'\n",
        "\n",
        "# Suppress warnings\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "warnings.simplefilter(action='ignore', category=Warning)\n",
        "\n",
        "# Import necessary modules\n",
        "import logging\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# Set seeds for random number generators in NumPy and Python\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "# Import PyTorch\n",
        "import torch\n",
        "torch.manual_seed(SEED)\n",
        "from torch import nn\n",
        "from torchsummary import summary\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.utils.data import TensorDataset, DataLoader, WeightedRandomSampler\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "!pkill -f tensorboard\n",
        "%load_ext tensorboard\n",
        "!mkdir -p {models_dir}\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Device: {device}\")\n",
        "\n",
        "# Import other libraries\n",
        "import copy\n",
        "import shutil\n",
        "from datetime import datetime\n",
        "from itertools import product\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix,classification_report\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import json\n",
        "\n",
        "# Configure plot display settings\n",
        "sns.set(font_scale=1.4)\n",
        "sns.set_style('white')\n",
        "plt.rc('font', size=14)\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWnQz-p-xyhD"
      },
      "source": [
        "<a id=\"data-preprocessing\"></a>\n",
        "## 4. Data Preprocessing\n",
        "\n",
        "Explore data, split into train/val/test sets, normalize features, and encode labels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPMtvy5Fo8Rw"
      },
      "source": [
        "### 4.1 Data Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 603
        },
        "id": "hHK2Aw7Ix4S8",
        "outputId": "0502449f-9f0b-473e-cd46-2c9a4afbfb9e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset shape: (105760, 40)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sample_index</th>\n",
              "      <th>time</th>\n",
              "      <th>pain_survey_1</th>\n",
              "      <th>pain_survey_2</th>\n",
              "      <th>pain_survey_3</th>\n",
              "      <th>pain_survey_4</th>\n",
              "      <th>n_legs</th>\n",
              "      <th>n_hands</th>\n",
              "      <th>n_eyes</th>\n",
              "      <th>joint_00</th>\n",
              "      <th>...</th>\n",
              "      <th>joint_21</th>\n",
              "      <th>joint_22</th>\n",
              "      <th>joint_23</th>\n",
              "      <th>joint_24</th>\n",
              "      <th>joint_25</th>\n",
              "      <th>joint_26</th>\n",
              "      <th>joint_27</th>\n",
              "      <th>joint_28</th>\n",
              "      <th>joint_29</th>\n",
              "      <th>joint_30</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>two</td>\n",
              "      <td>two</td>\n",
              "      <td>two</td>\n",
              "      <td>1.094705</td>\n",
              "      <td>...</td>\n",
              "      <td>3.499558e-06</td>\n",
              "      <td>1.945042e-06</td>\n",
              "      <td>3.999558e-06</td>\n",
              "      <td>1.153299e-05</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>0.017592</td>\n",
              "      <td>0.013508</td>\n",
              "      <td>0.026798</td>\n",
              "      <td>0.027815</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>two</td>\n",
              "      <td>two</td>\n",
              "      <td>two</td>\n",
              "      <td>1.135183</td>\n",
              "      <td>...</td>\n",
              "      <td>3.976952e-07</td>\n",
              "      <td>6.765107e-07</td>\n",
              "      <td>6.019627e-06</td>\n",
              "      <td>4.643774e-08</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.013352</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.013377</td>\n",
              "      <td>0.013716</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>two</td>\n",
              "      <td>two</td>\n",
              "      <td>two</td>\n",
              "      <td>1.080745</td>\n",
              "      <td>...</td>\n",
              "      <td>1.533820e-07</td>\n",
              "      <td>1.698525e-07</td>\n",
              "      <td>1.446051e-06</td>\n",
              "      <td>2.424536e-06</td>\n",
              "      <td>0.000003</td>\n",
              "      <td>0.016225</td>\n",
              "      <td>0.008110</td>\n",
              "      <td>0.024097</td>\n",
              "      <td>0.023105</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>two</td>\n",
              "      <td>two</td>\n",
              "      <td>two</td>\n",
              "      <td>0.938017</td>\n",
              "      <td>...</td>\n",
              "      <td>1.006865e-05</td>\n",
              "      <td>5.511079e-07</td>\n",
              "      <td>1.847597e-06</td>\n",
              "      <td>5.432416e-08</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.011832</td>\n",
              "      <td>0.007450</td>\n",
              "      <td>0.028613</td>\n",
              "      <td>0.024648</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>two</td>\n",
              "      <td>two</td>\n",
              "      <td>two</td>\n",
              "      <td>1.090185</td>\n",
              "      <td>...</td>\n",
              "      <td>4.437266e-06</td>\n",
              "      <td>1.735459e-07</td>\n",
              "      <td>1.552722e-06</td>\n",
              "      <td>5.825366e-08</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>0.005360</td>\n",
              "      <td>0.002532</td>\n",
              "      <td>0.033026</td>\n",
              "      <td>0.025328</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>two</td>\n",
              "      <td>two</td>\n",
              "      <td>two</td>\n",
              "      <td>1.146031</td>\n",
              "      <td>...</td>\n",
              "      <td>1.073167e-06</td>\n",
              "      <td>1.753837e-07</td>\n",
              "      <td>2.957340e-07</td>\n",
              "      <td>6.217311e-08</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>0.006150</td>\n",
              "      <td>0.006444</td>\n",
              "      <td>0.033101</td>\n",
              "      <td>0.023767</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>two</td>\n",
              "      <td>two</td>\n",
              "      <td>two</td>\n",
              "      <td>1.025870</td>\n",
              "      <td>...</td>\n",
              "      <td>1.074800e-06</td>\n",
              "      <td>1.772156e-07</td>\n",
              "      <td>1.976558e-06</td>\n",
              "      <td>1.576086e-06</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>0.006495</td>\n",
              "      <td>0.006421</td>\n",
              "      <td>0.031804</td>\n",
              "      <td>0.019056</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>two</td>\n",
              "      <td>two</td>\n",
              "      <td>two</td>\n",
              "      <td>1.038597</td>\n",
              "      <td>...</td>\n",
              "      <td>8.829074e-07</td>\n",
              "      <td>1.790415e-07</td>\n",
              "      <td>2.210562e-06</td>\n",
              "      <td>1.485741e-06</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.015998</td>\n",
              "      <td>0.005397</td>\n",
              "      <td>0.035552</td>\n",
              "      <td>0.015732</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>two</td>\n",
              "      <td>two</td>\n",
              "      <td>two</td>\n",
              "      <td>0.984251</td>\n",
              "      <td>...</td>\n",
              "      <td>1.621055e-06</td>\n",
              "      <td>1.165161e-06</td>\n",
              "      <td>3.030164e-07</td>\n",
              "      <td>5.416678e-07</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.020539</td>\n",
              "      <td>0.008517</td>\n",
              "      <td>0.008635</td>\n",
              "      <td>0.015257</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>two</td>\n",
              "      <td>two</td>\n",
              "      <td>two</td>\n",
              "      <td>1.054999</td>\n",
              "      <td>...</td>\n",
              "      <td>1.609114e-06</td>\n",
              "      <td>3.959558e-06</td>\n",
              "      <td>2.017157e-06</td>\n",
              "      <td>1.154349e-06</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>0.007682</td>\n",
              "      <td>0.021383</td>\n",
              "      <td>0.034006</td>\n",
              "      <td>0.028966</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10 rows √ó 40 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   sample_index  time  pain_survey_1  pain_survey_2  pain_survey_3  \\\n",
              "0             0     0              2              0              2   \n",
              "1             0     1              2              2              2   \n",
              "2             0     2              2              0              2   \n",
              "3             0     3              2              2              2   \n",
              "4             0     4              2              2              2   \n",
              "5             0     5              2              0              2   \n",
              "6             0     6              2              1              2   \n",
              "7             0     7              2              2              2   \n",
              "8             0     8              2              2              0   \n",
              "9             0     9              0              2              2   \n",
              "\n",
              "   pain_survey_4 n_legs n_hands n_eyes  joint_00  ...      joint_21  \\\n",
              "0              1    two     two    two  1.094705  ...  3.499558e-06   \n",
              "1              2    two     two    two  1.135183  ...  3.976952e-07   \n",
              "2              2    two     two    two  1.080745  ...  1.533820e-07   \n",
              "3              2    two     two    two  0.938017  ...  1.006865e-05   \n",
              "4              2    two     two    two  1.090185  ...  4.437266e-06   \n",
              "5              1    two     two    two  1.146031  ...  1.073167e-06   \n",
              "6              1    two     two    two  1.025870  ...  1.074800e-06   \n",
              "7              2    two     two    two  1.038597  ...  8.829074e-07   \n",
              "8              1    two     two    two  0.984251  ...  1.621055e-06   \n",
              "9              2    two     two    two  1.054999  ...  1.609114e-06   \n",
              "\n",
              "       joint_22      joint_23      joint_24  joint_25  joint_26  joint_27  \\\n",
              "0  1.945042e-06  3.999558e-06  1.153299e-05  0.000004  0.017592  0.013508   \n",
              "1  6.765107e-07  6.019627e-06  4.643774e-08  0.000000  0.013352  0.000000   \n",
              "2  1.698525e-07  1.446051e-06  2.424536e-06  0.000003  0.016225  0.008110   \n",
              "3  5.511079e-07  1.847597e-06  5.432416e-08  0.000000  0.011832  0.007450   \n",
              "4  1.735459e-07  1.552722e-06  5.825366e-08  0.000007  0.005360  0.002532   \n",
              "5  1.753837e-07  2.957340e-07  6.217311e-08  0.000007  0.006150  0.006444   \n",
              "6  1.772156e-07  1.976558e-06  1.576086e-06  0.000005  0.006495  0.006421   \n",
              "7  1.790415e-07  2.210562e-06  1.485741e-06  0.000000  0.015998  0.005397   \n",
              "8  1.165161e-06  3.030164e-07  5.416678e-07  0.000000  0.020539  0.008517   \n",
              "9  3.959558e-06  2.017157e-06  1.154349e-06  0.000007  0.007682  0.021383   \n",
              "\n",
              "   joint_28  joint_29  joint_30  \n",
              "0  0.026798  0.027815       0.5  \n",
              "1  0.013377  0.013716       0.5  \n",
              "2  0.024097  0.023105       0.5  \n",
              "3  0.028613  0.024648       0.5  \n",
              "4  0.033026  0.025328       0.5  \n",
              "5  0.033101  0.023767       0.5  \n",
              "6  0.031804  0.019056       0.5  \n",
              "7  0.035552  0.015732       0.5  \n",
              "8  0.008635  0.015257       0.5  \n",
              "9  0.034006  0.028966       0.5  \n",
              "\n",
              "[10 rows x 40 columns]"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Print the shape of the dataset\n",
        "print(f\"Dataset shape: {X_train.shape}\")\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "X_train.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlDwxJ38o8Rw"
      },
      "source": [
        "### 4.2 Categorical Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "LfBKIdsrQDW3"
      },
      "outputs": [],
      "source": [
        "# Merge features and labels\n",
        "data = X_train.merge(y_train, on='sample_index')\n",
        "\n",
        "# Create a mapping dictionary to convert categorical labels to numerical values\n",
        "map_dict_legs = { 'two': 2, 'one+peg_leg': 1}\n",
        "map_dict_hands = { 'two': 2, 'one+hook_hand': 1}\n",
        "map_dict_eyes = { 'two': 2, 'one+eye_patch': 1}\n",
        "data['n_legs'] = data['n_legs'].map(map_dict_legs)\n",
        "data['n_hands'] = data['n_hands'].map(map_dict_hands)\n",
        "data['n_eyes'] = data['n_eyes'].map(map_dict_eyes)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUwlOzWBo8Rx"
      },
      "source": [
        "### 4.3 Stratified Train/Val/Test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original label distribution: {'no_pain': 511, 'low_pain': 94, 'high_pain': 56}\n",
            "Train label counts before balancing: {'no_pain': 385, 'low_pain': 71, 'high_pain': 42}\n",
            "Train label counts after balancing: {'no_pain': 385, 'low_pain': 71, 'high_pain': 42}\n",
            "\n",
            "Label proportions:\n",
            "Train:\n",
            " label\n",
            "no_pain      0.773092\n",
            "low_pain     0.142570\n",
            "high_pain    0.084337\n",
            "Name: proportion, dtype: float64\n",
            "Val:\n",
            " label\n",
            "no_pain      0.7750\n",
            "low_pain     0.1375\n",
            "high_pain    0.0875\n",
            "Name: proportion, dtype: float64\n",
            "Test:\n",
            " label\n",
            "no_pain     0.666667\n",
            "low_pain    0.333333\n",
            "Name: proportion, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "# df has columns: ['sample_index', 'label']\n",
        "N_NO_PAIN_KEEP = 1000   # how many \"no_pain\" users to keep\n",
        "N_LOW_PAIN_KEEP = 150   # how many \"low_pain\" users to keep\n",
        "N_VAL_USERS = 160\n",
        "N_TEST_USERS = 3\n",
        "\n",
        "# --- Step 1: Compute each user's dominant label ---\n",
        "user_labels = (\n",
        "    data.groupby('sample_index')['label']\n",
        "    .agg(lambda x: x.value_counts().index[0])  # dominant label per user\n",
        "    .reset_index()\n",
        ")\n",
        "\n",
        "print(\"Original label distribution:\", user_labels['label'].value_counts().to_dict())\n",
        "\n",
        "# --- Step 2: Stratified Shuffle Split for train/val/test ---\n",
        "n_total = len(user_labels)\n",
        "test_size = (N_VAL_USERS + N_TEST_USERS) / n_total\n",
        "\n",
        "sss_outer = StratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=42)\n",
        "for train_idx, temp_idx in sss_outer.split(user_labels['sample_index'], user_labels['label']):\n",
        "    train_users = user_labels.iloc[train_idx]['sample_index']\n",
        "    temp_users = user_labels.iloc[temp_idx]['sample_index']\n",
        "\n",
        "temp_labels = user_labels[user_labels['sample_index'].isin(temp_users)]\n",
        "\n",
        "# Split temp into val/test (also stratified)\n",
        "sss_inner = StratifiedShuffleSplit(n_splits=1, test_size=N_TEST_USERS / (N_VAL_USERS + N_TEST_USERS), random_state=42)\n",
        "for val_idx, test_idx in sss_inner.split(temp_labels['sample_index'], temp_labels['label']):\n",
        "    val_users = temp_labels.iloc[val_idx]['sample_index']\n",
        "    test_users = temp_labels.iloc[test_idx]['sample_index']\n",
        "\n",
        "# --- Step 3: Partial balancing of the training set ---\n",
        "train_labels = user_labels[user_labels['sample_index'].isin(train_users)]\n",
        "\n",
        "print(\"Train label counts before balancing:\", train_labels['label'].value_counts().to_dict())\n",
        "\n",
        "rng = random.Random(42)\n",
        "no_pain_users = train_labels[train_labels['label'] == 'no_pain']['sample_index'].tolist()\n",
        "low_pain_users = train_labels[train_labels['label'] == 'low_pain']['sample_index'].tolist()\n",
        "high_pain_users = train_labels[train_labels['label'] == 'high_pain']['sample_index'].tolist()\n",
        "\n",
        "no_pain_keep = min(N_NO_PAIN_KEEP, len(no_pain_users))\n",
        "low_pain_keep = min(N_LOW_PAIN_KEEP, len(low_pain_users))\n",
        "high_pain_keep = len(high_pain_users)\n",
        "\n",
        "selected_no_pain = rng.sample(no_pain_users, no_pain_keep)\n",
        "selected_low_pain = rng.sample(low_pain_users, low_pain_keep)\n",
        "selected_high_pain = high_pain_users  # keep all\n",
        "\n",
        "selected_users = selected_no_pain + selected_low_pain + selected_high_pain\n",
        "balanced_train_labels = train_labels[train_labels['sample_index'].isin(selected_users)]\n",
        "\n",
        "print(\"Train label counts after balancing:\", balanced_train_labels['label'].value_counts().to_dict())\n",
        "\n",
        "train_users = balanced_train_labels['sample_index']\n",
        "\n",
        "# --- Step 4: Filter the main dataframe ---\n",
        "df_train = data[data['sample_index'].isin(train_users)]\n",
        "df_val = data[data['sample_index'].isin(val_users)]\n",
        "df_test = data[data['sample_index'].isin(test_users)]\n",
        "\n",
        "# --- Step 5: Display final distributions ---\n",
        "print(\"\\nLabel proportions:\")\n",
        "print(\"Train:\\n\", df_train['label'].value_counts(normalize=True))\n",
        "print(\"Val:\\n\", df_val['label'].value_counts(normalize=True))\n",
        "print(\"Test:\\n\", df_test['label'].value_counts(normalize=True))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UqptiNjNQDW3",
        "outputId": "08945ee0-5169-4577-f7ef-92734ebf3a13"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "# import random\n",
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "# # df has columns: ['sample_index', 'label']\n",
        "# N_VAL_USERS = 120\n",
        "# N_TEST_USERS = 120\n",
        "\n",
        "# # --- Step 1: Compute each user's dominant label (or label distribution)\n",
        "# user_labels = (\n",
        "#     data.groupby('sample_index')['label']\n",
        "#     .agg(lambda x: x.value_counts().index[0])  # dominant label per user\n",
        "#     .reset_index()\n",
        "# )\n",
        "\n",
        "# train_users, temp_users = train_test_split(\n",
        "#     user_labels['sample_index'],\n",
        "#     test_size=(N_VAL_USERS + N_TEST_USERS) / len(user_labels),\n",
        "#     stratify=user_labels['label'],\n",
        "#     random_state=SEED\n",
        "# )\n",
        "\n",
        "# # Split temp into val/test (also stratified)\n",
        "# temp_labels = user_labels[user_labels['sample_index'].isin(temp_users)]\n",
        "# if N_TEST_USERS != 0:\n",
        "#   val_users, test_users = train_test_split(\n",
        "#       temp_labels['sample_index'],\n",
        "#       test_size=N_TEST_USERS / (N_VAL_USERS + N_TEST_USERS),\n",
        "#       stratify=temp_labels['label'],\n",
        "#       random_state=SEED\n",
        "#   )\n",
        "# else:\n",
        "#   val_users = temp_users\n",
        "#   test_users = []\n",
        "\n",
        "# # --- Step 3: Filter your main df\n",
        "# df_train = data[data['sample_index'].isin(train_users)]\n",
        "# df_val = data[data['sample_index'].isin(val_users)]\n",
        "# df_test = data[data['sample_index'].isin(test_users)]\n",
        "\n",
        "# # --- Step 4: Check label proportions\n",
        "# print(\"Label proportions:\")\n",
        "# print(\"Train:\\n\", df_train['label'].value_counts(normalize=True))\n",
        "# print(\"Val:\\n\", df_val['label'].value_counts(normalize=True))\n",
        "# print(\"Test:\\n\", df_test['label'].value_counts(normalize=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kI13WygTQDW4",
        "outputId": "464ef4a7-36f2-4950-fee7-8591512f296a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((79680, 41), (25600, 41), (480, 41))"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_train.shape, df_val.shape, df_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PsE1KsTTQDW4",
        "outputId": "2014ea4e-c61f-4498-a92b-209ca98eddca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total pirates in training set: 498\n",
            "Total pirates in validation set: 160\n",
            "Total pirates in test set: 3\n"
          ]
        }
      ],
      "source": [
        "# Print the total number of pirates for each dataset\n",
        "print(f\"Total pirates in training set: {df_train['sample_index'].nunique()}\")\n",
        "print(f\"Total pirates in validation set: {df_val['sample_index'].nunique()}\")\n",
        "print(f\"Total pirates in test set: {df_test['sample_index'].nunique()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9Pp8l1bo8Rx"
      },
      "source": [
        "### 4.4 Feature Normalization (min-max)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "YkF51CmhQDW5"
      },
      "outputs": [],
      "source": [
        "# Define the columns to be normalised\n",
        "\n",
        "scale_columns = [\n",
        "    col for col in data.columns\n",
        "    if (col.startswith('joint_') or col.startswith('pain_survey')) and not col.startswith('joint_30')\n",
        "]\n",
        "\n",
        "\n",
        "\n",
        "# Calculate the minimum and maximum values from the training data only\n",
        "mins_train = df_train[scale_columns].min()\n",
        "maxs_train = df_train[scale_columns].max()\n",
        "\n",
        "#mins_val = df_val[scale_columns].min()\n",
        "#maxs_val = df_val[scale_columns].max()\n",
        "#\n",
        "#mins_test = df_test[scale_columns].min()\n",
        "#maxs_test = df_test[scale_columns].max()\n",
        "\n",
        "####\n",
        "#CHANGED ALL THE REGULARIZATION TO USE MIN AND MAX VALUES FROM THE TRAINING DATA FOR GENERALIZATION\n",
        "###\n",
        "\n",
        "# Apply normalisation to the specified columns in all datasets\n",
        "for column in scale_columns:\n",
        "    denom = maxs_train[column] - mins_train[column]\n",
        "    if np.isclose(denom, 0.0):\n",
        "        df_train[column] = 0.0\n",
        "        df_val[column] = 0.0\n",
        "        df_test[column] = 0.0\n",
        "        continue\n",
        "\n",
        "    # Normalise the training set\n",
        "    df_train[column] = (df_train[column] - mins_train[column]) / denom\n",
        "\n",
        "    # Normalise the validation set\n",
        "    df_val[column] = (df_val[column] - mins_train[column]) / denom\n",
        "\n",
        "    # Normalise the test set\n",
        "    df_test[column] = (df_test[column] - mins_train[column]) / denom\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        },
        "id": "4gQk4AVMBVuD",
        "outputId": "f48fc20d-61f7-4619-d593-ddd51899693c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sample_index</th>\n",
              "      <th>time</th>\n",
              "      <th>pain_survey_1</th>\n",
              "      <th>pain_survey_2</th>\n",
              "      <th>pain_survey_3</th>\n",
              "      <th>pain_survey_4</th>\n",
              "      <th>n_legs</th>\n",
              "      <th>n_hands</th>\n",
              "      <th>n_eyes</th>\n",
              "      <th>joint_00</th>\n",
              "      <th>...</th>\n",
              "      <th>joint_22</th>\n",
              "      <th>joint_23</th>\n",
              "      <th>joint_24</th>\n",
              "      <th>joint_25</th>\n",
              "      <th>joint_26</th>\n",
              "      <th>joint_27</th>\n",
              "      <th>joint_28</th>\n",
              "      <th>joint_29</th>\n",
              "      <th>joint_30</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0.777046</td>\n",
              "      <td>...</td>\n",
              "      <td>1.503263e-06</td>\n",
              "      <td>0.000105</td>\n",
              "      <td>0.000405</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>0.014214</td>\n",
              "      <td>0.011376</td>\n",
              "      <td>0.018978</td>\n",
              "      <td>0.020291</td>\n",
              "      <td>0.5</td>\n",
              "      <td>no_pain</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0.805855</td>\n",
              "      <td>...</td>\n",
              "      <td>4.403064e-07</td>\n",
              "      <td>0.000158</td>\n",
              "      <td>0.000001</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.010748</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.009473</td>\n",
              "      <td>0.010006</td>\n",
              "      <td>0.5</td>\n",
              "      <td>no_pain</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0.767110</td>\n",
              "      <td>...</td>\n",
              "      <td>1.575589e-08</td>\n",
              "      <td>0.000038</td>\n",
              "      <td>0.000085</td>\n",
              "      <td>0.000003</td>\n",
              "      <td>0.013097</td>\n",
              "      <td>0.006830</td>\n",
              "      <td>0.017065</td>\n",
              "      <td>0.016856</td>\n",
              "      <td>0.5</td>\n",
              "      <td>no_pain</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0.665528</td>\n",
              "      <td>...</td>\n",
              "      <td>3.352260e-07</td>\n",
              "      <td>0.000049</td>\n",
              "      <td>0.000002</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.009505</td>\n",
              "      <td>0.006274</td>\n",
              "      <td>0.020264</td>\n",
              "      <td>0.017981</td>\n",
              "      <td>0.5</td>\n",
              "      <td>no_pain</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0.773829</td>\n",
              "      <td>...</td>\n",
              "      <td>1.885071e-08</td>\n",
              "      <td>0.000041</td>\n",
              "      <td>0.000002</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>0.004216</td>\n",
              "      <td>0.002132</td>\n",
              "      <td>0.023389</td>\n",
              "      <td>0.018477</td>\n",
              "      <td>0.5</td>\n",
              "      <td>no_pain</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0.813575</td>\n",
              "      <td>...</td>\n",
              "      <td>2.039074e-08</td>\n",
              "      <td>0.000008</td>\n",
              "      <td>0.000002</td>\n",
              "      <td>0.000008</td>\n",
              "      <td>0.004861</td>\n",
              "      <td>0.005427</td>\n",
              "      <td>0.023442</td>\n",
              "      <td>0.017338</td>\n",
              "      <td>0.5</td>\n",
              "      <td>no_pain</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0.728054</td>\n",
              "      <td>...</td>\n",
              "      <td>2.192577e-08</td>\n",
              "      <td>0.000052</td>\n",
              "      <td>0.000055</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>0.005143</td>\n",
              "      <td>0.005407</td>\n",
              "      <td>0.022523</td>\n",
              "      <td>0.013901</td>\n",
              "      <td>0.5</td>\n",
              "      <td>no_pain</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0.737113</td>\n",
              "      <td>...</td>\n",
              "      <td>2.345575e-08</td>\n",
              "      <td>0.000058</td>\n",
              "      <td>0.000052</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.012911</td>\n",
              "      <td>0.004546</td>\n",
              "      <td>0.025178</td>\n",
              "      <td>0.011477</td>\n",
              "      <td>0.5</td>\n",
              "      <td>no_pain</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0.698434</td>\n",
              "      <td>...</td>\n",
              "      <td>8.497671e-07</td>\n",
              "      <td>0.000008</td>\n",
              "      <td>0.000019</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.016622</td>\n",
              "      <td>0.007172</td>\n",
              "      <td>0.006115</td>\n",
              "      <td>0.011130</td>\n",
              "      <td>0.5</td>\n",
              "      <td>no_pain</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>9 rows √ó 41 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   sample_index  time  pain_survey_1  pain_survey_2  pain_survey_3  \\\n",
              "0             0     0            1.0            0.0            1.0   \n",
              "1             0     1            1.0            1.0            1.0   \n",
              "2             0     2            1.0            0.0            1.0   \n",
              "3             0     3            1.0            1.0            1.0   \n",
              "4             0     4            1.0            1.0            1.0   \n",
              "5             0     5            1.0            0.0            1.0   \n",
              "6             0     6            1.0            0.5            1.0   \n",
              "7             0     7            1.0            1.0            1.0   \n",
              "8             0     8            1.0            1.0            0.0   \n",
              "\n",
              "   pain_survey_4  n_legs  n_hands  n_eyes  joint_00  ...      joint_22  \\\n",
              "0            0.5       2        2       2  0.777046  ...  1.503263e-06   \n",
              "1            1.0       2        2       2  0.805855  ...  4.403064e-07   \n",
              "2            1.0       2        2       2  0.767110  ...  1.575589e-08   \n",
              "3            1.0       2        2       2  0.665528  ...  3.352260e-07   \n",
              "4            1.0       2        2       2  0.773829  ...  1.885071e-08   \n",
              "5            0.5       2        2       2  0.813575  ...  2.039074e-08   \n",
              "6            0.5       2        2       2  0.728054  ...  2.192577e-08   \n",
              "7            1.0       2        2       2  0.737113  ...  2.345575e-08   \n",
              "8            0.5       2        2       2  0.698434  ...  8.497671e-07   \n",
              "\n",
              "   joint_23  joint_24  joint_25  joint_26  joint_27  joint_28  joint_29  \\\n",
              "0  0.000105  0.000405  0.000004  0.014214  0.011376  0.018978  0.020291   \n",
              "1  0.000158  0.000001  0.000000  0.010748  0.000000  0.009473  0.010006   \n",
              "2  0.000038  0.000085  0.000003  0.013097  0.006830  0.017065  0.016856   \n",
              "3  0.000049  0.000002  0.000000  0.009505  0.006274  0.020264  0.017981   \n",
              "4  0.000041  0.000002  0.000007  0.004216  0.002132  0.023389  0.018477   \n",
              "5  0.000008  0.000002  0.000008  0.004861  0.005427  0.023442  0.017338   \n",
              "6  0.000052  0.000055  0.000005  0.005143  0.005407  0.022523  0.013901   \n",
              "7  0.000058  0.000052  0.000000  0.012911  0.004546  0.025178  0.011477   \n",
              "8  0.000008  0.000019  0.000000  0.016622  0.007172  0.006115  0.011130   \n",
              "\n",
              "   joint_30    label  \n",
              "0       0.5  no_pain  \n",
              "1       0.5  no_pain  \n",
              "2       0.5  no_pain  \n",
              "3       0.5  no_pain  \n",
              "4       0.5  no_pain  \n",
              "5       0.5  no_pain  \n",
              "6       0.5  no_pain  \n",
              "7       0.5  no_pain  \n",
              "8       0.5  no_pain  \n",
              "\n",
              "[9 rows x 41 columns]"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_train.head(9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "V_1543vlBEqf"
      },
      "outputs": [],
      "source": [
        "# @title  Delete Some Columns Experimental\n",
        "\n",
        "#del_columns = [\n",
        "#    col for col in data.columns\n",
        "#    if not (col.startswith('pain_survey') or col.startswith('sample_index') or col.startswith('label') or col.startswith('time') or\n",
        "#            col.endswith('00') or col.endswith('01') or col.endswith('02') or col.endswith('03') or col.endswith('04') or col.endswith('05')\n",
        "#            or col.endswith('06') or col.endswith('07') or col.endswith('08') or col.endswith('09') or col.endswith('10') or col.endswith('11')\n",
        "#            or col.endswith('12') or col.endswith('25') or col.endswith('26') or col.endswith('27') or col.endswith('28') or col.endswith('29'))\n",
        "#]\n",
        "#\n",
        "#for column in del_columns:\n",
        "#\n",
        "#    # Normalise the training set\n",
        "#    df_train[column] =  0.0\n",
        "#    df_val[column] =  0.0\n",
        "#    df_test[column] =  0.0\n",
        "#\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 342
        },
        "id": "R-cyjeAuYt4D",
        "outputId": "0ab5ca55-70c0-47bc-8296-58e1cc4234f7"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sample_index</th>\n",
              "      <th>time</th>\n",
              "      <th>pain_survey_1</th>\n",
              "      <th>pain_survey_2</th>\n",
              "      <th>pain_survey_3</th>\n",
              "      <th>pain_survey_4</th>\n",
              "      <th>n_legs</th>\n",
              "      <th>n_hands</th>\n",
              "      <th>n_eyes</th>\n",
              "      <th>joint_00</th>\n",
              "      <th>...</th>\n",
              "      <th>joint_22</th>\n",
              "      <th>joint_23</th>\n",
              "      <th>joint_24</th>\n",
              "      <th>joint_25</th>\n",
              "      <th>joint_26</th>\n",
              "      <th>joint_27</th>\n",
              "      <th>joint_28</th>\n",
              "      <th>joint_29</th>\n",
              "      <th>joint_30</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0.777046</td>\n",
              "      <td>...</td>\n",
              "      <td>1.503263e-06</td>\n",
              "      <td>0.000105</td>\n",
              "      <td>0.000405</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>0.014214</td>\n",
              "      <td>0.011376</td>\n",
              "      <td>0.018978</td>\n",
              "      <td>0.020291</td>\n",
              "      <td>0.5</td>\n",
              "      <td>no_pain</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0.805855</td>\n",
              "      <td>...</td>\n",
              "      <td>4.403064e-07</td>\n",
              "      <td>0.000158</td>\n",
              "      <td>0.000001</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.010748</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.009473</td>\n",
              "      <td>0.010006</td>\n",
              "      <td>0.5</td>\n",
              "      <td>no_pain</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0.767110</td>\n",
              "      <td>...</td>\n",
              "      <td>1.575589e-08</td>\n",
              "      <td>0.000038</td>\n",
              "      <td>0.000085</td>\n",
              "      <td>0.000003</td>\n",
              "      <td>0.013097</td>\n",
              "      <td>0.006830</td>\n",
              "      <td>0.017065</td>\n",
              "      <td>0.016856</td>\n",
              "      <td>0.5</td>\n",
              "      <td>no_pain</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0.665528</td>\n",
              "      <td>...</td>\n",
              "      <td>3.352260e-07</td>\n",
              "      <td>0.000049</td>\n",
              "      <td>0.000002</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.009505</td>\n",
              "      <td>0.006274</td>\n",
              "      <td>0.020264</td>\n",
              "      <td>0.017981</td>\n",
              "      <td>0.5</td>\n",
              "      <td>no_pain</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0.773829</td>\n",
              "      <td>...</td>\n",
              "      <td>1.885071e-08</td>\n",
              "      <td>0.000041</td>\n",
              "      <td>0.000002</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>0.004216</td>\n",
              "      <td>0.002132</td>\n",
              "      <td>0.023389</td>\n",
              "      <td>0.018477</td>\n",
              "      <td>0.5</td>\n",
              "      <td>no_pain</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows √ó 41 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   sample_index  time  pain_survey_1  pain_survey_2  pain_survey_3  \\\n",
              "0             0     0            1.0            0.0            1.0   \n",
              "1             0     1            1.0            1.0            1.0   \n",
              "2             0     2            1.0            0.0            1.0   \n",
              "3             0     3            1.0            1.0            1.0   \n",
              "4             0     4            1.0            1.0            1.0   \n",
              "\n",
              "   pain_survey_4  n_legs  n_hands  n_eyes  joint_00  ...      joint_22  \\\n",
              "0            0.5       2        2       2  0.777046  ...  1.503263e-06   \n",
              "1            1.0       2        2       2  0.805855  ...  4.403064e-07   \n",
              "2            1.0       2        2       2  0.767110  ...  1.575589e-08   \n",
              "3            1.0       2        2       2  0.665528  ...  3.352260e-07   \n",
              "4            1.0       2        2       2  0.773829  ...  1.885071e-08   \n",
              "\n",
              "   joint_23  joint_24  joint_25  joint_26  joint_27  joint_28  joint_29  \\\n",
              "0  0.000105  0.000405  0.000004  0.014214  0.011376  0.018978  0.020291   \n",
              "1  0.000158  0.000001  0.000000  0.010748  0.000000  0.009473  0.010006   \n",
              "2  0.000038  0.000085  0.000003  0.013097  0.006830  0.017065  0.016856   \n",
              "3  0.000049  0.000002  0.000000  0.009505  0.006274  0.020264  0.017981   \n",
              "4  0.000041  0.000002  0.000007  0.004216  0.002132  0.023389  0.018477   \n",
              "\n",
              "   joint_30    label  \n",
              "0       0.5  no_pain  \n",
              "1       0.5  no_pain  \n",
              "2       0.5  no_pain  \n",
              "3       0.5  no_pain  \n",
              "4       0.5  no_pain  \n",
              "\n",
              "[5 rows x 41 columns]"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDuw27RUo8Rx"
      },
      "source": [
        "### 4.5 Label Distribution Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gDhUJUrkQDW6",
        "outputId": "078c32b5-8b61-49d8-9368-e7cd1a7604c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training labels: {'no_pain': 385, 'low_pain': 71, 'high_pain': 42}\n",
            "Validation labels: {'no_pain': 124, 'low_pain': 22, 'high_pain': 14}\n",
            "Test labels: {'no_pain': 2, 'low_pain': 1, 'high_pain': 0}\n"
          ]
        }
      ],
      "source": [
        "# Initialise a dictionary to count occurrences of each activity in the training set\n",
        "training_labels = {\n",
        "    'no_pain': 0,\n",
        "    'low_pain': 0,\n",
        "    'high_pain': 0\n",
        "}\n",
        "\n",
        "# Count occurrences of each activity for unique IDs in the training set\n",
        "for id in df_train['sample_index'].unique():\n",
        "    label = df_train[df_train['sample_index'] == id]['label'].values[0]\n",
        "    training_labels[label] += 1\n",
        "\n",
        "\n",
        "# Print the distribution of training labels\n",
        "print('Training labels:', training_labels)\n",
        "\n",
        "# Initialise a dictionary to count occurrences of each activity in the training set\n",
        "val_labels = {\n",
        "    'no_pain': 0,\n",
        "    'low_pain': 0,\n",
        "    'high_pain': 0\n",
        "}\n",
        "\n",
        "# Count occurrences of each activity for unique IDs in the training set\n",
        "for id in df_val['sample_index'].unique():\n",
        "    label = df_val[df_val['sample_index'] == id]['label'].values[0]\n",
        "    val_labels[label] += 1\n",
        "\n",
        "# Print the distribution of validation labels\n",
        "print('Validation labels:', val_labels)\n",
        "\n",
        "# Initialise a dictionary to count occurrences of each activity in the test set\n",
        "test_labels = {\n",
        "    'no_pain': 0,\n",
        "    'low_pain': 0,\n",
        "    'high_pain': 0\n",
        "}\n",
        "\n",
        "# Count occurrences of each activity for unique IDs in the test set\n",
        "for id in df_test['sample_index'].unique():\n",
        "    label = df_test[df_test['sample_index'] == id]['label'].values[0]\n",
        "    test_labels[label] += 1\n",
        "\n",
        "# Print the distribution of test labels\n",
        "print('Test labels:', test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "DT3wxqdLQDW7"
      },
      "outputs": [],
      "source": [
        "# Define a training mapping of label names to integer labels\n",
        "label_mapping = {\n",
        "    'no_pain': 0,\n",
        "    'low_pain': 1,\n",
        "    'high_pain': 2\n",
        "}\n",
        "\n",
        "# Map label names to integers in the training set\n",
        "df_train['label'] = df_train['label'].map(label_mapping)\n",
        "\n",
        "# Map label names to integers in the validation set\n",
        "df_val['label'] = df_val['label'].map(label_mapping)\n",
        "\n",
        "# Map label names to integers in the test set\n",
        "df_test['label'] = df_test['label'].map(label_mapping)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kc0Ve8LYBYaZ",
        "outputId": "24206440-2288-4b80-8aab-ad855f9004e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   sample_index  time  pain_survey_1  pain_survey_2  pain_survey_3  \\\n",
            "0             0     0            1.0            0.0            1.0   \n",
            "1             0     1            1.0            1.0            1.0   \n",
            "2             0     2            1.0            0.0            1.0   \n",
            "\n",
            "   pain_survey_4  n_legs  n_hands  n_eyes  joint_00  ...      joint_22  \\\n",
            "0            0.5       2        2       2  0.777046  ...  1.503263e-06   \n",
            "1            1.0       2        2       2  0.805855  ...  4.403064e-07   \n",
            "2            1.0       2        2       2  0.767110  ...  1.575589e-08   \n",
            "\n",
            "   joint_23  joint_24  joint_25  joint_26  joint_27  joint_28  joint_29  \\\n",
            "0  0.000105  0.000405  0.000004  0.014214  0.011376  0.018978  0.020291   \n",
            "1  0.000158  0.000001  0.000000  0.010748  0.000000  0.009473  0.010006   \n",
            "2  0.000038  0.000085  0.000003  0.013097  0.006830  0.017065  0.016856   \n",
            "\n",
            "   joint_30  label  \n",
            "0       0.5      0  \n",
            "1       0.5      0  \n",
            "2       0.5      0  \n",
            "\n",
            "[3 rows x 41 columns]\n"
          ]
        }
      ],
      "source": [
        "print(df_train.head(3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOi0Yx5bo8Ry"
      },
      "source": [
        "<a id=\"sequence-building\"></a>\n",
        "## 5. Sequence Building\n",
        "\n",
        "Convert variable-length time-series into fixed-size windows for RNN input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "3qv_eAbDQDW7"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Define window and stride boolean variable -> if True, during training we will visit more time the same pirate with overlapping windows\n",
        "# if False, each pirate will be visited only once during training\n",
        "one_pirate_window = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "sbmKJyH-QDW7"
      },
      "outputs": [],
      "source": [
        "if one_pirate_window:\n",
        "    # Define the window size\n",
        "    WINDOW_SIZE = 5 # before: 80\n",
        "\n",
        "    # Stride size\n",
        "    STRIDE = 1\n",
        "else:\n",
        "    # Define the window size -> select an higher window size in order to get more pirates\n",
        "    WINDOW_SIZE = 160\n",
        "\n",
        "    # Stride size\n",
        "    STRIDE = 160"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ut3JifRfo8Ry"
      },
      "source": [
        "### 5.1 Window & Stride Configuration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUdcu3oGo8Ry"
      },
      "source": [
        "### 5.2 Build Sequences Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "dY-ksbv0QDW8"
      },
      "outputs": [],
      "source": [
        "def build_sequences(df, window=200, stride=200):\n",
        "    assert window % stride == 0\n",
        "\n",
        "    dataset = []\n",
        "    labels = []\n",
        "    ids = []  # <--- NEW: to store pirate/sample IDs\n",
        "\n",
        "    for id in df['sample_index'].unique():\n",
        "        columns = [col for col in df.columns if col not in ['sample_index', 'label', 'time']]\n",
        "        temp = df[df['sample_index'] == id][columns].values\n",
        "        label = df[df['sample_index'] == id]['label'].values[0]\n",
        "\n",
        "        remainder = len(temp) % window\n",
        "        padding_len = (window - remainder) % window\n",
        "        if padding_len:\n",
        "            padding = np.zeros((padding_len, len(columns)), dtype='float32')\n",
        "            temp = np.concatenate((temp, padding))\n",
        "\n",
        "        idx = 0\n",
        "        while idx + window <= len(temp):\n",
        "            dataset.append(temp[idx:idx + window])\n",
        "            labels.append(label)\n",
        "            ids.append(id)  # <--- NEW: add same ID for each window\n",
        "            idx += stride\n",
        "\n",
        "    dataset = np.array(dataset)\n",
        "    labels = np.array(labels)\n",
        "    ids = np.array(ids)  # <--- convert to numpy\n",
        "\n",
        "    return dataset, labels, ids  # <--- UPDATED return\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utsTrSH5o8Ry"
      },
      "source": [
        "### 5.3 Generate Sequences for Train/Val/Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPVEgwEmQDW8",
        "outputId": "e96c54e5-fdad-44d9-86e5-548b08b2abb0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_train shape: (77688, 5, 38), y_train shape: (77688,)\n",
            "X_val shape: (24960, 5, 38), y_val shape: (24960,)\n",
            "X_test shape: (468, 5, 38), y_test shape: (468,)\n"
          ]
        }
      ],
      "source": [
        "# Generate sequences and labels for the training set\n",
        "X_train, y_train, ids_train = build_sequences(df_train, WINDOW_SIZE, STRIDE)\n",
        "\n",
        "# Generate sequences and labels for the validation set\n",
        "X_val, y_val, ids_val = build_sequences(df_val, WINDOW_SIZE, STRIDE)\n",
        "\n",
        "# Generate sequences and labels for the test set\n",
        "X_test, y_test, ids_test = build_sequences(df_test, WINDOW_SIZE, STRIDE)\n",
        "\n",
        "# Print the shapes of the generated datasets and their labels\n",
        "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
        "print(f\"X_val shape: {X_val.shape}, y_val shape: {y_val.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jLnJ0YMo8Rz"
      },
      "source": [
        "### 5.4 Data Type Conversion & Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "X1mmtqJwQDW8"
      },
      "outputs": [],
      "source": [
        "# Convert dataset into float32 for PyTorch compatibility\n",
        "X_train = X_train.astype('float32')\n",
        "X_val = X_val.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "# y_train = y_train.astype('int64')\n",
        "# y_val = y_val.astype('int64')\n",
        "# y_test = y_test.astype('int64')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxfz2MegQDW8",
        "outputId": "4885cc3e-aee6-41f4-93ba-0adc1fab914b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of Classes: 3\n"
          ]
        }
      ],
      "source": [
        "# Define the input shape based on the training data\n",
        "input_shape = X_train.shape[1:]\n",
        "\n",
        "# Define the number of classes based on the categorical labels\n",
        "num_classes = len(np.unique(y_train))\n",
        "print(f\"Number of Classes: {num_classes}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "JcSpP3DBQDW8"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Discard nan values from the dataset\n",
        "if np.isnan(X_train).any() or np.isnan(X_val).any() or np.isnan(X_test).any():\n",
        "    X_train = np.nan_to_num(X_train)\n",
        "    X_val = np.nan_to_num(X_val)\n",
        "    X_test = np.nan_to_num(X_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "s45svGakQDW8"
      },
      "outputs": [],
      "source": [
        "# Convert numpy arrays to PyTorch datasets (pairs features with labels)\n",
        "train_ds = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n",
        "val_ds   = TensorDataset(torch.from_numpy(X_val), torch.from_numpy(y_val))\n",
        "test_ds  = TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5b-SGD2to8Rz"
      },
      "source": [
        "<a id=\"dataloaders\"></a>\n",
        "## 6. DataLoaders\n",
        "\n",
        "Create PyTorch DataLoaders for efficient batching and parallel loading."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "a2dd26aqQDW8"
      },
      "outputs": [],
      "source": [
        "# Define the batch size, which is the number of samples in each batch\n",
        "BATCH_SIZE = 1024 # we can change it depending on the GPU RAM available (by default 512)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "8CPWMPHOQDW9"
      },
      "outputs": [],
      "source": [
        "def make_loader(ds, batch_size, shuffle=True, drop_last=False, sampler=None):\n",
        "    \"\"\"\n",
        "    Create a DataLoader with performance optimizations.\n",
        "    - If `sampler` is given, it overrides `shuffle=True`.\n",
        "    \"\"\"\n",
        "    cpu_cores = os.cpu_count() or 2\n",
        "    num_workers = max(2, min(4, cpu_cores))\n",
        "\n",
        "    return DataLoader(\n",
        "        ds,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=(sampler is None and shuffle),  # disable shuffle if sampler provided\n",
        "        sampler=sampler,                        # use WeightedRandomSampler if given\n",
        "        drop_last=drop_last,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True,\n",
        "        pin_memory_device=\"cuda\" if torch.cuda.is_available() else \"\",\n",
        "        prefetch_factor=4\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "cPlQI3R8QDW9"
      },
      "outputs": [],
      "source": [
        "# Create data loaders with different settings for each phase\n",
        "#train_loader = make_loader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=False)\n",
        "#val_loader   = make_loader(val_ds, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
        "#test_loader  = make_loader(test_ds, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "P9y_LklBNxYH"
      },
      "outputs": [],
      "source": [
        "weights_fine = torch.tensor([1.0, 1.5, 2.0])\n",
        "labels = train_ds.tensors[1]  # get labels tensor\n",
        "\n",
        "sample_weights = [weights_fine[int(y)].item() for y in labels]\n",
        "sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights))\n",
        "\n",
        "train_loader = make_loader(train_ds, batch_size=BATCH_SIZE, sampler=sampler, shuffle=False)\n",
        "val_loader   = make_loader(val_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_loader  = make_loader(test_ds, batch_size=BATCH_SIZE, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        },
        "id": "7YQexn6nPR6z",
        "outputId": "af90099b-aea1-4c26-f337-e5b326ca5bd5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of all_labels_train_loader: (77688,)\n",
            "Type of all_labels_train_loader: int64\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1200x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "labels_from_train_loader = []\n",
        "\n",
        "\n",
        "# Extract labels from train_loader\n",
        "for _, yb in train_loader:\n",
        "    labels_from_train_loader.append(yb.cpu().numpy())\n",
        "\n",
        "# Concatenate labels into single NumPy arrays\n",
        "all_labels_train_loader = np.concatenate(labels_from_train_loader)\n",
        "\n",
        "\n",
        "print(f\"Shape of all_labels_train_loader: {all_labels_train_loader.shape}\")\n",
        "print(f\"Type of all_labels_train_loader: {all_labels_train_loader.dtype}\")\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Reverse the label mapping for plotting\n",
        "reverse_label_mapping = {v: k for k, v in label_mapping.items()}\n",
        "\n",
        "# Convert numerical labels back to original string labels for better visualization\n",
        "labels_normal_loader_str = np.vectorize(reverse_label_mapping.get)(all_labels_train_loader)\n",
        "\n",
        "\n",
        "# Create DataFrames for plotting\n",
        "df_normal = pd.DataFrame({'Label': labels_normal_loader_str})\n",
        "\n",
        "\n",
        "# Set up the plot\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Plot for normal train_loader\n",
        "plt.subplot(1, 1, 1)\n",
        "sns.countplot(x='Label', data=df_normal, order=list(label_mapping.keys()))\n",
        "plt.title('Label Distribution (train_loader - without weighted sampling)')\n",
        "plt.xlabel('Pain Level')\n",
        "plt.ylabel('Count')\n",
        "\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33LHsppfQDW-",
        "outputId": "47481511-d12c-4453-be5d-52234166ffb9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Features batch shape: torch.Size([1024, 5, 38])\n",
            "Labels batch shape: torch.Size([1024])\n"
          ]
        }
      ],
      "source": [
        "# Get one batch from the training data loader\n",
        "for xb, yb in train_loader:\n",
        "    print(\"Features batch shape:\", xb.shape)\n",
        "    print(\"Labels batch shape:\", yb.shape)\n",
        "    break # Stop after getting one batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "Jl4n83GrQDW-"
      },
      "outputs": [],
      "source": [
        "def recurrent_summary(model, input_size):\n",
        "    \"\"\"\n",
        "    Custom summary function that emulates torchinfo's output while correctly\n",
        "    counting parameters for RNN/GRU/LSTM layers.\n",
        "\n",
        "    This function is designed for models whose direct children are\n",
        "    nn.Linear, nn.RNN, nn.GRU, or nn.LSTM layers.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The model to analyze.\n",
        "        input_size (tuple): Shape of the input tensor (e.g., (seq_len, features)).\n",
        "    \"\"\"\n",
        "\n",
        "    # Dictionary to store output shapes captured by forward hooks\n",
        "    output_shapes = {}\n",
        "    # List to track hook handles for later removal\n",
        "    hooks = []\n",
        "\n",
        "    def get_hook(name):\n",
        "        \"\"\"Factory function to create a forward hook for a specific module.\"\"\"\n",
        "        def hook(module, input, output):\n",
        "            # Handle RNN layer outputs (returns a tuple)\n",
        "            if isinstance(output, tuple):\n",
        "                # output[0]: all hidden states with shape (batch, seq_len, hidden*directions)\n",
        "                shape1 = list(output[0].shape)\n",
        "                shape1[0] = -1  # Replace batch dimension with -1\n",
        "\n",
        "                # output[1]: final hidden state h_n (or tuple (h_n, c_n) for LSTM)\n",
        "                if isinstance(output[1], tuple):  # LSTM case: (h_n, c_n)\n",
        "                    shape2 = list(output[1][0].shape)  # Extract h_n only\n",
        "                else:  # RNN/GRU case: h_n only\n",
        "                    shape2 = list(output[1].shape)\n",
        "\n",
        "                # Replace batch dimension (middle position) with -1\n",
        "                shape2[1] = -1\n",
        "\n",
        "                output_shapes[name] = f\"[{shape1}, {shape2}]\"\n",
        "\n",
        "            # Handle standard layer outputs (e.g., Linear)\n",
        "            else:\n",
        "                shape = list(output.shape)\n",
        "                shape[0] = -1  # Replace batch dimension with -1\n",
        "                output_shapes[name] = f\"{shape}\"\n",
        "        return hook\n",
        "\n",
        "    # 1. Determine the device where model parameters reside\n",
        "    try:\n",
        "        device = next(model.parameters()).device\n",
        "    except StopIteration:\n",
        "        device = torch.device(\"cpu\")  # Fallback for models without parameters\n",
        "\n",
        "    # 2. Create a dummy input tensor with batch_size=1\n",
        "    dummy_input = torch.randn(1, *input_size).to(device)\n",
        "\n",
        "    # 3. Register forward hooks on target layers\n",
        "    # Iterate through direct children of the model (e.g., self.rnn, self.classifier)\n",
        "    for name, module in model.named_children():\n",
        "        if isinstance(module, (nn.Linear, nn.RNN, nn.GRU, nn.LSTM)):\n",
        "            # Register the hook and store its handle for cleanup\n",
        "            hook_handle = module.register_forward_hook(get_hook(name))\n",
        "            hooks.append(hook_handle)\n",
        "\n",
        "    # 4. Execute a dummy forward pass in evaluation mode\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        try:\n",
        "            model(dummy_input)\n",
        "        except Exception as e:\n",
        "            print(f\"Error during dummy forward pass: {e}\")\n",
        "            # Clean up hooks even if an error occurs\n",
        "            for h in hooks:\n",
        "                h.remove()\n",
        "            return\n",
        "\n",
        "    # 5. Remove all registered hooks\n",
        "    for h in hooks:\n",
        "        h.remove()\n",
        "\n",
        "    # --- 6. Print the summary table ---\n",
        "\n",
        "    print(\"-\" * 79)\n",
        "    # Column headers\n",
        "    print(f\"{'Layer (type)':<25} {'Output Shape':<28} {'Param #':<18}\")\n",
        "    print(\"=\" * 79)\n",
        "\n",
        "    total_params = 0\n",
        "    total_trainable_params = 0\n",
        "\n",
        "    # Iterate through modules again to collect and display parameter information\n",
        "    for name, module in model.named_children():\n",
        "        if name in output_shapes:\n",
        "            # Count total and trainable parameters for this module\n",
        "            module_params = sum(p.numel() for p in module.parameters())\n",
        "            trainable_params = sum(p.numel() for p in module.parameters() if p.requires_grad)\n",
        "\n",
        "            total_params += module_params\n",
        "            total_trainable_params += trainable_params\n",
        "\n",
        "            # Format strings for display\n",
        "            layer_name = f\"{name} ({type(module).__name__})\"\n",
        "            output_shape_str = str(output_shapes[name])\n",
        "            params_str = f\"{trainable_params:,}\"\n",
        "\n",
        "            print(f\"{layer_name:<25} {output_shape_str:<28} {params_str:<15}\")\n",
        "\n",
        "    print(\"=\" * 79)\n",
        "    print(f\"Total params: {total_params:,}\")\n",
        "    print(f\"Trainable params: {total_trainable_params:,}\")\n",
        "    print(f\"Non-trainable params: {total_params - total_trainable_params:,}\")\n",
        "    print(\"-\" * 79)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5VULB4UQDW_"
      },
      "source": [
        "<a id=\"hyperparameters\"></a>\n",
        "## 7. Network Hyperparameters\n",
        "\n",
        "Configure training settings, architecture parameters, and regularization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "0a183G6zQDW_"
      },
      "outputs": [],
      "source": [
        " # Training configuration\n",
        "LEARNING_RATE = 1e-3\n",
        "EPOCHS = 500\n",
        "PATIENCE = 40\n",
        "\n",
        "# Architecture\n",
        "HIDDEN_LAYERS = 2        # Hidden layers\n",
        "HIDDEN_SIZE = [32,16,32,16]    # Neurons per layer -> prev hidden size = 128\n",
        "\n",
        "# Regularisation\n",
        "DROPOUT_RATE = 0.5     # Dropout probability\n",
        "\n",
        "# For now disable weight decay\n",
        "L1_LAMBDA = 0.0001       # L1 penalty\n",
        "L2_LAMBDA = 0.001         # L2 penalty\n",
        "\n",
        "# Set up loss function and optimizer\n",
        "weights = torch.tensor([0.8, 1.0, 1.5]).to(device)\n",
        "\n",
        "\n",
        "\n",
        "# TO WEIGHT MORE THE \"MORE DIFFICULT\" CASES AND THE LESS FREQUENT LABELS:\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=None, gamma=2.0):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.ce = nn.CrossEntropyLoss(weight=alpha, reduction='none')\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        ce_loss = self.ce(inputs, targets)\n",
        "        pt = torch.exp(-ce_loss)\n",
        "        focal_loss = ((1 - pt) ** self.gamma) * ce_loss\n",
        "        return focal_loss.mean()\n",
        "\n",
        "alpha = None  # None if we don't want to alterate the weights of each label losses (FocalLoss already do it)\n",
        "#criterion = FocalLoss(alpha=alpha, gamma=1.3)  # gamma = 0 it's like Crossentropy(), gamma < 1 it's like in between Crossentropy and FocalLoss,\n",
        "                                               # gamma = 1 it's a good compromise, gamma = 1.5 or gamma = 2 to weight so much the less present labels\n",
        "\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(weight=weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "7-RAXl_BQDXA"
      },
      "outputs": [],
      "source": [
        "# Initialize best model tracking variables\n",
        "best_model = None\n",
        "best_performance = float('-inf')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ouaaN67Ho8R3"
      },
      "source": [
        "<a id=\"model-architecture\"></a>\n",
        "## 8. Model Architecture\n",
        "\n",
        "Custom RNN/LSTM/GRU classifier with configurable bidirectionality and dropout."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4I8vKDBo8R4"
      },
      "source": [
        "### 7.1 Recurrent Classifier Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "6nyxSmz4QDW_"
      },
      "outputs": [],
      "source": [
        "class RecurrentClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    Generic RNN classifier (RNN, LSTM, GRU).\n",
        "    Uses the last hidden state for classification.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "            self,\n",
        "            input_size,\n",
        "            hidden_size,\n",
        "            num_layers,\n",
        "            num_classes,\n",
        "            rnn_type=  'LSTM',        # 'RNN', 'LSTM', or 'GRU'\n",
        "            bidirectional=False,\n",
        "            dropout_rate=0.2\n",
        "            ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.rnn_type = rnn_type\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_size = hidden_size\n",
        "        self.bidirectional = bidirectional\n",
        "\n",
        "        # Map string name to PyTorch RNN class\n",
        "        rnn_map = {\n",
        "            'RNN': nn.RNN,\n",
        "            'LSTM': nn.LSTM,\n",
        "            'GRU': nn.GRU\n",
        "        }\n",
        "\n",
        "        if rnn_type not in rnn_map:\n",
        "            raise ValueError(\"rnn_type must be 'RNN', 'LSTM', or 'GRU'\")\n",
        "\n",
        "        rnn_module = rnn_map[rnn_type]\n",
        "\n",
        "        # Dropout is only applied between layers (if num_layers > 1)\n",
        "        dropout_val = dropout_rate if num_layers > 1 else 0 # dropout between RNN layers, applied for regularization\n",
        "\n",
        "        # Create the recurrent layer\n",
        "        self.rnn = rnn_module(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,       # Input shape: (batch, seq_len, features)\n",
        "            bidirectional=bidirectional, # We are defining a bidirectional RNN since we want to extract also the future contextual information for making better predictions\n",
        "            dropout=dropout_val\n",
        "        )\n",
        "\n",
        "        # Calculate input size for the final classifier\n",
        "        if self.bidirectional:\n",
        "            classifier_input_size = hidden_size * 2 # Concat fwd + bwd\n",
        "        else:\n",
        "            classifier_input_size = hidden_size\n",
        "\n",
        "        # Final classification layer\n",
        "        self.classifier = nn.Linear(classifier_input_size, num_classes) # output layer for classifying\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x shape: (batch_size, seq_length, input_size)\n",
        "        \"\"\"\n",
        "\n",
        "        # rnn_out shape: (batch_size, seq_len, hidden_size * num_directions)\n",
        "        rnn_out, hidden = self.rnn(x) # feeds the input sequence into the RNN layer\n",
        "        # rnn_out -> contains the hidden state output for every timestep\n",
        "\n",
        "        # LSTM returns (h_n, c_n), we only need h_n\n",
        "        if self.rnn_type == 'LSTM':\n",
        "            hidden = hidden[0]  # final hidden state of the last timestep\n",
        "\n",
        "        # hidden shape: (num_layers * num_directions, batch_size, hidden_size)\n",
        "\n",
        "        if self.bidirectional:\n",
        "            # For bidirectional, hidden states are interleaved:\n",
        "            # [layer_0_fwd, layer_0_bwd, layer_1_fwd, layer_1_bwd, ...]\n",
        "            # We want the last layer's forward and backward states\n",
        "            fwd_hidden = hidden[-2, :, :]  # Last layer, forward direction\n",
        "            bwd_hidden = hidden[-1, :, :]  # Last layer, backward direction\n",
        "            hidden_to_classify = torch.cat([fwd_hidden, bwd_hidden], dim=1)\n",
        "        else:\n",
        "            hidden_to_classify = hidden[-1]\n",
        "\n",
        "        # Get logits\n",
        "        logits = self.classifier(hidden_to_classify)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "r82JJturB8f0"
      },
      "outputs": [],
      "source": [
        "class FlexibleRecurrentClassifier(nn.Module):\n",
        "    def __init__(self, input_size, hidden_sizes, num_classes,\n",
        "                 rnn_type='LSTM', bidirectional=False, dropout_rate=0.2,\n",
        "                 use_batch_norm=False):\n",
        "        super().__init__()\n",
        "        assert isinstance(hidden_sizes, (list, tuple)) and len(hidden_sizes) >= 1\n",
        "\n",
        "        self.rnn_type = rnn_type\n",
        "        self.bidirectional = bidirectional\n",
        "        self.num_layers = len(hidden_sizes)\n",
        "        self.use_batch_norm = use_batch_norm\n",
        "\n",
        "        rnn_map = {'RNN': nn.RNN, 'LSTM': nn.LSTM, 'GRU': nn.GRU}\n",
        "        if rnn_type not in rnn_map:\n",
        "            raise ValueError(\"rnn_type must be 'RNN', 'LSTM', or 'GRU'\")\n",
        "\n",
        "        rnn_module = rnn_map[rnn_type]\n",
        "        self.rnns = nn.ModuleList()\n",
        "        self.batch_norms = nn.ModuleList() if use_batch_norm else None\n",
        "\n",
        "        input_dim = input_size\n",
        "        for hidden_dim in hidden_sizes:\n",
        "            self.rnns.append(\n",
        "                rnn_module(\n",
        "                    input_size=input_dim,\n",
        "                    hidden_size=hidden_dim,\n",
        "                    num_layers=1,\n",
        "                    batch_first=True,\n",
        "                    bidirectional=bidirectional,\n",
        "                    dropout=0.0\n",
        "                )\n",
        "            )\n",
        "            output_dim = hidden_dim * (2 if bidirectional else 1)\n",
        "\n",
        "            if use_batch_norm:\n",
        "                self.batch_norms.append(nn.BatchNorm1d(output_dim))\n",
        "\n",
        "            input_dim = output_dim\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.classifier = nn.Linear(input_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: (batch_size, seq_len, input_size)\n",
        "        Returns:\n",
        "            logits: (batch_size, num_classes)\n",
        "        \"\"\"\n",
        "        out = x\n",
        "\n",
        "        for i, rnn in enumerate(self.rnns):\n",
        "            out, hidden = rnn(out)\n",
        "\n",
        "            if self.use_batch_norm:\n",
        "                # (batch, seq, features) -> (batch, features, seq)\n",
        "                out = out.transpose(1, 2)\n",
        "                out = self.batch_norms[i](out)\n",
        "                out = out.transpose(1, 2)\n",
        "\n",
        "            out = self.dropout(out)\n",
        "\n",
        "        # Use final timestep output (more common than hidden state)\n",
        "        final_output = out[:, -1, :]  # (batch, hidden_dim * num_directions)\n",
        "\n",
        "        logits = self.classifier(final_output)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLdCrp-qQDXA"
      },
      "source": [
        "<a id=\"training-functions\"></a>\n",
        "## 9. Training Functions\n",
        "\n",
        "Helper functions for training, validation, logging, and early stopping."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "7QgHx0BHQDXA"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(model, train_loader, criterion, optimizer, scaler, device, l1_lambda=0, l2_lambda=0):\n",
        "    \"\"\"\n",
        "    Perform one complete training epoch through the entire training dataset.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The neural network model to train\n",
        "        train_loader (DataLoader): PyTorch DataLoader containing training data batches\n",
        "        criterion (nn.Module): Loss function (e.g., CrossEntropyLoss, MSELoss)\n",
        "        optimizer (torch.optim): Optimization algorithm (e.g., Adam, SGD)\n",
        "        scaler (GradScaler): PyTorch's gradient scaler for mixed precision training\n",
        "        device (torch.device): Computing device ('cuda' for GPU, 'cpu' for CPU)\n",
        "        l1_lambda (float): Lambda for L1 regularization\n",
        "        l2_lambda (float): Lambda for L2 regularization\n",
        "\n",
        "    Returns:\n",
        "        tuple: (average_loss, f1 score) - Training loss and f1 score for this epoch\n",
        "    \"\"\"\n",
        "    model.train()  # Set model to training mode\n",
        "\n",
        "    running_loss = 0.0\n",
        "    all_predictions = []\n",
        "    all_targets = []\n",
        "\n",
        "    # Iterate through training batches\n",
        "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "        # Move data to device (GPU/CPU)\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        # Clear gradients from previous step\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        # Forward pass with mixed precision (if CUDA available)\n",
        "        with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n",
        "            logits = model(inputs)\n",
        "            loss = criterion(logits, targets)\n",
        "\n",
        "            # Add L1 and L2 regularization\n",
        "            l1_norm = sum(p.abs().sum() for p in model.parameters())\n",
        "            l2_norm = sum(p.pow(2).sum() for p in model.parameters())\n",
        "            loss = loss + l1_lambda * l1_norm + l2_lambda * l2_norm\n",
        "\n",
        "\n",
        "        # Backward pass with gradient scaling\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        # Accumulate metrics\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        predictions = logits.argmax(dim=1)\n",
        "        all_predictions.append(predictions.cpu().numpy())\n",
        "        all_targets.append(targets.cpu().numpy())\n",
        "\n",
        "    # Calculate epoch metrics\n",
        "    epoch_loss = running_loss / len(train_loader.dataset)\n",
        "    epoch_f1 = f1_score(\n",
        "        np.concatenate(all_targets),\n",
        "        np.concatenate(all_predictions),\n",
        "        average='weighted'\n",
        "    )\n",
        "\n",
        "    return epoch_loss, epoch_f1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKsdfMrSo8R4"
      },
      "source": [
        "### 9.1 Train One Epoch Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "eVlcqfzqQDXA"
      },
      "outputs": [],
      "source": [
        "def validate_one_epoch(model, val_loader, criterion, device):\n",
        "    \"\"\"\n",
        "    Perform one complete validation epoch through the entire validation dataset.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The neural network model to evaluate (must be in eval mode)\n",
        "        val_loader (DataLoader): PyTorch DataLoader containing validation data batches\n",
        "        criterion (nn.Module): Loss function used to calculate validation loss\n",
        "        device (torch.device): Computing device ('cuda' for GPU, 'cpu' for CPU)\n",
        "\n",
        "    Returns:\n",
        "        tuple: (average_loss, accuracy) - Validation loss and accuracy for this epoch\n",
        "\n",
        "    Note:\n",
        "        This function automatically sets the model to evaluation mode and disables\n",
        "        gradient computation for efficiency during validation.\n",
        "    \"\"\"\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "\n",
        "    running_loss = 0.0\n",
        "    all_predictions = []\n",
        "    all_targets = []\n",
        "\n",
        "    # Disable gradient computation for validation\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in val_loader:\n",
        "            # Move data to device\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            # Forward pass with mixed precision (if CUDA available)\n",
        "            with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n",
        "                logits = model(inputs)\n",
        "                loss = criterion(logits, targets)\n",
        "\n",
        "            # Accumulate metrics\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            predictions = logits.argmax(dim=1)\n",
        "            all_predictions.append(predictions.cpu().numpy())\n",
        "            all_targets.append(targets.cpu().numpy())\n",
        "\n",
        "    # Calculate epoch metrics\n",
        "    epoch_loss = running_loss / len(val_loader.dataset)\n",
        "    epoch_accuracy = f1_score(\n",
        "        np.concatenate(all_targets),\n",
        "        np.concatenate(all_predictions),\n",
        "        average='weighted'\n",
        "    )\n",
        "\n",
        "    return epoch_loss, epoch_accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAGFlt3Fo8R4"
      },
      "source": [
        "### 9.2 Validate One Epoch Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "dZwJNmCZQDXA"
      },
      "outputs": [],
      "source": [
        "def log_metrics_to_tensorboard(writer, epoch, train_loss, train_f1, val_loss, val_f1, model):\n",
        "    \"\"\"\n",
        "    Log training metrics and model parameters to TensorBoard for visualization.\n",
        "\n",
        "    Args:\n",
        "        writer (SummaryWriter): TensorBoard SummaryWriter object for logging\n",
        "        epoch (int): Current epoch number (used as x-axis in TensorBoard plots)\n",
        "        train_loss (float): Training loss for this epoch\n",
        "        train_f1 (float): Training f1 score for this epoch\n",
        "        val_loss (float): Validation loss for this epoch\n",
        "        val_f1 (float): Validation f1 score for this epoch\n",
        "        model (nn.Module): The neural network model (for logging weights/gradients)\n",
        "\n",
        "    Note:\n",
        "        This function logs scalar metrics (loss/f1 score) and histograms of model\n",
        "        parameters and gradients, which helps monitor training progress and detect\n",
        "        issues like vanishing/exploding gradients.\n",
        "    \"\"\"\n",
        "    # Log scalar metrics\n",
        "    writer.add_scalar('Loss/Training', train_loss, epoch)\n",
        "    writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
        "    writer.add_scalar('F1/Training', train_f1, epoch)\n",
        "    writer.add_scalar('F1/Validation', val_f1, epoch)\n",
        "\n",
        "    # Log model parameters and gradients\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.requires_grad:\n",
        "            # Check if the tensor is not empty before adding a histogram\n",
        "            if param.numel() > 0:\n",
        "                writer.add_histogram(f'{name}/weights', param.data, epoch)\n",
        "            if param.grad is not None:\n",
        "                # Check if the gradient tensor is not empty before adding a histogram\n",
        "                if param.grad.numel() > 0:\n",
        "                    if param.grad is not None and torch.isfinite(param.grad).all():\n",
        "                        writer.add_histogram(f'{name}/gradients', param.grad.data, epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOa3T9Pho8R4"
      },
      "source": [
        "### 9.3 Fit  Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "KgpOY62qQDXB"
      },
      "outputs": [],
      "source": [
        "def fit(model, train_loader, val_loader, epochs, criterion, optimizer, scaler, device,\n",
        "        l1_lambda=0, l2_lambda=0, patience=0, evaluation_metric=\"val_f1\", mode='max',\n",
        "        restore_best_weights=True, writer=None, verbose=10, experiment_name=\"\"):\n",
        "    \"\"\"\n",
        "    Train the neural network model on the training data and validate on the validation data.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The neural network model to train\n",
        "        train_loader (DataLoader): PyTorch DataLoader containing training data batches\n",
        "        val_loader (DataLoader): PyTorch DataLoader containing validation data batches\n",
        "        epochs (int): Number of training epochs\n",
        "        criterion (nn.Module): Loss function (e.g., CrossEntropyLoss, MSELoss)\n",
        "        optimizer (torch.optim): Optimization algorithm (e.g., Adam, SGD)\n",
        "        scaler (GradScaler): PyTorch's gradient scaler for mixed precision training\n",
        "        device (torch.device): Computing device ('cuda' for GPU, 'cpu' for CPU)\n",
        "        l1_lambda (float): L1 regularization coefficient (default: 0)\n",
        "        l2_lambda (float): L2 regularization coefficient (default: 0)\n",
        "        patience (int): Number of epochs to wait for improvement before early stopping (default: 0)\n",
        "        evaluation_metric (str): Metric to monitor for early stopping (default: \"val_f1\")\n",
        "        mode (str): 'max' for maximizing the metric, 'min' for minimizing (default: 'max')\n",
        "        restore_best_weights (bool): Whether to restore model weights from best epoch (default: True)\n",
        "        writer (SummaryWriter, optional): TensorBoard SummaryWriter object for logging (default: None)\n",
        "        verbose (int, optional): Frequency of printing training progress (default: 10)\n",
        "        experiment_name (str, optional): Experiment name for saving models (default: \"\")\n",
        "\n",
        "    Returns:\n",
        "        tuple: (model, training_history) - Trained model and metrics history\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize metrics tracking\n",
        "    training_history = {\n",
        "        'train_loss': [], 'val_loss': [],\n",
        "        'train_f1': [], 'val_f1': []\n",
        "    }\n",
        "\n",
        "    # Configure early stopping if patience is set\n",
        "    if patience > 0:\n",
        "        patience_counter = 0\n",
        "        best_metric = float('-inf') if mode == 'max' else float('inf')\n",
        "        best_epoch = 0\n",
        "\n",
        "    print(f\"Training {epochs} epochs...\")\n",
        "\n",
        "    # Main training loop: iterate through epochs\n",
        "    for epoch in range(1, epochs + 1):\n",
        "\n",
        "        # Forward pass through training data, compute gradients, update weights\n",
        "        train_loss, train_f1 = train_one_epoch(\n",
        "            model, train_loader, criterion, optimizer, scaler, device, l1_lambda, l2_lambda\n",
        "        )\n",
        "\n",
        "        # Evaluate model on validation data without updating weights\n",
        "        val_loss, val_f1 = validate_one_epoch(\n",
        "            model, val_loader, criterion, device\n",
        "        )\n",
        "\n",
        "        # Store metrics for plotting and analysis\n",
        "        training_history['train_loss'].append(train_loss)\n",
        "        training_history['val_loss'].append(val_loss)\n",
        "        training_history['train_f1'].append(train_f1)\n",
        "        training_history['val_f1'].append(val_f1)\n",
        "\n",
        "        # Write metrics to TensorBoard for visualization\n",
        "        if writer is not None:\n",
        "            log_metrics_to_tensorboard(\n",
        "                writer, epoch, train_loss, train_f1, val_loss, val_f1, model\n",
        "            )\n",
        "\n",
        "        # Print progress every N epochs or on first epoch\n",
        "        if verbose > 0:\n",
        "            if epoch % verbose == 0 or epoch == 1:\n",
        "                print(f\"Epoch {epoch:3d}/{epochs} | \"\n",
        "                    f\"Train: Loss={train_loss:.4f}, F1 Score={train_f1:.4f} | \"\n",
        "                    f\"Val: Loss={val_loss:.4f}, F1 Score={val_f1:.4f}\")\n",
        "\n",
        "        # Early stopping logic: monitor metric and save best model\n",
        "        if patience > 0:\n",
        "            current_metric = training_history[evaluation_metric][-1]\n",
        "            is_improvement = (current_metric > best_metric) if mode == 'max' else (current_metric < best_metric)\n",
        "\n",
        "            if is_improvement:\n",
        "                best_metric = current_metric\n",
        "                best_f1 = val_f1\n",
        "                best_epoch = epoch\n",
        "                torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
        "                patience_counter = 0\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "                if patience_counter >= patience:\n",
        "                    print(f\"Early stopping triggered after {epoch} epochs.\")\n",
        "                    break\n",
        "\n",
        "    # Restore best model weights if early stopping was used\n",
        "    if restore_best_weights and patience > 0:\n",
        "        model.load_state_dict(torch.load(MODEL_SAVE_PATH))\n",
        "        print(f\"Best model restored from epoch {best_epoch} with {evaluation_metric} {best_metric:.4f} and with val_f1 {best_f1:.4f}\")\n",
        "\n",
        "    # Save final model if no early stopping\n",
        "    if patience == 0:\n",
        "        torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
        "\n",
        "    # Close TensorBoard writer\n",
        "    if writer is not None:\n",
        "        writer.close()\n",
        "\n",
        "    return model, training_history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8g2Pr_To8R5"
      },
      "source": [
        "### 9.4 Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00tkWSbCQDXB",
        "outputId": "5b685d9b-5907-4e5e-da65-f25f0c90bafe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------------------------------------------------------------------------\n",
            "Layer (type)              Output Shape                 Param #           \n",
            "===============================================================================\n",
            "classifier (Linear)       [-1, 3]                      99             \n",
            "===============================================================================\n",
            "Total params: 99\n",
            "Trainable params: 99\n",
            "Non-trainable params: 0\n",
            "-------------------------------------------------------------------------------\n",
            "Training 500 epochs...\n",
            "Epoch   1/500 | Train: Loss=1.4347, F1 Score=0.5351 | Val: Loss=0.8514, F1 Score=0.6768\n",
            "Epoch   2/500 | Train: Loss=1.1535, F1 Score=0.5419 | Val: Loss=0.8399, F1 Score=0.6768\n",
            "Epoch   3/500 | Train: Loss=0.9965, F1 Score=0.5974 | Val: Loss=0.7422, F1 Score=0.7191\n",
            "Epoch   4/500 | Train: Loss=0.9074, F1 Score=0.6281 | Val: Loss=0.7348, F1 Score=0.7199\n",
            "Epoch   5/500 | Train: Loss=0.8706, F1 Score=0.6556 | Val: Loss=0.6748, F1 Score=0.7683\n",
            "Epoch   6/500 | Train: Loss=0.8380, F1 Score=0.7027 | Val: Loss=0.6552, F1 Score=0.7534\n",
            "Epoch   7/500 | Train: Loss=0.8135, F1 Score=0.7026 | Val: Loss=0.6471, F1 Score=0.7469\n",
            "Epoch   8/500 | Train: Loss=0.8125, F1 Score=0.6953 | Val: Loss=0.6933, F1 Score=0.7344\n",
            "Epoch   9/500 | Train: Loss=0.8044, F1 Score=0.6967 | Val: Loss=0.6422, F1 Score=0.7499\n",
            "Epoch  10/500 | Train: Loss=0.7960, F1 Score=0.7005 | Val: Loss=0.6380, F1 Score=0.7535\n",
            "Epoch  11/500 | Train: Loss=0.7906, F1 Score=0.7071 | Val: Loss=0.6322, F1 Score=0.7511\n",
            "Epoch  12/500 | Train: Loss=0.7903, F1 Score=0.7043 | Val: Loss=0.6305, F1 Score=0.8127\n",
            "Epoch  13/500 | Train: Loss=0.7829, F1 Score=0.7133 | Val: Loss=0.6244, F1 Score=0.7504\n",
            "Epoch  14/500 | Train: Loss=0.7860, F1 Score=0.7087 | Val: Loss=0.6116, F1 Score=0.8039\n",
            "Epoch  15/500 | Train: Loss=0.7605, F1 Score=0.7118 | Val: Loss=0.5161, F1 Score=0.8127\n",
            "Epoch  16/500 | Train: Loss=0.7342, F1 Score=0.7298 | Val: Loss=0.4766, F1 Score=0.8658\n",
            "Epoch  17/500 | Train: Loss=0.7191, F1 Score=0.7388 | Val: Loss=0.4574, F1 Score=0.8680\n",
            "Epoch  18/500 | Train: Loss=0.6968, F1 Score=0.7484 | Val: Loss=0.4340, F1 Score=0.8683\n",
            "Epoch  19/500 | Train: Loss=0.6975, F1 Score=0.7465 | Val: Loss=0.4331, F1 Score=0.8651\n",
            "Epoch  20/500 | Train: Loss=0.6787, F1 Score=0.7563 | Val: Loss=0.4473, F1 Score=0.8543\n",
            "Epoch  21/500 | Train: Loss=0.6597, F1 Score=0.7577 | Val: Loss=0.4517, F1 Score=0.8503\n",
            "Epoch  22/500 | Train: Loss=0.6544, F1 Score=0.7645 | Val: Loss=0.4427, F1 Score=0.8615\n",
            "Epoch  23/500 | Train: Loss=0.6499, F1 Score=0.7628 | Val: Loss=0.4645, F1 Score=0.8575\n",
            "Epoch  24/500 | Train: Loss=0.6401, F1 Score=0.7648 | Val: Loss=0.4342, F1 Score=0.8648\n",
            "Epoch  25/500 | Train: Loss=0.6308, F1 Score=0.7645 | Val: Loss=0.4454, F1 Score=0.8761\n",
            "Epoch  26/500 | Train: Loss=0.6208, F1 Score=0.7668 | Val: Loss=0.4504, F1 Score=0.8480\n",
            "Epoch  27/500 | Train: Loss=0.6157, F1 Score=0.7706 | Val: Loss=0.4401, F1 Score=0.8750\n",
            "Epoch  28/500 | Train: Loss=0.6122, F1 Score=0.7655 | Val: Loss=0.4724, F1 Score=0.8697\n",
            "Epoch  29/500 | Train: Loss=0.6155, F1 Score=0.7660 | Val: Loss=0.4552, F1 Score=0.8578\n",
            "Epoch  30/500 | Train: Loss=0.6091, F1 Score=0.7690 | Val: Loss=0.4403, F1 Score=0.8718\n",
            "Epoch  31/500 | Train: Loss=0.6049, F1 Score=0.7676 | Val: Loss=0.4474, F1 Score=0.8469\n",
            "Epoch  32/500 | Train: Loss=0.6063, F1 Score=0.7692 | Val: Loss=0.4540, F1 Score=0.8723\n",
            "Epoch  33/500 | Train: Loss=0.5971, F1 Score=0.7715 | Val: Loss=0.4677, F1 Score=0.8692\n",
            "Epoch  34/500 | Train: Loss=0.5953, F1 Score=0.7732 | Val: Loss=0.4513, F1 Score=0.8681\n",
            "Epoch  35/500 | Train: Loss=0.5915, F1 Score=0.7754 | Val: Loss=0.4552, F1 Score=0.8697\n",
            "Epoch  36/500 | Train: Loss=0.5900, F1 Score=0.7824 | Val: Loss=0.4574, F1 Score=0.8635\n",
            "Epoch  37/500 | Train: Loss=0.5908, F1 Score=0.7830 | Val: Loss=0.4411, F1 Score=0.8656\n",
            "Epoch  38/500 | Train: Loss=0.5864, F1 Score=0.7828 | Val: Loss=0.4402, F1 Score=0.8604\n",
            "Epoch  39/500 | Train: Loss=0.5823, F1 Score=0.7869 | Val: Loss=0.4421, F1 Score=0.8664\n",
            "Epoch  40/500 | Train: Loss=0.5747, F1 Score=0.7891 | Val: Loss=0.4414, F1 Score=0.8592\n",
            "Epoch  41/500 | Train: Loss=0.5749, F1 Score=0.7880 | Val: Loss=0.4460, F1 Score=0.8636\n",
            "Epoch  42/500 | Train: Loss=0.5723, F1 Score=0.7915 | Val: Loss=0.4364, F1 Score=0.8590\n",
            "Epoch  43/500 | Train: Loss=0.5781, F1 Score=0.7933 | Val: Loss=0.4278, F1 Score=0.8625\n",
            "Epoch  44/500 | Train: Loss=0.5619, F1 Score=0.8006 | Val: Loss=0.4214, F1 Score=0.8675\n",
            "Epoch  45/500 | Train: Loss=0.5575, F1 Score=0.8041 | Val: Loss=0.4329, F1 Score=0.8676\n",
            "Epoch  46/500 | Train: Loss=0.5573, F1 Score=0.8072 | Val: Loss=0.4191, F1 Score=0.8687\n",
            "Epoch  47/500 | Train: Loss=0.5575, F1 Score=0.8098 | Val: Loss=0.4186, F1 Score=0.8707\n",
            "Epoch  48/500 | Train: Loss=0.5517, F1 Score=0.8156 | Val: Loss=0.4134, F1 Score=0.8617\n",
            "Epoch  49/500 | Train: Loss=0.5469, F1 Score=0.8208 | Val: Loss=0.4155, F1 Score=0.8643\n",
            "Epoch  50/500 | Train: Loss=0.5434, F1 Score=0.8223 | Val: Loss=0.4060, F1 Score=0.8736\n",
            "Epoch  51/500 | Train: Loss=0.5382, F1 Score=0.8247 | Val: Loss=0.4095, F1 Score=0.8747\n",
            "Epoch  52/500 | Train: Loss=0.5346, F1 Score=0.8272 | Val: Loss=0.3961, F1 Score=0.8779\n",
            "Epoch  53/500 | Train: Loss=0.5342, F1 Score=0.8307 | Val: Loss=0.4005, F1 Score=0.8735\n",
            "Epoch  54/500 | Train: Loss=0.5297, F1 Score=0.8332 | Val: Loss=0.4031, F1 Score=0.8740\n",
            "Epoch  55/500 | Train: Loss=0.5293, F1 Score=0.8339 | Val: Loss=0.4289, F1 Score=0.8533\n",
            "Epoch  56/500 | Train: Loss=0.5272, F1 Score=0.8363 | Val: Loss=0.3808, F1 Score=0.8821\n",
            "Epoch  57/500 | Train: Loss=0.5211, F1 Score=0.8400 | Val: Loss=0.4016, F1 Score=0.8788\n",
            "Epoch  58/500 | Train: Loss=0.5173, F1 Score=0.8436 | Val: Loss=0.4093, F1 Score=0.8709\n",
            "Epoch  59/500 | Train: Loss=0.5166, F1 Score=0.8465 | Val: Loss=0.3845, F1 Score=0.8865\n",
            "Epoch  60/500 | Train: Loss=0.5098, F1 Score=0.8503 | Val: Loss=0.3783, F1 Score=0.8838\n",
            "Epoch  61/500 | Train: Loss=0.5104, F1 Score=0.8504 | Val: Loss=0.3745, F1 Score=0.8894\n",
            "Epoch  62/500 | Train: Loss=0.5056, F1 Score=0.8513 | Val: Loss=0.3934, F1 Score=0.8809\n",
            "Epoch  63/500 | Train: Loss=0.5049, F1 Score=0.8538 | Val: Loss=0.3765, F1 Score=0.8829\n",
            "Epoch  64/500 | Train: Loss=0.4958, F1 Score=0.8573 | Val: Loss=0.3624, F1 Score=0.8909\n",
            "Epoch  65/500 | Train: Loss=0.4914, F1 Score=0.8604 | Val: Loss=0.3684, F1 Score=0.8868\n",
            "Epoch  66/500 | Train: Loss=0.4920, F1 Score=0.8612 | Val: Loss=0.3793, F1 Score=0.8917\n",
            "Epoch  67/500 | Train: Loss=0.4898, F1 Score=0.8636 | Val: Loss=0.3790, F1 Score=0.8820\n",
            "Epoch  68/500 | Train: Loss=0.4871, F1 Score=0.8635 | Val: Loss=0.3723, F1 Score=0.8910\n",
            "Epoch  69/500 | Train: Loss=0.4814, F1 Score=0.8659 | Val: Loss=0.3666, F1 Score=0.8914\n",
            "Epoch  70/500 | Train: Loss=0.4774, F1 Score=0.8682 | Val: Loss=0.3912, F1 Score=0.8887\n",
            "Epoch  71/500 | Train: Loss=0.4797, F1 Score=0.8671 | Val: Loss=0.3881, F1 Score=0.8846\n",
            "Epoch  72/500 | Train: Loss=0.4757, F1 Score=0.8702 | Val: Loss=0.3709, F1 Score=0.8936\n",
            "Epoch  73/500 | Train: Loss=0.4711, F1 Score=0.8706 | Val: Loss=0.3750, F1 Score=0.8926\n",
            "Epoch  74/500 | Train: Loss=0.4722, F1 Score=0.8720 | Val: Loss=0.3578, F1 Score=0.8940\n",
            "Epoch  75/500 | Train: Loss=0.4745, F1 Score=0.8712 | Val: Loss=0.3625, F1 Score=0.8961\n",
            "Epoch  76/500 | Train: Loss=0.4617, F1 Score=0.8753 | Val: Loss=0.3723, F1 Score=0.8847\n",
            "Epoch  77/500 | Train: Loss=0.4648, F1 Score=0.8753 | Val: Loss=0.3692, F1 Score=0.8924\n",
            "Epoch  78/500 | Train: Loss=0.4607, F1 Score=0.8752 | Val: Loss=0.3667, F1 Score=0.8890\n",
            "Epoch  79/500 | Train: Loss=0.4531, F1 Score=0.8800 | Val: Loss=0.3512, F1 Score=0.8941\n",
            "Epoch  80/500 | Train: Loss=0.4504, F1 Score=0.8798 | Val: Loss=0.3658, F1 Score=0.8963\n",
            "Epoch  81/500 | Train: Loss=0.4501, F1 Score=0.8816 | Val: Loss=0.3628, F1 Score=0.8980\n",
            "Epoch  82/500 | Train: Loss=0.4560, F1 Score=0.8774 | Val: Loss=0.3701, F1 Score=0.8872\n",
            "Epoch  83/500 | Train: Loss=0.4538, F1 Score=0.8802 | Val: Loss=0.3546, F1 Score=0.8974\n",
            "Epoch  84/500 | Train: Loss=0.4503, F1 Score=0.8807 | Val: Loss=0.3672, F1 Score=0.8947\n",
            "Epoch  85/500 | Train: Loss=0.4504, F1 Score=0.8801 | Val: Loss=0.3728, F1 Score=0.8946\n",
            "Epoch  86/500 | Train: Loss=0.4407, F1 Score=0.8853 | Val: Loss=0.3556, F1 Score=0.8935\n",
            "Epoch  87/500 | Train: Loss=0.4487, F1 Score=0.8803 | Val: Loss=0.3921, F1 Score=0.8832\n",
            "Epoch  88/500 | Train: Loss=0.4467, F1 Score=0.8827 | Val: Loss=0.3706, F1 Score=0.8970\n",
            "Epoch  89/500 | Train: Loss=0.4350, F1 Score=0.8866 | Val: Loss=0.3705, F1 Score=0.8944\n",
            "Epoch  90/500 | Train: Loss=0.4416, F1 Score=0.8846 | Val: Loss=0.3669, F1 Score=0.8891\n",
            "Epoch  91/500 | Train: Loss=0.4406, F1 Score=0.8835 | Val: Loss=0.3688, F1 Score=0.8956\n",
            "Epoch  92/500 | Train: Loss=0.4363, F1 Score=0.8865 | Val: Loss=0.3565, F1 Score=0.8966\n",
            "Epoch  93/500 | Train: Loss=0.4303, F1 Score=0.8893 | Val: Loss=0.3977, F1 Score=0.8873\n",
            "Epoch  94/500 | Train: Loss=0.4372, F1 Score=0.8863 | Val: Loss=0.3663, F1 Score=0.8966\n",
            "Epoch  95/500 | Train: Loss=0.4341, F1 Score=0.8873 | Val: Loss=0.3660, F1 Score=0.8907\n",
            "Epoch  96/500 | Train: Loss=0.4278, F1 Score=0.8905 | Val: Loss=0.3790, F1 Score=0.8860\n",
            "Epoch  97/500 | Train: Loss=0.4249, F1 Score=0.8909 | Val: Loss=0.3879, F1 Score=0.8845\n",
            "Epoch  98/500 | Train: Loss=0.4261, F1 Score=0.8897 | Val: Loss=0.3694, F1 Score=0.8919\n",
            "Epoch  99/500 | Train: Loss=0.4327, F1 Score=0.8886 | Val: Loss=0.3734, F1 Score=0.8877\n",
            "Epoch 100/500 | Train: Loss=0.4219, F1 Score=0.8924 | Val: Loss=0.4158, F1 Score=0.8728\n",
            "Epoch 101/500 | Train: Loss=0.4231, F1 Score=0.8916 | Val: Loss=0.3777, F1 Score=0.8915\n",
            "Epoch 102/500 | Train: Loss=0.4267, F1 Score=0.8912 | Val: Loss=0.3738, F1 Score=0.8968\n",
            "Epoch 103/500 | Train: Loss=0.4219, F1 Score=0.8922 | Val: Loss=0.3742, F1 Score=0.8933\n",
            "Epoch 104/500 | Train: Loss=0.4205, F1 Score=0.8921 | Val: Loss=0.3895, F1 Score=0.8939\n",
            "Epoch 105/500 | Train: Loss=0.4150, F1 Score=0.8931 | Val: Loss=0.3866, F1 Score=0.8976\n",
            "Epoch 106/500 | Train: Loss=0.4212, F1 Score=0.8920 | Val: Loss=0.3846, F1 Score=0.8983\n",
            "Epoch 107/500 | Train: Loss=0.4149, F1 Score=0.8941 | Val: Loss=0.3824, F1 Score=0.8944\n",
            "Epoch 108/500 | Train: Loss=0.4119, F1 Score=0.8947 | Val: Loss=0.3946, F1 Score=0.8826\n",
            "Epoch 109/500 | Train: Loss=0.4093, F1 Score=0.8977 | Val: Loss=0.3926, F1 Score=0.8962\n",
            "Epoch 110/500 | Train: Loss=0.4133, F1 Score=0.8932 | Val: Loss=0.3967, F1 Score=0.8893\n",
            "Epoch 111/500 | Train: Loss=0.4042, F1 Score=0.8983 | Val: Loss=0.3890, F1 Score=0.8894\n",
            "Epoch 112/500 | Train: Loss=0.4077, F1 Score=0.8964 | Val: Loss=0.4169, F1 Score=0.8889\n",
            "Epoch 113/500 | Train: Loss=0.4101, F1 Score=0.8958 | Val: Loss=0.3842, F1 Score=0.8986\n",
            "Epoch 114/500 | Train: Loss=0.4070, F1 Score=0.8946 | Val: Loss=0.3786, F1 Score=0.8986\n",
            "Epoch 115/500 | Train: Loss=0.4110, F1 Score=0.8949 | Val: Loss=0.3838, F1 Score=0.8948\n",
            "Epoch 116/500 | Train: Loss=0.4086, F1 Score=0.8955 | Val: Loss=0.3959, F1 Score=0.8928\n",
            "Epoch 117/500 | Train: Loss=0.4013, F1 Score=0.8979 | Val: Loss=0.3820, F1 Score=0.8963\n",
            "Epoch 118/500 | Train: Loss=0.3984, F1 Score=0.8989 | Val: Loss=0.4198, F1 Score=0.8925\n",
            "Epoch 119/500 | Train: Loss=0.3980, F1 Score=0.8993 | Val: Loss=0.4046, F1 Score=0.8877\n",
            "Early stopping triggered after 119 epochs.\n",
            "Best model restored from epoch 79 with val_loss 0.3512 and with val_f1 0.8941\n"
          ]
        }
      ],
      "source": [
        "# Create model and display architecture with parameter count\n",
        "#rnn_model = RecurrentClassifier(\n",
        "#    input_size=input_shape[-1], # Pass the number of features\n",
        "#    hidden_size=HIDDEN_SIZE,\n",
        "#    num_layers=HIDDEN_LAYERS,\n",
        "#    num_classes=num_classes,\n",
        "#    dropout_rate=DROPOUT_RATE,\n",
        "#    bidirectional=BIDIRECTIONAL,\n",
        "#    rnn_type=RNN_TYPE\n",
        "#    ).to(device)\n",
        "rnn_model = FlexibleRecurrentClassifier(\n",
        "    input_size=input_shape[-1], # Pass the number of features\n",
        "    hidden_sizes=HIDDEN_SIZE,\n",
        "    num_classes=num_classes,\n",
        "    dropout_rate=DROPOUT_RATE,\n",
        "    bidirectional=BIDIRECTIONAL,\n",
        "    rnn_type=RNN_TYPE\n",
        "    ).to(device)\n",
        "recurrent_summary(rnn_model, input_size=input_shape)\n",
        "\n",
        "# Set up TensorBoard logging and save model architecture\n",
        "writer = SummaryWriter(f\"./{logs_dir}/{EXPERIMENT_NAME}\")\n",
        "x = torch.randn(1, input_shape[0], input_shape[1]).to(device)\n",
        "writer.add_graph(rnn_model, x)\n",
        "\n",
        "# Define optimizer with L2 regularization\n",
        "optimizer = torch.optim.AdamW(rnn_model.parameters(), lr=LEARNING_RATE, weight_decay=L2_LAMBDA)\n",
        "\n",
        "# Enable mixed precision training for GPU acceleration\n",
        "scaler = torch.amp.GradScaler(enabled=(device.type == 'cuda'))\n",
        "\n",
        "# Train model and track training history\n",
        "rnn_model, training_history = fit(\n",
        "    model=rnn_model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    l1_lambda=L1_LAMBDA,\n",
        "    epochs=EPOCHS,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer,\n",
        "    scaler=scaler,\n",
        "    device=device,\n",
        "    writer=None,\n",
        "    verbose=1,\n",
        "    experiment_name=EXPERIMENT_NAME,\n",
        "    patience=PATIENCE,\n",
        "    evaluation_metric=\"val_loss\",\n",
        "    mode= 'min'\n",
        "\n",
        "    )\n",
        "\n",
        "# Update best model if current performance is superior\n",
        "if training_history['val_f1'][-1] > best_performance:\n",
        "    best_model = rnn_model\n",
        "    best_performance = training_history['val_f1'][-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "aK2k-TrcWKz7"
      },
      "outputs": [],
      "source": [
        "# @title Plot History\n",
        "# Create a figure with two side-by-side subplots (two columns)\n",
        "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(18, 5))\n",
        "\n",
        "# Plot of training and validation loss on the first axis\n",
        "ax1.plot(training_history['train_loss'], label='Training loss', alpha=0.3, color='#ff7f0e', linestyle='--')\n",
        "ax1.plot(training_history['val_loss'], label='Validation loss', alpha=0.9, color='#ff7f0e')\n",
        "ax1.set_title('Loss')\n",
        "ax1.legend()\n",
        "ax1.grid(alpha=0.3)\n",
        "\n",
        "# Plot of training and validation accuracy on the second axis\n",
        "ax2.plot(training_history['train_f1'], label='Training f1', alpha=0.3, color='#ff7f0e', linestyle='--')\n",
        "ax2.plot(training_history['val_f1'], label='Validation f1', alpha=0.9, color='#ff7f0e')\n",
        "ax2.set_title('F1 Score')\n",
        "ax2.legend()\n",
        "ax2.grid(alpha=0.3)\n",
        "\n",
        "# Adjust the layout and display the plot\n",
        "plt.tight_layout()\n",
        "plt.subplots_adjust(right=0.85)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "BnkLnkjZYGq2"
      },
      "outputs": [],
      "source": [
        "# @title Plot Confusion Matrix\n",
        "# Collect predictions and ground truth labels\n",
        "val_preds, val_targets = [], []\n",
        "with torch.no_grad():  # Disable gradient computation for inference\n",
        "    for xb, yb in val_loader:\n",
        "        xb = xb.to(device)\n",
        "\n",
        "        # Forward pass: get model predictions\n",
        "        logits = rnn_model(xb)\n",
        "        preds = logits.argmax(dim=1).cpu().numpy()\n",
        "\n",
        "        # Store batch results\n",
        "        val_preds.append(preds)\n",
        "        val_targets.append(yb.numpy())\n",
        "\n",
        "# Combine all batches into single arrays\n",
        "val_preds = np.concatenate(val_preds)\n",
        "val_targets = np.concatenate(val_targets)\n",
        "\n",
        "# Calculate overall validation metrics\n",
        "val_acc = accuracy_score(val_targets, val_preds)\n",
        "val_prec = precision_score(val_targets, val_preds, average='weighted')\n",
        "val_rec = recall_score(val_targets, val_preds, average='weighted')\n",
        "val_f1 = f1_score(val_targets, val_preds, average='weighted')\n",
        "print(f\"Accuracy over the validation set: {val_acc:.4f}\")\n",
        "print(f\"Precision over the validation set: {val_prec:.4f}\")\n",
        "print(f\"Recall over the validation set: {val_rec:.4f}\")\n",
        "print(f\"F1 score over the validation set: {val_f1:.4f}\")\n",
        "\n",
        "# Generate confusion matrix for detailed error analysis\n",
        "cm = confusion_matrix(val_targets, val_preds)\n",
        "\n",
        "# Create numeric labels for heatmap annotation\n",
        "labels = np.array([f\"{num}\" for num in cm.flatten()]).reshape(cm.shape)\n",
        "\n",
        "# Visualise confusion matrix\n",
        "plt.figure(figsize=(8, 7))\n",
        "sns.heatmap(cm, annot=labels, fmt='',\n",
        "            cmap='Blues')\n",
        "plt.xlabel('Predicted labels')\n",
        "plt.ylabel('True labels')\n",
        "plt.title('Confusion Matrix ‚Äî Validation Set')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sb45ROPxo8R5"
      },
      "source": [
        "<a id=\"evaluation\"></a>\n",
        "## 11. Evaluation & Metrics\n",
        "\n",
        "Visualize training history and compute validation metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_nldg6cToo-k"
      },
      "outputs": [],
      "source": [
        "#rnn_model = RecurrentClassifier(\n",
        "#    input_size=input_shape[-1], # Pass the number of features\n",
        "#    hidden_size=HIDDEN_SIZE,\n",
        "#    num_layers=HIDDEN_LAYERS,\n",
        "#    num_classes=num_classes,\n",
        "#    dropout_rate=DROPOUT_RATE,\n",
        "#    bidirectional=BIDIRECTIONAL,\n",
        "#    rnn_type=RNN_TYPE\n",
        "#    ).to(device)\n",
        "rnn_model = FlexibleRecurrentClassifier(\n",
        "    input_size=input_shape[-1], # Pass the number of features\n",
        "    hidden_sizes=HIDDEN_SIZE,\n",
        "    num_classes=num_classes,\n",
        "    dropout_rate=DROPOUT_RATE,\n",
        "    bidirectional=BIDIRECTIONAL,\n",
        "    rnn_type=RNN_TYPE\n",
        "    ).to(device)\n",
        "print(\"Setting the model to evaluation mode...\")\n",
        "rnn_model.load_state_dict(torch.load(MODEL_LOAD_PATH, map_location=device))\n",
        "rnn_model.eval()  # Set model to evaluation mode\n",
        "recurrent_summary(rnn_model, input_size=input_shape)\n",
        "print(\"\\nStarting model evaluation on test set...\")\n",
        "\n",
        "# Verify data dimensions before inference\n",
        "# Removed: The 'FlexibleRecurrentClassifier' does not have a direct 'rnn' attribute.\n",
        "# print(f\"Model input size: {rnn_model.rnn.input_size}\")\n",
        "print(f\"Test data feature size: {X_test.shape[-1]}\")\n",
        "\n",
        "test_preds, test_targets = [], []\n",
        "test_probabilities = []\n",
        "with torch.no_grad():  # Disable gradient computation for inference\n",
        "    for xb, yb in test_loader:\n",
        "        xb = xb.to(torch.float32)\n",
        "        xb = xb.to(device)\n",
        "\n",
        "        # Forward pass: get model predictions\n",
        "        logits = rnn_model(xb)\n",
        "        preds = logits.argmax(dim=1).cpu().numpy()\n",
        "\n",
        "        # Store batch results\n",
        "        test_preds.append(preds)\n",
        "        test_targets.append(yb.numpy())\n",
        "\n",
        "\n",
        "# Combine all batches into single arrays\n",
        "test_preds = np.concatenate(test_preds)\n",
        "test_targets = np.concatenate(test_targets)\n",
        "\n",
        "print(f\"\\nTest set evaluation completed successfully!\")\n",
        "print(f\"Total test samples: {len(test_preds)}\")\n",
        "print(f\"Predictions shape: {test_preds.shape}\")\n",
        "print(f\"Targets shape: {test_targets.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "By24YYFTo8R6"
      },
      "source": [
        "<a id=\"model-loading\"></a>\n",
        "## 12. Model Loading & Final Testing\n",
        "\n",
        "Load the trained model and evaluate on the internal test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kncRPNp7o8R6"
      },
      "source": [
        "### 12.1 Test Set Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rtwjJ18o8R6"
      },
      "source": [
        "### 12.2 Test Set Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oiKGI1Bzo8R6"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# --- Load and prepare model ---\n",
        "#rnn_model = RecurrentClassifier(\n",
        "#    input_size=input_shape[-1],  # number of features\n",
        "#    hidden_size=HIDDEN_SIZE,\n",
        "#    num_layers=HIDDEN_LAYERS,\n",
        "#    num_classes=num_classes,\n",
        "#    dropout_rate=DROPOUT_RATE,\n",
        "#    bidirectional=BIDIRECTIONAL,\n",
        "#    rnn_type=RNN_TYPE\n",
        "#).to(device)\n",
        "\n",
        "rnn_model = FlexibleRecurrentClassifier(\n",
        "    input_size=input_shape[-1], # Pass the number of features\n",
        "    hidden_sizes=HIDDEN_SIZE,\n",
        "    num_classes=num_classes,\n",
        "    dropout_rate=DROPOUT_RATE,\n",
        "    bidirectional=BIDIRECTIONAL,\n",
        "    rnn_type=RNN_TYPE\n",
        "    ).to(device)\n",
        "\n",
        "\n",
        "print(\"Setting the model to evaluation mode...\")\n",
        "rnn_model.load_state_dict(torch.load(MODEL_LOAD_PATH, map_location=device))\n",
        "rnn_model.eval()\n",
        "recurrent_summary(rnn_model, input_size=input_shape)\n",
        "print(\"\\nStarting model evaluation on test set...\")\n",
        "\n",
        "\n",
        "\n",
        "# --- Inference ---\n",
        "test_preds, test_targets = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for xb, yb in test_loader:\n",
        "        xb = xb.to(torch.float32).to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        logits = rnn_model(xb)\n",
        "        preds = logits.argmax(dim=1).cpu().numpy()\n",
        "\n",
        "        test_preds.append(preds)\n",
        "        test_targets.append(yb.numpy())\n",
        "\n",
        "# --- Combine batches ---\n",
        "test_preds = np.concatenate(test_preds)\n",
        "test_targets = np.concatenate(test_targets)\n",
        "\n",
        "print(f\"\\n‚úÖ Test set evaluation completed successfully!\")\n",
        "print(f\"Total test samples: {len(test_preds)}\")\n",
        "\n",
        "# =========================================================\n",
        "# üß© Majority Vote Aggregation per Pirate\n",
        "# =========================================================\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "\n",
        "sample_indices = ids_test  # <-- use your real IDs from build_sequences()\n",
        "\n",
        "# Combine results into DataFrame\n",
        "test_df = pd.DataFrame({\n",
        "    \"sample_index\": sample_indices,\n",
        "    \"label\": test_preds,\n",
        "    \"target\": test_targets\n",
        "})\n",
        "\n",
        "# Majority vote per pirate\n",
        "majority_vote = (\n",
        "    test_df.groupby(\"sample_index\")[\"label\"]\n",
        "    .apply(lambda x: Counter(x).most_common(1)[0][0])\n",
        "    .reset_index(name=\"voted_label\")\n",
        ")\n",
        "\n",
        "# Majority of targets (optional)\n",
        "majority_target = (\n",
        "    test_df.groupby(\"sample_index\")[\"target\"]\n",
        "    .apply(lambda x: Counter(x).most_common(1)[0][0])\n",
        "    .reset_index(name=\"true_label\")\n",
        ")\n",
        "\n",
        "# Merge and analyze\n",
        "final_eval_df = majority_vote.merge(majority_target, on=\"sample_index\")\n",
        "\n",
        "# Optional label decoding\n",
        "if \"label_mapping\" in globals():\n",
        "    reverse_pain_map = {v: k for k, v in label_mapping.items()}\n",
        "    final_eval_df[\"voted_label\"] = final_eval_df[\"voted_label\"].map(reverse_pain_map)\n",
        "    final_eval_df[\"true_label\"] = final_eval_df[\"true_label\"].map(reverse_pain_map)\n",
        "\n",
        "\n",
        "\n",
        "# üìä --- Label Distribution ---\n",
        "label_counts = final_eval_df[\"voted_label\"].value_counts().reindex([\"no_pain\", \"low_pain\", \"high_pain\"]).fillna(0).astype(int)\n",
        "\n",
        "print(\"\\nüìä  Label Distribution (after Majority Vote):\")\n",
        "for label, count in label_counts.items():\n",
        "    print(f\"   ‚Ä¢ {label:<10}: {count:>3} pirates\")\n",
        "\n",
        "print(f\"\\nüë•  Total pirates after grouping: {len(final_eval_df)}\")\n",
        "\n",
        "\n",
        "acc = accuracy_score(final_eval_df[\"true_label\"], final_eval_df[\"voted_label\"])\n",
        "prec = precision_score(final_eval_df[\"true_label\"], final_eval_df[\"voted_label\"], average='weighted')\n",
        "rec = recall_score(final_eval_df[\"true_label\"], final_eval_df[\"voted_label\"], average='weighted')\n",
        "f1 = f1_score(final_eval_df[\"true_label\"], final_eval_df[\"voted_label\"], average='weighted')\n",
        "\n",
        "print(f\"\\n‚úÖ Accuracy:  {acc:.4f}\")\n",
        "print(f\"üéØ Precision: {prec:.4f}\")\n",
        "print(f\"üìà Recall:    {rec:.4f}\")\n",
        "print(f\"üåü F1 Score:  {f1:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eEJ3aeOLCstT"
      },
      "outputs": [],
      "source": [
        "print(final_eval_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2M0MOw0po8R6"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Desired display names for labels\n",
        "display_class_names = [\"no_pain\", \"low_pain\", \"high_pain\"]\n",
        "\n",
        "# Numerical labels as they exist in final_eval_df[\"true_label\"] and final_eval_df[\"voted_label\"]\n",
        "numerical_class_labels = sorted(list(label_mapping.values()))\n",
        "\n",
        "# Re-map string labels back to numerical for confusion_matrix calculation\n",
        "numerical_true_labels = final_eval_df[\"true_label\"].map(label_mapping)\n",
        "numerical_voted_labels = final_eval_df[\"voted_label\"].map(label_mapping)\n",
        "\n",
        "# Compute confusion matrix with the actual numerical labels present in the data\n",
        "cm_test = confusion_matrix(\n",
        "    numerical_true_labels,  # Use numerical labels here\n",
        "    numerical_voted_labels, # Use numerical labels here\n",
        "    labels=numerical_class_labels # Use numerical labels here\n",
        ")\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "# Format numeric labels for display\n",
        "labels_text = np.array([f\"{num}\" for num in cm_test.flatten()]).reshape(cm_test.shape)\n",
        "\n",
        "sns.heatmap(cm_test, annot=labels_text, fmt='', cmap='Blues',\n",
        "            xticklabels=display_class_names, yticklabels=display_class_names) # Use display names for plot\n",
        "\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.title('Confusion Matrix ‚Äî Test Set (Majority Vote)')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tate3jWjW4M4"
      },
      "outputs": [],
      "source": [
        "cm_raw = confusion_matrix(test_targets, test_preds)\n",
        "sns.heatmap(cm_raw, annot=True, fmt='d', cmap='Greens',\n",
        "            xticklabels=display_class_names, yticklabels=display_class_names)\n",
        "plt.title(\"Confusion Matrix ‚Äî Window Level\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOUQLuMLo8R7"
      },
      "source": [
        "<a id=\"submission\"></a>\n",
        "## 13. Competition Submission\n",
        "\n",
        "Generate predictions for the competition test set and create submission file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n_lmCT037g94"
      },
      "outputs": [],
      "source": [
        "def build_sequences_inference(df, window=40, stride=5):\n",
        "    dataset = []\n",
        "    pirate_ids = []\n",
        "\n",
        "    for pirate_id in df['sample_index'].unique():\n",
        "        columns = [col for col in df.columns if col not in ['sample_index', 'time', 'label']]\n",
        "        temp = df[df['sample_index'] == pirate_id][columns].values\n",
        "\n",
        "        # Padding\n",
        "        padding_len = window - len(temp) % window\n",
        "        if padding_len < window:\n",
        "            padding = np.zeros((padding_len, temp.shape[1]), dtype='float32')\n",
        "            temp = np.concatenate((temp, padding))\n",
        "\n",
        "        # Slice into windows\n",
        "        idx = 0\n",
        "        while idx + window <= len(temp):\n",
        "            dataset.append(temp[idx:idx + window])\n",
        "            pirate_ids.append(pirate_id)  # track which pirate this window came from\n",
        "            idx += stride\n",
        "\n",
        "    dataset = np.array(dataset, dtype='float32')\n",
        "    return dataset, pirate_ids\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "av26pqsHo8R7"
      },
      "source": [
        "### 13.1 Load & Preprocess Competition Test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aU0Xob88pW0G"
      },
      "outputs": [],
      "source": [
        "# --- Load and preprocess the actual test dataset ---\n",
        "print(\"Loading test dataset for final evaluation...\")\n",
        "\n",
        "# Load the actual test dataset (this doesn't have labels)\n",
        "X_test_final = pd.read_csv('pirate_pain_test.csv')\n",
        "\n",
        "# Create a mapping dictionary to convert categorical labels to numerical values\n",
        "map_dict_legs = { 'two': 2, 'one+peg_leg': 1}\n",
        "map_dict_hands = { 'two': 2, 'one+hook_hand': 1}\n",
        "map_dict_eyes = { 'two': 2, 'one+eye_patch': 1}\n",
        "X_test_final['n_legs'] = X_test_final['n_legs'].map(map_dict_legs)\n",
        "X_test_final['n_hands'] = X_test_final['n_hands'].map(map_dict_hands)\n",
        "X_test_final['n_eyes'] = X_test_final['n_eyes'].map(map_dict_eyes)\n",
        "\n",
        "\n",
        "# Convert inputs from float64 to float32\n",
        "X_test_final = X_test_final.astype(np.float32)\n",
        "\n",
        "# Define the columns to be normalised (use training statistics for proper normalization)\n",
        "scale_columns = [\n",
        "    col for col in data.columns\n",
        "    if (col.startswith('joint_') or col.startswith('pain_survey')) and not col.startswith('joint_30')\n",
        "]\n",
        "\n",
        "# Use training data statistics for normalization\n",
        "print(\"Normalizing test data using training statistics...\")\n",
        "for column in scale_columns:\n",
        "    if column in df_train.columns:\n",
        "        # Use training min/max for proper normalization\n",
        "        train_min = df_train[column].min()\n",
        "        train_max = df_train[column].max()\n",
        "        denom = train_max - train_min\n",
        "        if np.isclose(denom, 0.0):\n",
        "            X_test_final[column] = 0.0\n",
        "        else:\n",
        "            X_test_final[column] = (X_test_final[column] - train_min) / denom\n",
        "    else:\n",
        "        print(f\"Warning: Column {column} not found in training data\")\n",
        "\n",
        "\n",
        "print(f\"Preprocessed competition test dataset shape: {X_test_final.shape}\")\n",
        "\n",
        "# Generate Sequences\n",
        "X_test_final_sequence, pirate_ids  = build_sequences_inference(X_test_final, WINDOW_SIZE, STRIDE)\n",
        "print(f\"X_test_final_sequenced shape: {X_test_final_sequence.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DvFkTaX9o8R7"
      },
      "source": [
        "### 13.2 Generate Predictions for Competition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ut28QqWM5LsZ"
      },
      "outputs": [],
      "source": [
        "#rnn_model = RecurrentClassifier(\n",
        "#    input_size=input_shape[-1], # Pass the number of features\n",
        "#    hidden_size=HIDDEN_SIZE,\n",
        "#    num_layers=HIDDEN_LAYERS,\n",
        "#    num_classes=num_classes,\n",
        "#    dropout_rate=DROPOUT_RATE,\n",
        "#    bidirectional=BIDIRECTIONAL,\n",
        "#    rnn_type=RNN_TYPE\n",
        "#    ).to(device)\n",
        "\n",
        "rnn_model = FlexibleRecurrentClassifier(\n",
        "    input_size=input_shape[-1], # Pass the number of features\n",
        "    hidden_sizes=HIDDEN_SIZE,\n",
        "    num_classes=num_classes,\n",
        "    dropout_rate=DROPOUT_RATE,\n",
        "    bidirectional=BIDIRECTIONAL,\n",
        "    rnn_type=RNN_TYPE\n",
        "    ).to(device)\n",
        "\n",
        "\n",
        "print(\"Setting the model to evaluation mode...\")\n",
        "rnn_model.load_state_dict(torch.load(MODEL_LOAD_PATH, map_location=device))\n",
        "rnn_model.eval()  # Set model to evaluation mode\n",
        "recurrent_summary(rnn_model, input_size=input_shape)\n",
        "\n",
        "# Verify data dimensions before inference\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TMoj6vwE-W_v"
      },
      "outputs": [],
      "source": [
        "X_test_final_tensor = torch.from_numpy(X_test_final_sequence).to(torch.float32).to(device)\n",
        "\n",
        "final_loader = DataLoader(\n",
        "    TensorDataset(X_test_final_tensor),\n",
        "    batch_size=64,  # adjustable\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# Lists to collect results\n",
        "all_preds = []\n",
        "all_probs = []\n",
        "\n",
        "# Disable gradient computation for inference\n",
        "with torch.no_grad():\n",
        "    for (xb,) in final_loader:\n",
        "        xb = xb.to(device)\n",
        "\n",
        "        # Forward pass through the model\n",
        "        logits = rnn_model(xb)\n",
        "\n",
        "        # Convert to probabilities (softmax)\n",
        "        probs = torch.softmax(logits, dim=1)\n",
        "\n",
        "        # Get predicted class (highest probability)\n",
        "        preds = logits.argmax(dim=1).cpu().numpy()\n",
        "\n",
        "        # Store results\n",
        "        all_preds.append(preds)\n",
        "        all_probs.append(probs.cpu().numpy())\n",
        "\n",
        "# Combine all batches\n",
        "all_preds = np.concatenate(all_preds)\n",
        "all_probs = np.concatenate(all_probs)\n",
        "\n",
        "print(f\"\\n‚úÖ Final prediction completed!\")\n",
        "\n",
        "\n",
        "# Combine predictions with pirate IDs\n",
        "pred_df = pd.DataFrame(\n",
        "    {\n",
        "        \"sample_index\": pirate_ids,\n",
        "        \"label\": all_preds\n",
        "    }\n",
        ")\n",
        "\n",
        "# Group by pirate and choose the most common prediction (majority vote)\n",
        "final_preds = (\n",
        "    pred_df.groupby(\"sample_index\")[\"label\"]\n",
        "    .apply(lambda x: Counter(x).most_common(1)[0][0])\n",
        "    .reset_index()\n",
        ")\n",
        "pain_map = {'no_pain': 0, 'low_pain': 1, 'high_pain': 2}\n",
        "reverse_pain_map = {v: k for k, v in pain_map.items()}\n",
        "final_preds[\"label\"] = final_preds[\"label\"].map(reverse_pain_map)\n",
        "\n",
        "final_preds[\"sample_index\"] = final_preds[\"sample_index\"].astype(int).astype(str).str.zfill(3)\n",
        "\n",
        "\n",
        "\n",
        "print(f\"Final predictions shape: {final_preds.shape}\")\n",
        "print(final_preds.head())\n",
        "\n",
        "final_preds[[\"sample_index\", \"label\"]].to_csv(f\"submissions/{SUBMISSION_FILENAME}\", index=False)\n",
        "print(f\"‚úÖ Saved submissions/{SUBMISSION_FILENAME}\")\n",
        "\n",
        "# Count number of pirates in each predicted label category\n",
        "label_counts = final_preds[\"label\"].value_counts().sort_index()\n",
        "\n",
        "print(\"\\nüìä Label distribution (number of pirates per pain level):\")\n",
        "for label, count in label_counts.items():\n",
        "    print(f\"{label}: {count}\")\n",
        "\n",
        "# Optional: quick sanity check ‚Äî should equal total number of pirates\n",
        "print(f\"\\nTotal pirates: {label_counts.sum()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8JzCb__o8R7"
      },
      "source": [
        "### 13.3 Create Submission File"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgP2VyHPo8R7"
      },
      "source": [
        "### 13.4 Validate Submission Format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nKKCCVdlo8R7"
      },
      "outputs": [],
      "source": [
        "# --- Validate Submission Format ---\n",
        "print(\"Validating submission file format...\")\n",
        "submission_filepath = f\"submissions/{EXPERIMENT_NAME}.csv\"\n",
        "submission_df = pd.read_csv(f\"submissions/{SUBMISSION_FILENAME}\")\n",
        "# Load the sample submission for comparison\n",
        "sample_submission = pd.read_csv('sample_submission.csv')\n",
        "print(f\"Sample submission shape: {sample_submission.shape}\")\n",
        "print(f\"Our submission shape: {submission_df.shape}\")\n",
        "\n",
        "# Check format compatibility\n",
        "if submission_df.shape[0] != sample_submission.shape[0]:\n",
        "    print(f\"‚ö†Ô∏è  WARNING: Different number of samples!\")\n",
        "    print(f\"Expected: {sample_submission.shape[0]}, Got: {submission_df.shape[0]}\")\n",
        "else:\n",
        "    print(\"‚úÖ Number of samples matches sample submission\")\n",
        "\n",
        "# Check column names\n",
        "if list(submission_df.columns) == list(sample_submission.columns):\n",
        "    print(\"‚úÖ Column names match sample submission\")\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è  WARNING: Column names mismatch!\")\n",
        "    print(f\"Expected: {list(sample_submission.columns)}\")\n",
        "    print(f\"Got: {list(submission_df.columns)}\")\n",
        "\n",
        "# Check sample_index format\n",
        "expected_indices = [f\"{i:03d}\" for i in range(len(sample_submission))]\n",
        "if list(submission_df['sample_index']) == expected_indices:\n",
        "    print(\"‚úÖ Sample indices format is correct\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  WARNING: Sample indices format might be incorrect\")\n",
        "    print(f\"First few expected: {expected_indices[:5]}\")\n",
        "    print(f\"First few got: {list(submission_df['sample_index'][:5])}\")\n",
        "\n",
        "# Check label values\n",
        "expected_labels = set(sample_submission['label'].unique())\n",
        "our_labels = set(submission_df['label'].unique())\n",
        "if our_labels.issubset(expected_labels):\n",
        "    print(\"‚úÖ All label values are valid\")\n",
        "    print(f\"Used labels: {sorted(our_labels)}\")\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è  WARNING: Invalid label values detected!\")\n",
        "    print(f\"Expected labels: {sorted(expected_labels)}\")\n",
        "    print(f\"Our labels: {sorted(our_labels)}\")\n",
        "    print(f\"Invalid labels: {our_labels - expected_labels}\")\n",
        "\n",
        "print(f\"\\\\nüéØ Final submission file: '{submission_filepath}'\")\n",
        "print(\"üìä Submission ready for competition!\")\n",
        "\n",
        "# Display final summary\n",
        "print(f\"\\\\n\" + \"=\"*60)\n",
        "print(\"                    SUBMISSION SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(f\"Test Performance (Internal): F1={f1:.4f}, Accuracy={acc:.4f}\")\n",
        "print(f\"Competition Samples: {len(submission_df)}\")\n",
        "print(f\"Submission File: {submission_filepath}\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7o6bJN4hYOW9"
      },
      "source": [
        "## 14 Tensor Board"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D4pqTxf1inPl"
      },
      "outputs": [],
      "source": [
        "## Copy TensorBoard logs to accessible location for Colab\n",
        "#!rsync -a $current_dir\"/\"$logs_dir/ \"/content/\"$logs_dir\n",
        "## Launch TensorBoard interface\n",
        "#%tensorboard --logdir \"/content/\"$logs_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fc966286"
      },
      "source": [
        "## 15. Save Model Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5578b7fe"
      },
      "outputs": [],
      "source": [
        "# Gather all relevant model and training parameters\n",
        "model_config = {\n",
        "    'VAL_USERS' : N_VAL_USERS,\n",
        "    'TEST_USERS': N_TEST_USERS,\n",
        "    'EXPERIMENT_NAME': EXPERIMENT_NAME,\n",
        "    'RNN_TYPE': RNN_TYPE,\n",
        "    'BIDIRECTIONAL': BIDIRECTIONAL,\n",
        "    'input_size': input_shape[-1],\n",
        "    'num_classes': num_classes,\n",
        "    'HIDDEN_SIZE': HIDDEN_SIZE,\n",
        "    'HIDDEN_LAYERS': HIDDEN_LAYERS,\n",
        "    'DROPOUT_RATE': DROPOUT_RATE,\n",
        "    'LEARNING_RATE': LEARNING_RATE,\n",
        "    'EPOCHS': EPOCHS,\n",
        "    'PATIENCE': PATIENCE,\n",
        "    'L1_LAMBDA': L1_LAMBDA,\n",
        "    'L2_LAMBDA': L2_LAMBDA,\n",
        "    'BATCH_SIZE': BATCH_SIZE,\n",
        "    'WINDOW_SIZE': WINDOW_SIZE,\n",
        "    'STRIDE': STRIDE,\n",
        "    'SEED': SEED,\n",
        "    'one_pirate_window': one_pirate_window,\n",
        "    'TEST_F1': f'{f1:.4f}'\n",
        "}\n",
        "\n",
        "# Define the path to save the config file\n",
        "config_filepath = os.path.join(models_dir, f\"{EXPERIMENT_NAME}_config.json\")\n",
        "\n",
        "# Save the dictionary as a JSON file\n",
        "with open(config_filepath, 'w') as f:\n",
        "    json.dump(model_config, f, indent=4)\n",
        "\n",
        "print(f\"‚úÖ Model configuration saved to '{config_filepath}'\")\n",
        "\n",
        "# Display the saved configuration\n",
        "print(\"\\n--- Saved Model Configuration ---\")\n",
        "print(json.dumps(model_config, indent=4))\n",
        "print(\"-----------------------------------\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "myenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
