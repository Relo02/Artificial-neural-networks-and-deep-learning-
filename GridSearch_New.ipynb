{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jy5OeJu9o8Rt"
      },
      "source": [
        "# üè¥‚Äç‚ò†Ô∏è Pirate Pain Classification Challenge\n",
        "\n",
        "> ‚öì *\"Even pirates feel pain ‚Äî let's teach the model to feel it too.\"*\n",
        "\n",
        "---\n",
        "\n",
        "## üìö Table of Contents\n",
        "0. [README](#readme)  \n",
        "1. [Setup & Configuration](#setup)  \n",
        "2. [Data Loading](#data-loading)  \n",
        "3. [Import Libraries](#import-libraries)  \n",
        "4. [Data Preprocessing](#data-preprocessing)  \n",
        "5. [Sequence Building](#sequence-building)  \n",
        "6. [DataLoaders](#dataloaders)  \n",
        "7. [Network Hyperparameters](#hyperparameters)\n",
        "8. [Model Architecture](#model-architecture)  \n",
        "9. [Training Functions](#training-functions)  \n",
        "10. [Model Training](#model-training)  \n",
        "11. [Evaluation & Metrics](#evaluation)  \n",
        "12. [Model Loading & Final Testing](#model-loading)  \n",
        "13. [Competition Submission](#submission)\n",
        "\n",
        "---\n",
        "\n",
        "### ‚öôÔ∏è Quick Configuration Map\n",
        "\n",
        "> üß≠ *\"If ye seek to tweak the code, here be where to look!\"*\n",
        "\n",
        "- üß∫ **Batch Size:** ‚Üí [DataLoaders](#dataloaders)  \n",
        "- ‚öóÔ∏è **Hyperparameters:** ‚Üí [Network Hyperparameters](#hyperparameters)  \n",
        "- ü™û **Window Size & Stride:** ‚Üí [Sequence Building](#sequence-building)  \n",
        "- ‚öôÔ∏è **Model Type:** ‚Üí [Setup & Configuration](#setup)  \n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### üí∞ Treasure Storage ‚Äî Models & Submissions\n",
        "> üè¥‚Äç‚ò†Ô∏è *\"A wise pirate always knows where his treasure be buried ‚Äî guard yer models and submissions well!\"*\n",
        "\n",
        "- üíæ **Model & Submission Save/Load Path:** ‚Üí [Setup & Configuration](#setup)  \n",
        "  - üóÇÔ∏è Models be saved in a **`models/`** folder with the name:\n",
        "    **`experiment_name_dd-mm-HH-MM.pt`** (day-month-hour-minute).\n",
        "  - üìú Submissions be saved in a **`submissions/`** folder with the filename format:  \n",
        "    **`experiment_name_dd-mm-HH-MM.csv`** .\n",
        "  - üî° All related model parameters are saved in **`models/`** folder with the  name **`experiment_name_dd-mm-HH-MM_config.json`** .\n",
        "\n",
        "  \n",
        "  *‚ùóThe experiment name is set as **`RnnType_Bi_dd-mm-HH-MM`** or **`RnnType_dd-mm-HH-MM`** depending on if it is bidirectional or not*\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4oU_xMZwJUrZ"
      },
      "source": [
        "<a id=\"readme\"></a>\n",
        "## 0. Info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mjy_NO-5HPw3"
      },
      "source": [
        "\n",
        "\n",
        "This section lists all the main parameters that can be modified to control data loading, model behavior, and training.\n",
        "\n",
        "---\n",
        "\n",
        "### üìÅ File Paths\n",
        "| Variable | Description | Default Value |\n",
        "|-----------|--------------|----------------|\n",
        "| `TRAIN_DATA_PATH` | Training features | `'pirate_pain_train.csv'` |\n",
        "| `TRAIN_LABELS_PATH` | Training labels | `'pirate_pain_train_labels.csv'` |\n",
        "| `TEST_DATA_PATH` | Test set for inference | `'pirate_pain_test.csv'` *(optional)* |\n",
        "| `MODEL_SAVE_PATH` | Output model file | `'pirate_model.pt'` |\n",
        "| `RESULTS_FILE` | CSV for predictions | `'results_<date-time>.csv'` |\n",
        "\n",
        "---\n",
        "\n",
        "### üß† Model & Architecture\n",
        "| Parameter | Description | Typical Values |\n",
        "|------------|--------------|----------------|\n",
        "| `model_type` | Choose model class | `'RNN'`, `'LSTM'`, `'GRU'`, `'ANN'` |\n",
        "| `input_size` | Number of features per time step | *auto-detected from data* |\n",
        "| `hidden_size` | Hidden layer size | `64`, `128`, `256` |\n",
        "| `num_layers` | Number of RNN layers | `1-4` |\n",
        "| `dropout` | Dropout probability | `0.2‚Äì0.5` |\n",
        "| `num_classes` | Output classes (pain levels) | *from label set* |\n",
        "\n",
        "---\n",
        "\n",
        "### üèãÔ∏è Training Hyperparameters\n",
        "| Parameter | Description | Default / Range |\n",
        "|------------|--------------|-----------------|\n",
        "| `batch_size` | Samples per batch | `512/2^n` |\n",
        "| `learning_rate` | Optimizer learning rate | `1e-3` |\n",
        "| `num_epochs` | Training iterations | `500` |\n",
        "| `optimizer` | Optimization algorithm | `'AdamW'` |\n",
        "| `criterion` | Loss function | `CrossEntropyLoss()` |\n",
        "| `seed` | Random seed for reproducibility | `42` |\n",
        "\n",
        "---\n",
        "\n",
        "### üì§ Inference\n",
        "| Parameter | Description |\n",
        "|------------|--------------|\n",
        "| `LOAD_MODEL_PATH` | Path to pretrained `.pt` model (optional) |\n",
        "| `save_results` | Whether to write output CSV | `True` |\n",
        "\n",
        "---\n",
        "\n",
        "> üí° *Tip:* Adjust hyperparameters in the ‚ÄúConfiguration‚Äù or ‚ÄúTraining Setup‚Äù cell before running the notebook.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZLBQ6tJrcBB"
      },
      "source": [
        "<a id=\"setup\"></a>\n",
        "## 1. Setup & Configuration\n",
        "\n",
        "*Optional: Connect to Google Drive (for Colab users)*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nig16xZNnmnz",
        "outputId": "d34ce704-468c-4418-8395-f95276007483"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n",
            "/gdrive/MyDrive/pirate_dataset\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/gdrive\")\n",
        "current_dir = \"/gdrive/MyDrive/pirate_dataset\"\n",
        "%cd $current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZL1iYHipaeMD"
      },
      "source": [
        "*Set Model Type*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uWaTlLkTKgk5"
      },
      "outputs": [],
      "source": [
        "RNN_TYPE = 'LSTM'            # 'RNN', 'LSTM', or 'GRU'\n",
        "BIDIRECTIONAL = True        # True / False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "up7Qo6v-o8Ru"
      },
      "source": [
        "*Set Model Save Name*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wkBnTJHuo8Rv",
        "outputId": "c50a8a4f-6a30-4eba-c96a-b843312f9cf0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment name: LSTM_bi_12-11-13-27\n",
            "Submission filename: LSTM_bi_12-11-13-27.csv\n",
            "Model save path: models/LSTM_bi_12-11-13-27_model.pt\n",
            "Model load path: models/LSTM_bi_12-11-13-27_model.pt\n"
          ]
        }
      ],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "# Get current date and time for submission filename\n",
        "current_datetime = datetime.now().strftime(\"%d-%m-%H-%M\")\n",
        "\n",
        "if BIDIRECTIONAL:\n",
        "    EXPERIMENT_NAME = f\"{RNN_TYPE}_bi_{current_datetime}\"\n",
        "else:\n",
        "    EXPERIMENT_NAME = f\"{RNN_TYPE}_{current_datetime}\"\n",
        "\n",
        "SUBMISSION_FILENAME = f\"{EXPERIMENT_NAME}.csv\"\n",
        "\n",
        "\n",
        "# Directory configuration\n",
        "logs_dir = \"tensorboard\"\n",
        "models_dir = \"models\"\n",
        "\n",
        "# Model save/load paths\n",
        "MODEL_SAVE_PATH = f\"{models_dir}/{EXPERIMENT_NAME}_model.pt\"\n",
        "MODEL_LOAD_PATH = f\"{models_dir}/{EXPERIMENT_NAME}_model.pt\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(f\"Experiment name: {EXPERIMENT_NAME}\")\n",
        "print(f\"Submission filename: {SUBMISSION_FILENAME}\")\n",
        "print(f\"Model save path: {MODEL_SAVE_PATH}\")\n",
        "print(f\"Model load path: {MODEL_LOAD_PATH}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdaKCgHVvvHX"
      },
      "source": [
        "<a id=\"data-loading\"></a>\n",
        "## 2. Data Loading\n",
        "\n",
        "Load training and test datasets from CSV files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oLyI938Jvn-J"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "X_train = pd.read_csv('pirate_pain_train.csv')\n",
        "y_train = pd.read_csv('pirate_pain_train_labels.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3Lre5NWwCyk"
      },
      "source": [
        "<a id=\"import-libraries\"></a>\n",
        "## 3. Import Libraries\n",
        "\n",
        "Set random seeds for reproducibility and import all necessary packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nt3AnE8SwJg1",
        "outputId": "7df1fafe-640d-4488-e094-b70b9a933eaa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n",
            "PyTorch version: 2.8.0+cu126\n",
            "Device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Set seed for reproducibility\n",
        "SEED = 1122\n",
        "# Import necessary libraries\n",
        "import os\n",
        "\n",
        "# Set environment variables before importing modules\n",
        "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
        "os.environ['MPLCONFIGDIR'] = os.getcwd() + '/configs/'\n",
        "\n",
        "# Suppress warnings\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "warnings.simplefilter(action='ignore', category=Warning)\n",
        "\n",
        "# Import necessary modules\n",
        "import logging\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# Set seeds for random number generators in NumPy and Python\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "# Import PyTorch\n",
        "import torch\n",
        "torch.manual_seed(SEED)\n",
        "from torch import nn\n",
        "from torchsummary import summary\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "!pkill -f tensorboard\n",
        "%load_ext tensorboard\n",
        "!mkdir -p {models_dir}\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Device: {device}\")\n",
        "\n",
        "# Import other libraries\n",
        "import copy\n",
        "import shutil\n",
        "from datetime import datetime\n",
        "from itertools import product\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix,classification_report\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import json\n",
        "from itertools import product\n",
        "\n",
        "# Configure plot display settings\n",
        "sns.set(font_scale=1.4)\n",
        "sns.set_style('white')\n",
        "plt.rc('font', size=14)\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWnQz-p-xyhD"
      },
      "source": [
        "<a id=\"data-preprocessing\"></a>\n",
        "## 4. Data Preprocessing\n",
        "\n",
        "Explore data, split into train/val/test sets, normalize features, and encode labels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPMtvy5Fo8Rw"
      },
      "source": [
        "### 4.1 Data Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 603
        },
        "id": "hHK2Aw7Ix4S8",
        "outputId": "d5654ec3-b017-48ab-b732-4a636d2b60c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset shape: (105760, 40)\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "X_train"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-766d7168-d5d1-433c-a736-2aa0080aa2fe\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sample_index</th>\n",
              "      <th>time</th>\n",
              "      <th>pain_survey_1</th>\n",
              "      <th>pain_survey_2</th>\n",
              "      <th>pain_survey_3</th>\n",
              "      <th>pain_survey_4</th>\n",
              "      <th>n_legs</th>\n",
              "      <th>n_hands</th>\n",
              "      <th>n_eyes</th>\n",
              "      <th>joint_00</th>\n",
              "      <th>...</th>\n",
              "      <th>joint_21</th>\n",
              "      <th>joint_22</th>\n",
              "      <th>joint_23</th>\n",
              "      <th>joint_24</th>\n",
              "      <th>joint_25</th>\n",
              "      <th>joint_26</th>\n",
              "      <th>joint_27</th>\n",
              "      <th>joint_28</th>\n",
              "      <th>joint_29</th>\n",
              "      <th>joint_30</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>two</td>\n",
              "      <td>two</td>\n",
              "      <td>two</td>\n",
              "      <td>1.094705</td>\n",
              "      <td>...</td>\n",
              "      <td>3.499558e-06</td>\n",
              "      <td>1.945042e-06</td>\n",
              "      <td>3.999558e-06</td>\n",
              "      <td>1.153299e-05</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>0.017592</td>\n",
              "      <td>0.013508</td>\n",
              "      <td>0.026798</td>\n",
              "      <td>0.027815</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>two</td>\n",
              "      <td>two</td>\n",
              "      <td>two</td>\n",
              "      <td>1.135183</td>\n",
              "      <td>...</td>\n",
              "      <td>3.976952e-07</td>\n",
              "      <td>6.765107e-07</td>\n",
              "      <td>6.019627e-06</td>\n",
              "      <td>4.643774e-08</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.013352</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.013377</td>\n",
              "      <td>0.013716</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>two</td>\n",
              "      <td>two</td>\n",
              "      <td>two</td>\n",
              "      <td>1.080745</td>\n",
              "      <td>...</td>\n",
              "      <td>1.533820e-07</td>\n",
              "      <td>1.698525e-07</td>\n",
              "      <td>1.446051e-06</td>\n",
              "      <td>2.424536e-06</td>\n",
              "      <td>0.000003</td>\n",
              "      <td>0.016225</td>\n",
              "      <td>0.008110</td>\n",
              "      <td>0.024097</td>\n",
              "      <td>0.023105</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>two</td>\n",
              "      <td>two</td>\n",
              "      <td>two</td>\n",
              "      <td>0.938017</td>\n",
              "      <td>...</td>\n",
              "      <td>1.006865e-05</td>\n",
              "      <td>5.511079e-07</td>\n",
              "      <td>1.847597e-06</td>\n",
              "      <td>5.432416e-08</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.011832</td>\n",
              "      <td>0.007450</td>\n",
              "      <td>0.028613</td>\n",
              "      <td>0.024648</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>two</td>\n",
              "      <td>two</td>\n",
              "      <td>two</td>\n",
              "      <td>1.090185</td>\n",
              "      <td>...</td>\n",
              "      <td>4.437266e-06</td>\n",
              "      <td>1.735459e-07</td>\n",
              "      <td>1.552722e-06</td>\n",
              "      <td>5.825366e-08</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>0.005360</td>\n",
              "      <td>0.002532</td>\n",
              "      <td>0.033026</td>\n",
              "      <td>0.025328</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>two</td>\n",
              "      <td>two</td>\n",
              "      <td>two</td>\n",
              "      <td>1.146031</td>\n",
              "      <td>...</td>\n",
              "      <td>1.073167e-06</td>\n",
              "      <td>1.753837e-07</td>\n",
              "      <td>2.957340e-07</td>\n",
              "      <td>6.217311e-08</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>0.006150</td>\n",
              "      <td>0.006444</td>\n",
              "      <td>0.033101</td>\n",
              "      <td>0.023767</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>two</td>\n",
              "      <td>two</td>\n",
              "      <td>two</td>\n",
              "      <td>1.025870</td>\n",
              "      <td>...</td>\n",
              "      <td>1.074800e-06</td>\n",
              "      <td>1.772156e-07</td>\n",
              "      <td>1.976558e-06</td>\n",
              "      <td>1.576086e-06</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>0.006495</td>\n",
              "      <td>0.006421</td>\n",
              "      <td>0.031804</td>\n",
              "      <td>0.019056</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>two</td>\n",
              "      <td>two</td>\n",
              "      <td>two</td>\n",
              "      <td>1.038597</td>\n",
              "      <td>...</td>\n",
              "      <td>8.829074e-07</td>\n",
              "      <td>1.790415e-07</td>\n",
              "      <td>2.210562e-06</td>\n",
              "      <td>1.485741e-06</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.015998</td>\n",
              "      <td>0.005397</td>\n",
              "      <td>0.035552</td>\n",
              "      <td>0.015732</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>two</td>\n",
              "      <td>two</td>\n",
              "      <td>two</td>\n",
              "      <td>0.984251</td>\n",
              "      <td>...</td>\n",
              "      <td>1.621055e-06</td>\n",
              "      <td>1.165161e-06</td>\n",
              "      <td>3.030164e-07</td>\n",
              "      <td>5.416678e-07</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.020539</td>\n",
              "      <td>0.008517</td>\n",
              "      <td>0.008635</td>\n",
              "      <td>0.015257</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>two</td>\n",
              "      <td>two</td>\n",
              "      <td>two</td>\n",
              "      <td>1.054999</td>\n",
              "      <td>...</td>\n",
              "      <td>1.609114e-06</td>\n",
              "      <td>3.959558e-06</td>\n",
              "      <td>2.017157e-06</td>\n",
              "      <td>1.154349e-06</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>0.007682</td>\n",
              "      <td>0.021383</td>\n",
              "      <td>0.034006</td>\n",
              "      <td>0.028966</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10 rows √ó 40 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-766d7168-d5d1-433c-a736-2aa0080aa2fe')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-766d7168-d5d1-433c-a736-2aa0080aa2fe button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-766d7168-d5d1-433c-a736-2aa0080aa2fe');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-3f3b9b31-76b4-48b5-a3b3-8663b61f3504\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-3f3b9b31-76b4-48b5-a3b3-8663b61f3504')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-3f3b9b31-76b4-48b5-a3b3-8663b61f3504 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "   sample_index  time  pain_survey_1  pain_survey_2  pain_survey_3  \\\n",
              "0             0     0              2              0              2   \n",
              "1             0     1              2              2              2   \n",
              "2             0     2              2              0              2   \n",
              "3             0     3              2              2              2   \n",
              "4             0     4              2              2              2   \n",
              "5             0     5              2              0              2   \n",
              "6             0     6              2              1              2   \n",
              "7             0     7              2              2              2   \n",
              "8             0     8              2              2              0   \n",
              "9             0     9              0              2              2   \n",
              "\n",
              "   pain_survey_4 n_legs n_hands n_eyes  joint_00  ...      joint_21  \\\n",
              "0              1    two     two    two  1.094705  ...  3.499558e-06   \n",
              "1              2    two     two    two  1.135183  ...  3.976952e-07   \n",
              "2              2    two     two    two  1.080745  ...  1.533820e-07   \n",
              "3              2    two     two    two  0.938017  ...  1.006865e-05   \n",
              "4              2    two     two    two  1.090185  ...  4.437266e-06   \n",
              "5              1    two     two    two  1.146031  ...  1.073167e-06   \n",
              "6              1    two     two    two  1.025870  ...  1.074800e-06   \n",
              "7              2    two     two    two  1.038597  ...  8.829074e-07   \n",
              "8              1    two     two    two  0.984251  ...  1.621055e-06   \n",
              "9              2    two     two    two  1.054999  ...  1.609114e-06   \n",
              "\n",
              "       joint_22      joint_23      joint_24  joint_25  joint_26  joint_27  \\\n",
              "0  1.945042e-06  3.999558e-06  1.153299e-05  0.000004  0.017592  0.013508   \n",
              "1  6.765107e-07  6.019627e-06  4.643774e-08  0.000000  0.013352  0.000000   \n",
              "2  1.698525e-07  1.446051e-06  2.424536e-06  0.000003  0.016225  0.008110   \n",
              "3  5.511079e-07  1.847597e-06  5.432416e-08  0.000000  0.011832  0.007450   \n",
              "4  1.735459e-07  1.552722e-06  5.825366e-08  0.000007  0.005360  0.002532   \n",
              "5  1.753837e-07  2.957340e-07  6.217311e-08  0.000007  0.006150  0.006444   \n",
              "6  1.772156e-07  1.976558e-06  1.576086e-06  0.000005  0.006495  0.006421   \n",
              "7  1.790415e-07  2.210562e-06  1.485741e-06  0.000000  0.015998  0.005397   \n",
              "8  1.165161e-06  3.030164e-07  5.416678e-07  0.000000  0.020539  0.008517   \n",
              "9  3.959558e-06  2.017157e-06  1.154349e-06  0.000007  0.007682  0.021383   \n",
              "\n",
              "   joint_28  joint_29  joint_30  \n",
              "0  0.026798  0.027815       0.5  \n",
              "1  0.013377  0.013716       0.5  \n",
              "2  0.024097  0.023105       0.5  \n",
              "3  0.028613  0.024648       0.5  \n",
              "4  0.033026  0.025328       0.5  \n",
              "5  0.033101  0.023767       0.5  \n",
              "6  0.031804  0.019056       0.5  \n",
              "7  0.035552  0.015732       0.5  \n",
              "8  0.008635  0.015257       0.5  \n",
              "9  0.034006  0.028966       0.5  \n",
              "\n",
              "[10 rows x 40 columns]"
            ]
          },
          "execution_count": 649,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Print the shape of the dataset\n",
        "print(f\"Dataset shape: {X_train.shape}\")\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "X_train.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlDwxJ38o8Rw"
      },
      "source": [
        "### 4.2 Categorical Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LfBKIdsrQDW3"
      },
      "outputs": [],
      "source": [
        "# Merge features and labels\n",
        "data = X_train.merge(y_train, on='sample_index')\n",
        "\n",
        "# Create a mapping dictionary to convert categorical labels to numerical values\n",
        "map_dict_legs = { 'two': 2, 'one+peg_leg': 1}\n",
        "map_dict_hands = { 'two': 2, 'one+hook_hand': 1}\n",
        "map_dict_eyes = { 'two': 2, 'one+eye_patch': 1}\n",
        "data['n_legs'] = data['n_legs'].map(map_dict_legs)\n",
        "data['n_hands'] = data['n_hands'].map(map_dict_hands)\n",
        "data['n_eyes'] = data['n_eyes'].map(map_dict_eyes)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUwlOzWBo8Rx"
      },
      "source": [
        "### 4.3 Stratified Train/Val/Test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UqptiNjNQDW3",
        "outputId": "273c1740-234c-48f1-c728-3ddcb5f37e6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Label proportions:\n",
            "Train:\n",
            " label\n",
            "no_pain      0.771971\n",
            "low_pain     0.142518\n",
            "high_pain    0.085511\n",
            "Name: proportion, dtype: float64\n",
            "Val:\n",
            " label\n",
            "no_pain      0.775000\n",
            "low_pain     0.141667\n",
            "high_pain    0.083333\n",
            "Name: proportion, dtype: float64\n",
            "Test:\n",
            " label\n",
            "no_pain      0.775000\n",
            "low_pain     0.141667\n",
            "high_pain    0.083333\n",
            "Name: proportion, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# df has columns: ['sample_index', 'label']\n",
        "N_VAL_USERS = 120\n",
        "N_TEST_USERS = 120\n",
        "\n",
        "# --- Step 1: Compute each user's dominant label (or label distribution)\n",
        "user_labels = (\n",
        "    data.groupby('sample_index')['label']\n",
        "    .agg(lambda x: x.value_counts().index[0])  # dominant label per user\n",
        "    .reset_index()\n",
        ")\n",
        "\n",
        "train_users, temp_users = train_test_split(\n",
        "    user_labels['sample_index'],\n",
        "    test_size=(N_VAL_USERS + N_TEST_USERS) / len(user_labels),\n",
        "    stratify=user_labels['label'],\n",
        "    random_state=SEED\n",
        ")\n",
        "\n",
        "# Split temp into val/test (also stratified)\n",
        "temp_labels = user_labels[user_labels['sample_index'].isin(temp_users)]\n",
        "if N_TEST_USERS != 0:\n",
        "  val_users, test_users = train_test_split(\n",
        "      temp_labels['sample_index'],\n",
        "      test_size=N_TEST_USERS / (N_VAL_USERS + N_TEST_USERS),\n",
        "      stratify=temp_labels['label'],\n",
        "      random_state=SEED\n",
        "  )\n",
        "else:\n",
        "  val_users = temp_users\n",
        "  test_users = []\n",
        "\n",
        "# --- Step 3: Filter your main df\n",
        "df_train = data[data['sample_index'].isin(train_users)]\n",
        "df_val = data[data['sample_index'].isin(val_users)]\n",
        "df_test = data[data['sample_index'].isin(test_users)]\n",
        "\n",
        "# --- Step 4: Check label proportions\n",
        "print(\"Label proportions:\")\n",
        "print(\"Train:\\n\", df_train['label'].value_counts(normalize=True))\n",
        "print(\"Val:\\n\", df_val['label'].value_counts(normalize=True))\n",
        "print(\"Test:\\n\", df_test['label'].value_counts(normalize=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kI13WygTQDW4",
        "outputId": "d95cf8bc-2804-4743-8ef5-008ee718639e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((67360, 41), (19200, 41), (19200, 41))"
            ]
          },
          "execution_count": 652,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_train.shape, df_val.shape, df_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PsE1KsTTQDW4",
        "outputId": "41a6d896-82f5-487a-ff1d-59f437cd2b09"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total pirates in training set: 421\n",
            "Total pirates in validation set: 120\n",
            "Total pirates in test set: 120\n"
          ]
        }
      ],
      "source": [
        "# Print the total number of pirates for each dataset\n",
        "print(f\"Total pirates in training set: {df_train['sample_index'].nunique()}\")\n",
        "print(f\"Total pirates in validation set: {df_val['sample_index'].nunique()}\")\n",
        "print(f\"Total pirates in test set: {df_test['sample_index'].nunique()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9Pp8l1bo8Rx"
      },
      "source": [
        "### 4.4 Feature Normalization (min-max)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YkF51CmhQDW5"
      },
      "outputs": [],
      "source": [
        "# Define the columns to be normalised\n",
        "\n",
        "scale_columns = [\n",
        "    col for col in data.columns\n",
        "    if (col.startswith('joint_') or col.startswith('pain_survey')) and not col.startswith('joint_30')\n",
        "]\n",
        "\n",
        "\n",
        "\n",
        "# Calculate the minimum and maximum values from the training data only\n",
        "mins_train = df_train[scale_columns].min()\n",
        "maxs_train = df_train[scale_columns].max()\n",
        "\n",
        "#mins_val = df_val[scale_columns].min()\n",
        "#maxs_val = df_val[scale_columns].max()\n",
        "#\n",
        "#mins_test = df_test[scale_columns].min()\n",
        "#maxs_test = df_test[scale_columns].max()\n",
        "\n",
        "####\n",
        "#CHANGED ALL THE REGULARIZATION TO USE MIN AND MAX VALUES FROM THE TRAINING DATA FOR GENERALIZATION\n",
        "###\n",
        "\n",
        "# Apply normalisation to the specified columns in all datasets\n",
        "for column in scale_columns:\n",
        "    denom = maxs_train[column] - mins_train[column]\n",
        "    if np.isclose(denom, 0.0):\n",
        "        df_train[column] = 0.0\n",
        "        df_val[column] = 0.0\n",
        "        df_test[column] = 0.0\n",
        "        continue\n",
        "\n",
        "    # Normalise the training set\n",
        "    df_train[column] = (df_train[column] - mins_train[column]) / denom\n",
        "\n",
        "    # Normalise the validation set\n",
        "    df_val[column] = (df_val[column] - mins_train[column]) / denom\n",
        "\n",
        "    # Normalise the test set\n",
        "    df_test[column] = (df_test[column] - mins_train[column]) / denom\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        },
        "id": "4gQk4AVMBVuD",
        "outputId": "a636f911-38b3-42b2-bd7d-ec21ca78a1bd"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_train"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-169401c3-5530-4c3b-a2a3-1017027df4b9\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sample_index</th>\n",
              "      <th>time</th>\n",
              "      <th>pain_survey_1</th>\n",
              "      <th>pain_survey_2</th>\n",
              "      <th>pain_survey_3</th>\n",
              "      <th>pain_survey_4</th>\n",
              "      <th>n_legs</th>\n",
              "      <th>n_hands</th>\n",
              "      <th>n_eyes</th>\n",
              "      <th>joint_00</th>\n",
              "      <th>...</th>\n",
              "      <th>joint_22</th>\n",
              "      <th>joint_23</th>\n",
              "      <th>joint_24</th>\n",
              "      <th>joint_25</th>\n",
              "      <th>joint_26</th>\n",
              "      <th>joint_27</th>\n",
              "      <th>joint_28</th>\n",
              "      <th>joint_29</th>\n",
              "      <th>joint_30</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0.777507</td>\n",
              "      <td>...</td>\n",
              "      <td>1.374706e-06</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>3.162813e-04</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>0.014214</td>\n",
              "      <td>0.011376</td>\n",
              "      <td>0.018978</td>\n",
              "      <td>0.024117</td>\n",
              "      <td>0.5</td>\n",
              "      <td>no_pain</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0.806256</td>\n",
              "      <td>...</td>\n",
              "      <td>4.026521e-07</td>\n",
              "      <td>0.000022</td>\n",
              "      <td>9.828599e-07</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.010748</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.009473</td>\n",
              "      <td>0.011892</td>\n",
              "      <td>0.5</td>\n",
              "      <td>no_pain</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0.767592</td>\n",
              "      <td>...</td>\n",
              "      <td>1.440847e-08</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>6.626013e-05</td>\n",
              "      <td>0.000003</td>\n",
              "      <td>0.013097</td>\n",
              "      <td>0.006830</td>\n",
              "      <td>0.017065</td>\n",
              "      <td>0.020033</td>\n",
              "      <td>0.5</td>\n",
              "      <td>no_pain</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0.666220</td>\n",
              "      <td>...</td>\n",
              "      <td>3.065580e-07</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>1.199337e-06</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.009505</td>\n",
              "      <td>0.006274</td>\n",
              "      <td>0.020264</td>\n",
              "      <td>0.021371</td>\n",
              "      <td>0.5</td>\n",
              "      <td>no_pain</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0.774297</td>\n",
              "      <td>...</td>\n",
              "      <td>1.723863e-08</td>\n",
              "      <td>0.000006</td>\n",
              "      <td>1.307199e-06</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>0.004216</td>\n",
              "      <td>0.002132</td>\n",
              "      <td>0.023389</td>\n",
              "      <td>0.021961</td>\n",
              "      <td>0.5</td>\n",
              "      <td>no_pain</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0.813961</td>\n",
              "      <td>...</td>\n",
              "      <td>1.864695e-08</td>\n",
              "      <td>0.000001</td>\n",
              "      <td>1.414785e-06</td>\n",
              "      <td>0.000008</td>\n",
              "      <td>0.004861</td>\n",
              "      <td>0.005427</td>\n",
              "      <td>0.023442</td>\n",
              "      <td>0.020607</td>\n",
              "      <td>0.5</td>\n",
              "      <td>no_pain</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0.728617</td>\n",
              "      <td>...</td>\n",
              "      <td>2.005071e-08</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>4.297072e-05</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>0.005143</td>\n",
              "      <td>0.005407</td>\n",
              "      <td>0.022523</td>\n",
              "      <td>0.016522</td>\n",
              "      <td>0.5</td>\n",
              "      <td>no_pain</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0.737657</td>\n",
              "      <td>...</td>\n",
              "      <td>2.144985e-08</td>\n",
              "      <td>0.000008</td>\n",
              "      <td>4.049080e-05</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.012911</td>\n",
              "      <td>0.004546</td>\n",
              "      <td>0.025178</td>\n",
              "      <td>0.013640</td>\n",
              "      <td>0.5</td>\n",
              "      <td>no_pain</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0.699058</td>\n",
              "      <td>...</td>\n",
              "      <td>7.770963e-07</td>\n",
              "      <td>0.000001</td>\n",
              "      <td>1.457661e-05</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.016622</td>\n",
              "      <td>0.007172</td>\n",
              "      <td>0.006115</td>\n",
              "      <td>0.013229</td>\n",
              "      <td>0.5</td>\n",
              "      <td>no_pain</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>9 rows √ó 41 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-169401c3-5530-4c3b-a2a3-1017027df4b9')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-169401c3-5530-4c3b-a2a3-1017027df4b9 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-169401c3-5530-4c3b-a2a3-1017027df4b9');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-ae485e2f-a139-4cd5-846c-2a644b833df2\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ae485e2f-a139-4cd5-846c-2a644b833df2')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-ae485e2f-a139-4cd5-846c-2a644b833df2 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "   sample_index  time  pain_survey_1  pain_survey_2  pain_survey_3  \\\n",
              "0             0     0            1.0            0.0            1.0   \n",
              "1             0     1            1.0            1.0            1.0   \n",
              "2             0     2            1.0            0.0            1.0   \n",
              "3             0     3            1.0            1.0            1.0   \n",
              "4             0     4            1.0            1.0            1.0   \n",
              "5             0     5            1.0            0.0            1.0   \n",
              "6             0     6            1.0            0.5            1.0   \n",
              "7             0     7            1.0            1.0            1.0   \n",
              "8             0     8            1.0            1.0            0.0   \n",
              "\n",
              "   pain_survey_4  n_legs  n_hands  n_eyes  joint_00  ...      joint_22  \\\n",
              "0            0.5       2        2       2  0.777507  ...  1.374706e-06   \n",
              "1            1.0       2        2       2  0.806256  ...  4.026521e-07   \n",
              "2            1.0       2        2       2  0.767592  ...  1.440847e-08   \n",
              "3            1.0       2        2       2  0.666220  ...  3.065580e-07   \n",
              "4            1.0       2        2       2  0.774297  ...  1.723863e-08   \n",
              "5            0.5       2        2       2  0.813961  ...  1.864695e-08   \n",
              "6            0.5       2        2       2  0.728617  ...  2.005071e-08   \n",
              "7            1.0       2        2       2  0.737657  ...  2.144985e-08   \n",
              "8            0.5       2        2       2  0.699058  ...  7.770963e-07   \n",
              "\n",
              "   joint_23      joint_24  joint_25  joint_26  joint_27  joint_28  joint_29  \\\n",
              "0  0.000015  3.162813e-04  0.000004  0.014214  0.011376  0.018978  0.024117   \n",
              "1  0.000022  9.828599e-07  0.000000  0.010748  0.000000  0.009473  0.011892   \n",
              "2  0.000005  6.626013e-05  0.000003  0.013097  0.006830  0.017065  0.020033   \n",
              "3  0.000007  1.199337e-06  0.000000  0.009505  0.006274  0.020264  0.021371   \n",
              "4  0.000006  1.307199e-06  0.000007  0.004216  0.002132  0.023389  0.021961   \n",
              "5  0.000001  1.414785e-06  0.000008  0.004861  0.005427  0.023442  0.020607   \n",
              "6  0.000007  4.297072e-05  0.000005  0.005143  0.005407  0.022523  0.016522   \n",
              "7  0.000008  4.049080e-05  0.000000  0.012911  0.004546  0.025178  0.013640   \n",
              "8  0.000001  1.457661e-05  0.000000  0.016622  0.007172  0.006115  0.013229   \n",
              "\n",
              "   joint_30    label  \n",
              "0       0.5  no_pain  \n",
              "1       0.5  no_pain  \n",
              "2       0.5  no_pain  \n",
              "3       0.5  no_pain  \n",
              "4       0.5  no_pain  \n",
              "5       0.5  no_pain  \n",
              "6       0.5  no_pain  \n",
              "7       0.5  no_pain  \n",
              "8       0.5  no_pain  \n",
              "\n",
              "[9 rows x 41 columns]"
            ]
          },
          "execution_count": 655,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_train.head(9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V_1543vlBEqf"
      },
      "outputs": [],
      "source": [
        "# @title  Delete Some Columns Experimental\n",
        "\n",
        "#del_columns = [\n",
        "#    col for col in data.columns\n",
        "#    if not (col.startswith('pain_survey') or col.startswith('sample_index') or col.startswith('label') or col.startswith('time') or\n",
        "#            col.endswith('00') or col.endswith('01') or col.endswith('02') or col.endswith('03') or col.endswith('04') or col.endswith('05')\n",
        "#            or col.endswith('06') or col.endswith('07') or col.endswith('08') or col.endswith('09') or col.endswith('10') or col.endswith('11')\n",
        "#            or col.endswith('12') or col.endswith('25') or col.endswith('26') or col.endswith('27') or col.endswith('28') or col.endswith('29'))\n",
        "#]\n",
        "#\n",
        "#for column in del_columns:\n",
        "#\n",
        "#    # Normalise the training set\n",
        "#    df_train[column] =  0.0\n",
        "#    df_val[column] =  0.0\n",
        "#    df_test[column] =  0.0\n",
        "#\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 342
        },
        "id": "R-cyjeAuYt4D",
        "outputId": "78a664cc-1e31-4b1f-d5fa-f5b00aa7e882"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_train"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-2080bf71-2ad9-4462-88ba-eb922d72ceea\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sample_index</th>\n",
              "      <th>time</th>\n",
              "      <th>pain_survey_1</th>\n",
              "      <th>pain_survey_2</th>\n",
              "      <th>pain_survey_3</th>\n",
              "      <th>pain_survey_4</th>\n",
              "      <th>n_legs</th>\n",
              "      <th>n_hands</th>\n",
              "      <th>n_eyes</th>\n",
              "      <th>joint_00</th>\n",
              "      <th>...</th>\n",
              "      <th>joint_22</th>\n",
              "      <th>joint_23</th>\n",
              "      <th>joint_24</th>\n",
              "      <th>joint_25</th>\n",
              "      <th>joint_26</th>\n",
              "      <th>joint_27</th>\n",
              "      <th>joint_28</th>\n",
              "      <th>joint_29</th>\n",
              "      <th>joint_30</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0.777507</td>\n",
              "      <td>...</td>\n",
              "      <td>1.374706e-06</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>3.162813e-04</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>0.014214</td>\n",
              "      <td>0.011376</td>\n",
              "      <td>0.018978</td>\n",
              "      <td>0.024117</td>\n",
              "      <td>0.5</td>\n",
              "      <td>no_pain</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0.806256</td>\n",
              "      <td>...</td>\n",
              "      <td>4.026521e-07</td>\n",
              "      <td>0.000022</td>\n",
              "      <td>9.828599e-07</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.010748</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.009473</td>\n",
              "      <td>0.011892</td>\n",
              "      <td>0.5</td>\n",
              "      <td>no_pain</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0.767592</td>\n",
              "      <td>...</td>\n",
              "      <td>1.440847e-08</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>6.626013e-05</td>\n",
              "      <td>0.000003</td>\n",
              "      <td>0.013097</td>\n",
              "      <td>0.006830</td>\n",
              "      <td>0.017065</td>\n",
              "      <td>0.020033</td>\n",
              "      <td>0.5</td>\n",
              "      <td>no_pain</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0.666220</td>\n",
              "      <td>...</td>\n",
              "      <td>3.065580e-07</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>1.199337e-06</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.009505</td>\n",
              "      <td>0.006274</td>\n",
              "      <td>0.020264</td>\n",
              "      <td>0.021371</td>\n",
              "      <td>0.5</td>\n",
              "      <td>no_pain</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0.774297</td>\n",
              "      <td>...</td>\n",
              "      <td>1.723863e-08</td>\n",
              "      <td>0.000006</td>\n",
              "      <td>1.307199e-06</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>0.004216</td>\n",
              "      <td>0.002132</td>\n",
              "      <td>0.023389</td>\n",
              "      <td>0.021961</td>\n",
              "      <td>0.5</td>\n",
              "      <td>no_pain</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows √ó 41 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2080bf71-2ad9-4462-88ba-eb922d72ceea')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-2080bf71-2ad9-4462-88ba-eb922d72ceea button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-2080bf71-2ad9-4462-88ba-eb922d72ceea');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-d5a79566-d40e-48b2-ab21-47ac71027ea7\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d5a79566-d40e-48b2-ab21-47ac71027ea7')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-d5a79566-d40e-48b2-ab21-47ac71027ea7 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "   sample_index  time  pain_survey_1  pain_survey_2  pain_survey_3  \\\n",
              "0             0     0            1.0            0.0            1.0   \n",
              "1             0     1            1.0            1.0            1.0   \n",
              "2             0     2            1.0            0.0            1.0   \n",
              "3             0     3            1.0            1.0            1.0   \n",
              "4             0     4            1.0            1.0            1.0   \n",
              "\n",
              "   pain_survey_4  n_legs  n_hands  n_eyes  joint_00  ...      joint_22  \\\n",
              "0            0.5       2        2       2  0.777507  ...  1.374706e-06   \n",
              "1            1.0       2        2       2  0.806256  ...  4.026521e-07   \n",
              "2            1.0       2        2       2  0.767592  ...  1.440847e-08   \n",
              "3            1.0       2        2       2  0.666220  ...  3.065580e-07   \n",
              "4            1.0       2        2       2  0.774297  ...  1.723863e-08   \n",
              "\n",
              "   joint_23      joint_24  joint_25  joint_26  joint_27  joint_28  joint_29  \\\n",
              "0  0.000015  3.162813e-04  0.000004  0.014214  0.011376  0.018978  0.024117   \n",
              "1  0.000022  9.828599e-07  0.000000  0.010748  0.000000  0.009473  0.011892   \n",
              "2  0.000005  6.626013e-05  0.000003  0.013097  0.006830  0.017065  0.020033   \n",
              "3  0.000007  1.199337e-06  0.000000  0.009505  0.006274  0.020264  0.021371   \n",
              "4  0.000006  1.307199e-06  0.000007  0.004216  0.002132  0.023389  0.021961   \n",
              "\n",
              "   joint_30    label  \n",
              "0       0.5  no_pain  \n",
              "1       0.5  no_pain  \n",
              "2       0.5  no_pain  \n",
              "3       0.5  no_pain  \n",
              "4       0.5  no_pain  \n",
              "\n",
              "[5 rows x 41 columns]"
            ]
          },
          "execution_count": 657,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDuw27RUo8Rx"
      },
      "source": [
        "### 4.5 Label Distribution Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gDhUJUrkQDW6",
        "outputId": "6ae3fef7-ea53-47e2-9bc6-c31e7ce4ef82"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training labels: {'no_pain': 325, 'low_pain': 60, 'high_pain': 36}\n",
            "Validation labels: {'no_pain': 93, 'low_pain': 17, 'high_pain': 10}\n",
            "Test labels: {'no_pain': 93, 'low_pain': 17, 'high_pain': 10}\n"
          ]
        }
      ],
      "source": [
        "# Initialise a dictionary to count occurrences of each activity in the training set\n",
        "training_labels = {\n",
        "    'no_pain': 0,\n",
        "    'low_pain': 0,\n",
        "    'high_pain': 0\n",
        "}\n",
        "\n",
        "# Count occurrences of each activity for unique IDs in the training set\n",
        "for id in df_train['sample_index'].unique():\n",
        "    label = df_train[df_train['sample_index'] == id]['label'].values[0]\n",
        "    training_labels[label] += 1\n",
        "\n",
        "\n",
        "# Print the distribution of training labels\n",
        "print('Training labels:', training_labels)\n",
        "\n",
        "# Initialise a dictionary to count occurrences of each activity in the training set\n",
        "val_labels = {\n",
        "    'no_pain': 0,\n",
        "    'low_pain': 0,\n",
        "    'high_pain': 0\n",
        "}\n",
        "\n",
        "# Count occurrences of each activity for unique IDs in the training set\n",
        "for id in df_val['sample_index'].unique():\n",
        "    label = df_val[df_val['sample_index'] == id]['label'].values[0]\n",
        "    val_labels[label] += 1\n",
        "\n",
        "# Print the distribution of validation labels\n",
        "print('Validation labels:', val_labels)\n",
        "\n",
        "# Initialise a dictionary to count occurrences of each activity in the test set\n",
        "test_labels = {\n",
        "    'no_pain': 0,\n",
        "    'low_pain': 0,\n",
        "    'high_pain': 0\n",
        "}\n",
        "\n",
        "# Count occurrences of each activity for unique IDs in the test set\n",
        "for id in df_test['sample_index'].unique():\n",
        "    label = df_test[df_test['sample_index'] == id]['label'].values[0]\n",
        "    test_labels[label] += 1\n",
        "\n",
        "# Print the distribution of test labels\n",
        "print('Test labels:', test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DT3wxqdLQDW7"
      },
      "outputs": [],
      "source": [
        "# Define a training mapping of label names to integer labels\n",
        "label_mapping = {\n",
        "    'no_pain': 0,\n",
        "    'low_pain': 1,\n",
        "    'high_pain': 2\n",
        "}\n",
        "\n",
        "# Map label names to integers in the training set\n",
        "df_train['label'] = df_train['label'].map(label_mapping)\n",
        "\n",
        "# Map label names to integers in the validation set\n",
        "df_val['label'] = df_val['label'].map(label_mapping)\n",
        "\n",
        "# Map label names to integers in the test set\n",
        "df_test['label'] = df_test['label'].map(label_mapping)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kc0Ve8LYBYaZ",
        "outputId": "6b19cb47-1e41-484e-de0a-6ea0473f9bfa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   sample_index  time  pain_survey_1  pain_survey_2  pain_survey_3  \\\n",
            "0             0     0            1.0            0.0            1.0   \n",
            "1             0     1            1.0            1.0            1.0   \n",
            "2             0     2            1.0            0.0            1.0   \n",
            "\n",
            "   pain_survey_4  n_legs  n_hands  n_eyes  joint_00  ...      joint_22  \\\n",
            "0            0.5       2        2       2  0.777507  ...  1.374706e-06   \n",
            "1            1.0       2        2       2  0.806256  ...  4.026521e-07   \n",
            "2            1.0       2        2       2  0.767592  ...  1.440847e-08   \n",
            "\n",
            "   joint_23      joint_24  joint_25  joint_26  joint_27  joint_28  joint_29  \\\n",
            "0  0.000015  3.162813e-04  0.000004  0.014214  0.011376  0.018978  0.024117   \n",
            "1  0.000022  9.828599e-07  0.000000  0.010748  0.000000  0.009473  0.011892   \n",
            "2  0.000005  6.626013e-05  0.000003  0.013097  0.006830  0.017065  0.020033   \n",
            "\n",
            "   joint_30  label  \n",
            "0       0.5      0  \n",
            "1       0.5      0  \n",
            "2       0.5      0  \n",
            "\n",
            "[3 rows x 41 columns]\n"
          ]
        }
      ],
      "source": [
        "print(df_train.head(3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOi0Yx5bo8Ry"
      },
      "source": [
        "<a id=\"sequence-building\"></a>\n",
        "## 5. Sequence Building\n",
        "\n",
        "Convert variable-length time-series into fixed-size windows for RNN input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3qv_eAbDQDW7"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Define window and stride boolean variable -> if True, during training we will visit more time the same pirate with overlapping windows\n",
        "# if False, each pirate will be visited only once during training\n",
        "one_pirate_window = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sbmKJyH-QDW7"
      },
      "outputs": [],
      "source": [
        "if one_pirate_window:\n",
        "    # Define the window size\n",
        "    WINDOW_SIZE = 30 # before: 80\n",
        "\n",
        "    # Stride size\n",
        "    STRIDE = 10\n",
        "else:\n",
        "    # Define the window size -> select an higher window size in order to get more pirates\n",
        "    WINDOW_SIZE = 160\n",
        "\n",
        "    # Stride size\n",
        "    STRIDE = 160"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ut3JifRfo8Ry"
      },
      "source": [
        "### 5.1 Window & Stride Configuration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUdcu3oGo8Ry"
      },
      "source": [
        "### 5.2 Build Sequences Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dY-ksbv0QDW8"
      },
      "outputs": [],
      "source": [
        "def build_sequences(df, window=200, stride=200):\n",
        "    assert window % stride == 0\n",
        "\n",
        "    dataset = []\n",
        "    labels = []\n",
        "    ids = []  # <--- NEW: to store pirate/sample IDs\n",
        "\n",
        "    for id in df['sample_index'].unique():\n",
        "        columns = [col for col in df.columns if col not in ['sample_index', 'label', 'time']]\n",
        "        temp = df[df['sample_index'] == id][columns].values\n",
        "        label = df[df['sample_index'] == id]['label'].values[0]\n",
        "\n",
        "        remainder = len(temp) % window\n",
        "        padding_len = (window - remainder) % window\n",
        "        if padding_len:\n",
        "            padding = np.zeros((padding_len, len(columns)), dtype='float32')\n",
        "            temp = np.concatenate((temp, padding))\n",
        "\n",
        "        idx = 0\n",
        "        while idx + window <= len(temp):\n",
        "            dataset.append(temp[idx:idx + window])\n",
        "            labels.append(label)\n",
        "            ids.append(id)  # <--- NEW: add same ID for each window\n",
        "            idx += stride\n",
        "\n",
        "    dataset = np.array(dataset)\n",
        "    labels = np.array(labels)\n",
        "    ids = np.array(ids)  # <--- convert to numpy\n",
        "\n",
        "    return dataset, labels, ids  # <--- UPDATED return\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utsTrSH5o8Ry"
      },
      "source": [
        "### 5.3 Generate Sequences for Train/Val/Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPVEgwEmQDW8",
        "outputId": "0ba99416-aff9-4d68-c6b6-a0ace45137e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_train shape: (6736, 30, 38), y_train shape: (6736,)\n",
            "X_val shape: (1920, 30, 38), y_val shape: (1920,)\n",
            "X_test shape: (1920, 30, 38), y_test shape: (1920,)\n"
          ]
        }
      ],
      "source": [
        "# Generate sequences and labels for the training set\n",
        "X_train, y_train, ids_train = build_sequences(df_train, WINDOW_SIZE, STRIDE)\n",
        "\n",
        "# Generate sequences and labels for the validation set\n",
        "X_val, y_val, ids_val = build_sequences(df_val, WINDOW_SIZE, STRIDE)\n",
        "\n",
        "# Generate sequences and labels for the test set\n",
        "X_test, y_test, ids_test = build_sequences(df_test, WINDOW_SIZE, STRIDE)\n",
        "\n",
        "# Print the shapes of the generated datasets and their labels\n",
        "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
        "print(f\"X_val shape: {X_val.shape}, y_val shape: {y_val.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jLnJ0YMo8Rz"
      },
      "source": [
        "### 5.4 Data Type Conversion & Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X1mmtqJwQDW8"
      },
      "outputs": [],
      "source": [
        "# Convert dataset into float32 for PyTorch compatibility\n",
        "X_train = X_train.astype('float32')\n",
        "X_val = X_val.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "# y_train = y_train.astype('int64')\n",
        "# y_val = y_val.astype('int64')\n",
        "# y_test = y_test.astype('int64')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxfz2MegQDW8",
        "outputId": "f7a92858-4f52-437f-f2e4-7ff12234afe9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of Classes: 3\n"
          ]
        }
      ],
      "source": [
        "# Define the input shape based on the training data\n",
        "input_shape = X_train.shape[1:]\n",
        "\n",
        "# Define the number of classes based on the categorical labels\n",
        "num_classes = len(np.unique(y_train))\n",
        "print(f\"Number of Classes: {num_classes}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JcSpP3DBQDW8"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Discard nan values from the dataset\n",
        "if np.isnan(X_train).any() or np.isnan(X_val).any() or np.isnan(X_test).any():\n",
        "    X_train = np.nan_to_num(X_train)\n",
        "    X_val = np.nan_to_num(X_val)\n",
        "    X_test = np.nan_to_num(X_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s45svGakQDW8"
      },
      "outputs": [],
      "source": [
        "# Convert numpy arrays to PyTorch datasets (pairs features with labels)\n",
        "train_ds = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n",
        "val_ds   = TensorDataset(torch.from_numpy(X_val), torch.from_numpy(y_val))\n",
        "test_ds  = TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5b-SGD2to8Rz"
      },
      "source": [
        "<a id=\"dataloaders\"></a>\n",
        "## 6. DataLoaders\n",
        "\n",
        "Create PyTorch DataLoaders for efficient batching and parallel loading."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a2dd26aqQDW8"
      },
      "outputs": [],
      "source": [
        "# Define the batch size, which is the number of samples in each batch\n",
        "BATCH_SIZE = 1024 # we can change it depending on the GPU RAM available (by default 512)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8CPWMPHOQDW9"
      },
      "outputs": [],
      "source": [
        "def make_loader(ds, batch_size, shuffle, drop_last):\n",
        "    # Determine optimal number of worker processes for data loading\n",
        "    cpu_cores = os.cpu_count() or 2\n",
        "    num_workers = max(2, min(4, cpu_cores))\n",
        "\n",
        "    # Create DataLoader with performance optimizations\n",
        "    return DataLoader(\n",
        "        ds,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        drop_last=drop_last,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True,  # Faster GPU transfer\n",
        "        pin_memory_device=\"cuda\" if torch.cuda.is_available() else \"\",\n",
        "        prefetch_factor=4,  # Load 4 batches ahead\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cPlQI3R8QDW9"
      },
      "outputs": [],
      "source": [
        "# Create data loaders with different settings for each phase\n",
        "train_loader = make_loader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=False)\n",
        "val_loader   = make_loader(val_ds, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
        "test_loader  = make_loader(test_ds, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33LHsppfQDW-",
        "outputId": "b91d6b51-de80-449c-9d58-fe4dbdc6ec1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Features batch shape: torch.Size([1024, 30, 38])\n",
            "Labels batch shape: torch.Size([1024])\n"
          ]
        }
      ],
      "source": [
        "# Get one batch from the training data loader\n",
        "for xb, yb in train_loader:\n",
        "    print(\"Features batch shape:\", xb.shape)\n",
        "    print(\"Labels batch shape:\", yb.shape)\n",
        "    break # Stop after getting one batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jl4n83GrQDW-"
      },
      "outputs": [],
      "source": [
        "def recurrent_summary(model, input_size):\n",
        "    \"\"\"\n",
        "    Custom summary function that emulates torchinfo's output while correctly\n",
        "    counting parameters for RNN/GRU/LSTM layers.\n",
        "\n",
        "    This function is designed for models whose direct children are\n",
        "    nn.Linear, nn.RNN, nn.GRU, or nn.LSTM layers.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The model to analyze.\n",
        "        input_size (tuple): Shape of the input tensor (e.g., (seq_len, features)).\n",
        "    \"\"\"\n",
        "\n",
        "    # Dictionary to store output shapes captured by forward hooks\n",
        "    output_shapes = {}\n",
        "    # List to track hook handles for later removal\n",
        "    hooks = []\n",
        "\n",
        "    def get_hook(name):\n",
        "        \"\"\"Factory function to create a forward hook for a specific module.\"\"\"\n",
        "        def hook(module, input, output):\n",
        "            # Handle RNN layer outputs (returns a tuple)\n",
        "            if isinstance(output, tuple):\n",
        "                # output[0]: all hidden states with shape (batch, seq_len, hidden*directions)\n",
        "                shape1 = list(output[0].shape)\n",
        "                shape1[0] = -1  # Replace batch dimension with -1\n",
        "\n",
        "                # output[1]: final hidden state h_n (or tuple (h_n, c_n) for LSTM)\n",
        "                if isinstance(output[1], tuple):  # LSTM case: (h_n, c_n)\n",
        "                    shape2 = list(output[1][0].shape)  # Extract h_n only\n",
        "                else:  # RNN/GRU case: h_n only\n",
        "                    shape2 = list(output[1].shape)\n",
        "\n",
        "                # Replace batch dimension (middle position) with -1\n",
        "                shape2[1] = -1\n",
        "\n",
        "                output_shapes[name] = f\"[{shape1}, {shape2}]\"\n",
        "\n",
        "            # Handle standard layer outputs (e.g., Linear)\n",
        "            else:\n",
        "                shape = list(output.shape)\n",
        "                shape[0] = -1  # Replace batch dimension with -1\n",
        "                output_shapes[name] = f\"{shape}\"\n",
        "        return hook\n",
        "\n",
        "    # 1. Determine the device where model parameters reside\n",
        "    try:\n",
        "        device = next(model.parameters()).device\n",
        "    except StopIteration:\n",
        "        device = torch.device(\"cpu\")  # Fallback for models without parameters\n",
        "\n",
        "    # 2. Create a dummy input tensor with batch_size=1\n",
        "    dummy_input = torch.randn(1, *input_size).to(device)\n",
        "\n",
        "    # 3. Register forward hooks on target layers\n",
        "    # Iterate through direct children of the model (e.g., self.rnn, self.classifier)\n",
        "    for name, module in model.named_children():\n",
        "        if isinstance(module, (nn.Linear, nn.RNN, nn.GRU, nn.LSTM)):\n",
        "            # Register the hook and store its handle for cleanup\n",
        "            hook_handle = module.register_forward_hook(get_hook(name))\n",
        "            hooks.append(hook_handle)\n",
        "\n",
        "    # 4. Execute a dummy forward pass in evaluation mode\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        try:\n",
        "            model(dummy_input)\n",
        "        except Exception as e:\n",
        "            print(f\"Error during dummy forward pass: {e}\")\n",
        "            # Clean up hooks even if an error occurs\n",
        "            for h in hooks:\n",
        "                h.remove()\n",
        "            return\n",
        "\n",
        "    # 5. Remove all registered hooks\n",
        "    for h in hooks:\n",
        "        h.remove()\n",
        "\n",
        "    # --- 6. Print the summary table ---\n",
        "\n",
        "    print(\"-\" * 79)\n",
        "    # Column headers\n",
        "    print(f\"{'Layer (type)':<25} {'Output Shape':<28} {'Param #':<18}\")\n",
        "    print(\"=\" * 79)\n",
        "\n",
        "    total_params = 0\n",
        "    total_trainable_params = 0\n",
        "\n",
        "    # Iterate through modules again to collect and display parameter information\n",
        "    for name, module in model.named_children():\n",
        "        if name in output_shapes:\n",
        "            # Count total and trainable parameters for this module\n",
        "            module_params = sum(p.numel() for p in module.parameters())\n",
        "            trainable_params = sum(p.numel() for p in module.parameters() if p.requires_grad)\n",
        "\n",
        "            total_params += module_params\n",
        "            total_trainable_params += trainable_params\n",
        "\n",
        "            # Format strings for display\n",
        "            layer_name = f\"{name} ({type(module).__name__})\"\n",
        "            output_shape_str = str(output_shapes[name])\n",
        "            params_str = f\"{trainable_params:,}\"\n",
        "\n",
        "            print(f\"{layer_name:<25} {output_shape_str:<28} {params_str:<15}\")\n",
        "\n",
        "    print(\"=\" * 79)\n",
        "    print(f\"Total params: {total_params:,}\")\n",
        "    print(f\"Trainable params: {total_trainable_params:,}\")\n",
        "    print(f\"Non-trainable params: {total_params - total_trainable_params:,}\")\n",
        "    print(\"-\" * 79)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5VULB4UQDW_"
      },
      "source": [
        "<a id=\"hyperparameters\"></a>\n",
        "## 7. Network Hyperparameters\n",
        "\n",
        "Configure training settings, architecture parameters, and regularization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0a183G6zQDW_"
      },
      "outputs": [],
      "source": [
        " # Training configuration\n",
        "LEARNING_RATE = 1e-3\n",
        "EPOCHS = 500\n",
        "PATIENCE = 40\n",
        "\n",
        "# Architecture\n",
        "HIDDEN_LAYERS = 2        # Hidden layers\n",
        "HIDDEN_SIZE = [32,16,32,16]   # Neurons per layer -> prev hidden size = 128\n",
        "\n",
        "# Regularisation\n",
        "DROPOUT_RATE = 0.5     # Dropout probability\n",
        "\n",
        "# For now disable weight decay\n",
        "L1_LAMBDA = 0.0001       # L1 penalty\n",
        "L2_LAMBDA = 0.001         # L2 penalty\n",
        "\n",
        "# Set up loss function and optimizer\n",
        "weights = torch.tensor([0.8, 1.0, 1.2]).to(device)\n",
        "\n",
        "\n",
        "\n",
        "# TO WEIGHT MORE THE \"MORE DIFFICULT\" CASES AND THE LESS FREQUENT LABELS:\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=None, gamma=2.0):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.ce = nn.CrossEntropyLoss(weight=alpha, reduction='none')\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        ce_loss = self.ce(inputs, targets)\n",
        "        pt = torch.exp(-ce_loss)\n",
        "        focal_loss = ((1 - pt) ** self.gamma) * ce_loss\n",
        "        return focal_loss.mean()\n",
        "\n",
        "alpha = None  # None if we don't want to alterate the weights of each label losses (FocalLoss already do it)\n",
        "criterion = FocalLoss(alpha=alpha, gamma=1.3)  # gamma = 0 it's like Crossentropy(), gamma < 1 it's like in between Crossentropy and FocalLoss,\n",
        "                                               # gamma = 1 it's a good compromise, gamma = 1.5 or gamma = 2 to weight so much the less present labels\n",
        "\n",
        "\n",
        "#criterion = nn.CrossEntropyLoss(weight=weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7-RAXl_BQDXA"
      },
      "outputs": [],
      "source": [
        "# Initialize best model tracking variables\n",
        "best_model = None\n",
        "best_performance = float('-inf')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ouaaN67Ho8R3"
      },
      "source": [
        "<a id=\"model-architecture\"></a>\n",
        "## 8. Model Architecture\n",
        "\n",
        "Custom RNN/LSTM/GRU classifier with configurable bidirectionality and dropout."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4I8vKDBo8R4"
      },
      "source": [
        "### 7.1 Recurrent Classifier Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6nyxSmz4QDW_"
      },
      "outputs": [],
      "source": [
        "class RecurrentClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    Generic RNN classifier (RNN, LSTM, GRU).\n",
        "    Uses the last hidden state for classification.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "            self,\n",
        "            input_size,\n",
        "            hidden_size,\n",
        "            num_layers,\n",
        "            num_classes,\n",
        "            rnn_type=  'LSTM',        # 'RNN', 'LSTM', or 'GRU'\n",
        "            bidirectional=False,\n",
        "            dropout_rate=0.2\n",
        "            ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.rnn_type = rnn_type\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_size = hidden_size\n",
        "        self.bidirectional = bidirectional\n",
        "\n",
        "        # Map string name to PyTorch RNN class\n",
        "        rnn_map = {\n",
        "            'RNN': nn.RNN,\n",
        "            'LSTM': nn.LSTM,\n",
        "            'GRU': nn.GRU\n",
        "        }\n",
        "\n",
        "        if rnn_type not in rnn_map:\n",
        "            raise ValueError(\"rnn_type must be 'RNN', 'LSTM', or 'GRU'\")\n",
        "\n",
        "        rnn_module = rnn_map[rnn_type]\n",
        "\n",
        "        # Dropout is only applied between layers (if num_layers > 1)\n",
        "        dropout_val = dropout_rate if num_layers > 1 else 0 # dropout between RNN layers, applied for regularization\n",
        "\n",
        "        # Create the recurrent layer\n",
        "        self.rnn = rnn_module(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,       # Input shape: (batch, seq_len, features)\n",
        "            bidirectional=bidirectional, # We are defining a bidirectional RNN since we want to extract also the future contextual information for making better predictions\n",
        "            dropout=dropout_val\n",
        "        )\n",
        "\n",
        "        # Calculate input size for the final classifier\n",
        "        if self.bidirectional:\n",
        "            classifier_input_size = hidden_size * 2 # Concat fwd + bwd\n",
        "        else:\n",
        "            classifier_input_size = hidden_size\n",
        "\n",
        "        # Final classification layer\n",
        "        self.classifier = nn.Linear(classifier_input_size, num_classes) # output layer for classifying\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x shape: (batch_size, seq_length, input_size)\n",
        "        \"\"\"\n",
        "\n",
        "        # rnn_out shape: (batch_size, seq_len, hidden_size * num_directions)\n",
        "        rnn_out, hidden = self.rnn(x) # feeds the input sequence into the RNN layer\n",
        "        # rnn_out -> contains the hidden state output for every timestep\n",
        "\n",
        "        # LSTM returns (h_n, c_n), we only need h_n\n",
        "        if self.rnn_type == 'LSTM':\n",
        "            hidden = hidden[0]  # final hidden state of the last timestep\n",
        "\n",
        "        # hidden shape: (num_layers * num_directions, batch_size, hidden_size)\n",
        "\n",
        "        if self.bidirectional:\n",
        "            # For bidirectional, hidden states are interleaved:\n",
        "            # [layer_0_fwd, layer_0_bwd, layer_1_fwd, layer_1_bwd, ...]\n",
        "            # We want the last layer's forward and backward states\n",
        "            fwd_hidden = hidden[-2, :, :]  # Last layer, forward direction\n",
        "            bwd_hidden = hidden[-1, :, :]  # Last layer, backward direction\n",
        "            hidden_to_classify = torch.cat([fwd_hidden, bwd_hidden], dim=1)\n",
        "        else:\n",
        "            hidden_to_classify = hidden[-1]\n",
        "\n",
        "        # Get logits\n",
        "        logits = self.classifier(hidden_to_classify)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r82JJturB8f0"
      },
      "outputs": [],
      "source": [
        "class FlexibleRecurrentClassifier(nn.Module):\n",
        "    def __init__(self, input_size, hidden_sizes, num_classes,\n",
        "                 rnn_type='LSTM', bidirectional=False, dropout_rate=0.2,\n",
        "                 use_batch_norm=False):\n",
        "        super().__init__()\n",
        "        assert isinstance(hidden_sizes, (list, tuple)) and len(hidden_sizes) >= 1\n",
        "\n",
        "        self.rnn_type = rnn_type\n",
        "        self.bidirectional = bidirectional\n",
        "        self.num_layers = len(hidden_sizes)\n",
        "        self.use_batch_norm = use_batch_norm\n",
        "\n",
        "        rnn_map = {'RNN': nn.RNN, 'LSTM': nn.LSTM, 'GRU': nn.GRU}\n",
        "        if rnn_type not in rnn_map:\n",
        "            raise ValueError(\"rnn_type must be 'RNN', 'LSTM', or 'GRU'\")\n",
        "\n",
        "        rnn_module = rnn_map[rnn_type]\n",
        "        self.rnns = nn.ModuleList()\n",
        "        self.batch_norms = nn.ModuleList() if use_batch_norm else None\n",
        "\n",
        "        input_dim = input_size\n",
        "        for hidden_dim in hidden_sizes:\n",
        "            self.rnns.append(\n",
        "                rnn_module(\n",
        "                    input_size=input_dim,\n",
        "                    hidden_size=hidden_dim,\n",
        "                    num_layers=1,\n",
        "                    batch_first=True,\n",
        "                    bidirectional=bidirectional,\n",
        "                    dropout=0.0\n",
        "                )\n",
        "            )\n",
        "            output_dim = hidden_dim * (2 if bidirectional else 1)\n",
        "\n",
        "            if use_batch_norm:\n",
        "                self.batch_norms.append(nn.BatchNorm1d(output_dim))\n",
        "\n",
        "            input_dim = output_dim\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.classifier = nn.Linear(input_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: (batch_size, seq_len, input_size)\n",
        "        Returns:\n",
        "            logits: (batch_size, num_classes)\n",
        "        \"\"\"\n",
        "        out = x\n",
        "\n",
        "        for i, rnn in enumerate(self.rnns):\n",
        "            out, hidden = rnn(out)\n",
        "\n",
        "            if self.use_batch_norm:\n",
        "                # (batch, seq, features) -> (batch, features, seq)\n",
        "                out = out.transpose(1, 2)\n",
        "                out = self.batch_norms[i](out)\n",
        "                out = out.transpose(1, 2)\n",
        "\n",
        "            out = self.dropout(out)\n",
        "\n",
        "        # Use final timestep output (more common than hidden state)\n",
        "        final_output = out[:, -1, :]  # (batch, hidden_dim * num_directions)\n",
        "\n",
        "        logits = self.classifier(final_output)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLdCrp-qQDXA"
      },
      "source": [
        "<a id=\"training-functions\"></a>\n",
        "## 9. Training Functions\n",
        "\n",
        "Helper functions for training, validation, logging, and early stopping."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7QgHx0BHQDXA"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(model, train_loader, criterion, optimizer, scaler, device, l1_lambda=0, l2_lambda=0):\n",
        "    \"\"\"\n",
        "    Perform one complete training epoch through the entire training dataset.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The neural network model to train\n",
        "        train_loader (DataLoader): PyTorch DataLoader containing training data batches\n",
        "        criterion (nn.Module): Loss function (e.g., CrossEntropyLoss, MSELoss)\n",
        "        optimizer (torch.optim): Optimization algorithm (e.g., Adam, SGD)\n",
        "        scaler (GradScaler): PyTorch's gradient scaler for mixed precision training\n",
        "        device (torch.device): Computing device ('cuda' for GPU, 'cpu' for CPU)\n",
        "        l1_lambda (float): Lambda for L1 regularization\n",
        "        l2_lambda (float): Lambda for L2 regularization\n",
        "\n",
        "    Returns:\n",
        "        tuple: (average_loss, f1 score) - Training loss and f1 score for this epoch\n",
        "    \"\"\"\n",
        "    model.train()  # Set model to training mode\n",
        "\n",
        "    running_loss = 0.0\n",
        "    all_predictions = []\n",
        "    all_targets = []\n",
        "\n",
        "    # Iterate through training batches\n",
        "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "        # Move data to device (GPU/CPU)\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        # Clear gradients from previous step\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        # Forward pass with mixed precision (if CUDA available)\n",
        "        with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n",
        "            logits = model(inputs)\n",
        "            loss = criterion(logits, targets)\n",
        "\n",
        "            # Add L1 and L2 regularization\n",
        "            l1_norm = sum(p.abs().sum() for p in model.parameters())\n",
        "            l2_norm = sum(p.pow(2).sum() for p in model.parameters())\n",
        "            loss = loss + l1_lambda * l1_norm + l2_lambda * l2_norm\n",
        "\n",
        "\n",
        "        # Backward pass with gradient scaling\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        # Accumulate metrics\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        predictions = logits.argmax(dim=1)\n",
        "        all_predictions.append(predictions.cpu().numpy())\n",
        "        all_targets.append(targets.cpu().numpy())\n",
        "\n",
        "    # Calculate epoch metrics\n",
        "    epoch_loss = running_loss / len(train_loader.dataset)\n",
        "    epoch_f1 = f1_score(\n",
        "        np.concatenate(all_targets),\n",
        "        np.concatenate(all_predictions),\n",
        "        average='weighted'\n",
        "    )\n",
        "\n",
        "    return epoch_loss, epoch_f1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKsdfMrSo8R4"
      },
      "source": [
        "### 9.1 Train One Epoch Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVlcqfzqQDXA"
      },
      "outputs": [],
      "source": [
        "def validate_one_epoch(model, val_loader, criterion, device):\n",
        "    \"\"\"\n",
        "    Perform one complete validation epoch through the entire validation dataset.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The neural network model to evaluate (must be in eval mode)\n",
        "        val_loader (DataLoader): PyTorch DataLoader containing validation data batches\n",
        "        criterion (nn.Module): Loss function used to calculate validation loss\n",
        "        device (torch.device): Computing device ('cuda' for GPU, 'cpu' for CPU)\n",
        "\n",
        "    Returns:\n",
        "        tuple: (average_loss, accuracy) - Validation loss and accuracy for this epoch\n",
        "\n",
        "    Note:\n",
        "        This function automatically sets the model to evaluation mode and disables\n",
        "        gradient computation for efficiency during validation.\n",
        "    \"\"\"\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "\n",
        "    running_loss = 0.0\n",
        "    all_predictions = []\n",
        "    all_targets = []\n",
        "\n",
        "    # Disable gradient computation for validation\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in val_loader:\n",
        "            # Move data to device\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            # Forward pass with mixed precision (if CUDA available)\n",
        "            with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n",
        "                logits = model(inputs)\n",
        "                loss = criterion(logits, targets)\n",
        "\n",
        "            # Accumulate metrics\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            predictions = logits.argmax(dim=1)\n",
        "            all_predictions.append(predictions.cpu().numpy())\n",
        "            all_targets.append(targets.cpu().numpy())\n",
        "\n",
        "    # Calculate epoch metrics\n",
        "    epoch_loss = running_loss / len(val_loader.dataset)\n",
        "    epoch_accuracy = f1_score(\n",
        "        np.concatenate(all_targets),\n",
        "        np.concatenate(all_predictions),\n",
        "        average='weighted'\n",
        "    )\n",
        "\n",
        "    return epoch_loss, epoch_accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAGFlt3Fo8R4"
      },
      "source": [
        "### 9.2 Validate One Epoch Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZwJNmCZQDXA"
      },
      "outputs": [],
      "source": [
        "def log_metrics_to_tensorboard(writer, epoch, train_loss, train_f1, val_loss, val_f1, model):\n",
        "    \"\"\"\n",
        "    Log training metrics and model parameters to TensorBoard for visualization.\n",
        "\n",
        "    Args:\n",
        "        writer (SummaryWriter): TensorBoard SummaryWriter object for logging\n",
        "        epoch (int): Current epoch number (used as x-axis in TensorBoard plots)\n",
        "        train_loss (float): Training loss for this epoch\n",
        "        train_f1 (float): Training f1 score for this epoch\n",
        "        val_loss (float): Validation loss for this epoch\n",
        "        val_f1 (float): Validation f1 score for this epoch\n",
        "        model (nn.Module): The neural network model (for logging weights/gradients)\n",
        "\n",
        "    Note:\n",
        "        This function logs scalar metrics (loss/f1 score) and histograms of model\n",
        "        parameters and gradients, which helps monitor training progress and detect\n",
        "        issues like vanishing/exploding gradients.\n",
        "    \"\"\"\n",
        "    # Log scalar metrics\n",
        "    writer.add_scalar('Loss/Training', train_loss, epoch)\n",
        "    writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
        "    writer.add_scalar('F1/Training', train_f1, epoch)\n",
        "    writer.add_scalar('F1/Validation', val_f1, epoch)\n",
        "\n",
        "    # Log model parameters and gradients\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.requires_grad:\n",
        "            # Check if the tensor is not empty before adding a histogram\n",
        "            if param.numel() > 0:\n",
        "                writer.add_histogram(f'{name}/weights', param.data, epoch)\n",
        "            if param.grad is not None:\n",
        "                # Check if the gradient tensor is not empty before adding a histogram\n",
        "                if param.grad.numel() > 0:\n",
        "                    if param.grad is not None and torch.isfinite(param.grad).all():\n",
        "                        writer.add_histogram(f'{name}/gradients', param.grad.data, epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOa3T9Pho8R4"
      },
      "source": [
        "### 9.3 Fit  Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KgpOY62qQDXB"
      },
      "outputs": [],
      "source": [
        "def fit(model, train_loader, val_loader, epochs, criterion, optimizer, scaler, device,\n",
        "        l1_lambda=0, l2_lambda=0, patience=0, evaluation_metric=\"val_f1\", mode='max',\n",
        "        restore_best_weights=True, writer=None, verbose=10, experiment_name=\"\",save_model=True):\n",
        "    \"\"\"\n",
        "    Train the neural network model on the training data and validate on the validation data.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The neural network model to train\n",
        "        train_loader (DataLoader): PyTorch DataLoader containing training data batches\n",
        "        val_loader (DataLoader): PyTorch DataLoader containing validation data batches\n",
        "        epochs (int): Number of training epochs\n",
        "        criterion (nn.Module): Loss function (e.g., CrossEntropyLoss, MSELoss)\n",
        "        optimizer (torch.optim): Optimization algorithm (e.g., Adam, SGD)\n",
        "        scaler (GradScaler): PyTorch's gradient scaler for mixed precision training\n",
        "        device (torch.device): Computing device ('cuda' for GPU, 'cpu' for CPU)\n",
        "        l1_lambda (float): L1 regularization coefficient (default: 0)\n",
        "        l2_lambda (float): L2 regularization coefficient (default: 0)\n",
        "        patience (int): Number of epochs to wait for improvement before early stopping (default: 0)\n",
        "        evaluation_metric (str): Metric to monitor for early stopping (default: \"val_f1\")\n",
        "        mode (str): 'max' for maximizing the metric, 'min' for minimizing (default: 'max')\n",
        "        restore_best_weights (bool): Whether to restore model weights from best epoch (default: True)\n",
        "        writer (SummaryWriter, optional): TensorBoard SummaryWriter object for logging (default: None)\n",
        "        verbose (int, optional): Frequency of printing training progress (default: 10)\n",
        "        experiment_name (str, optional): Experiment name for saving models (default: \"\")\n",
        "\n",
        "    Returns:\n",
        "        tuple: (model, training_history) - Trained model and metrics history\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize metrics tracking\n",
        "    training_history = {\n",
        "        'train_loss': [], 'val_loss': [],\n",
        "        'train_f1': [], 'val_f1': []\n",
        "    }\n",
        "\n",
        "    # Configure early stopping if patience is set\n",
        "    if patience > 0:\n",
        "        patience_counter = 0\n",
        "        best_metric = float('-inf') if mode == 'max' else float('inf')\n",
        "        best_epoch = 0\n",
        "\n",
        "    print(f\"Training {epochs} epochs...\")\n",
        "\n",
        "    # Main training loop: iterate through epochs\n",
        "    for epoch in range(1, epochs + 1):\n",
        "\n",
        "        # Forward pass through training data, compute gradients, update weights\n",
        "        train_loss, train_f1 = train_one_epoch(\n",
        "            model, train_loader, criterion, optimizer, scaler, device, l1_lambda, l2_lambda\n",
        "        )\n",
        "\n",
        "        # Evaluate model on validation data without updating weights\n",
        "        val_loss, val_f1 = validate_one_epoch(\n",
        "            model, val_loader, criterion, device\n",
        "        )\n",
        "\n",
        "        # Store metrics for plotting and analysis\n",
        "        training_history['train_loss'].append(train_loss)\n",
        "        training_history['val_loss'].append(val_loss)\n",
        "        training_history['train_f1'].append(train_f1)\n",
        "        training_history['val_f1'].append(val_f1)\n",
        "\n",
        "        # Write metrics to TensorBoard for visualization\n",
        "        if writer is not None:\n",
        "            log_metrics_to_tensorboard(\n",
        "                writer, epoch, train_loss, train_f1, val_loss, val_f1, model\n",
        "            )\n",
        "\n",
        "        # Print progress every N epochs or on first epoch\n",
        "        if verbose > 0:\n",
        "            if epoch % verbose == 0 or epoch == 1:\n",
        "                print(f\"Epoch {epoch:3d}/{epochs} | \"\n",
        "                    f\"Train: Loss={train_loss:.4f}, F1 Score={train_f1:.4f} | \"\n",
        "                    f\"Val: Loss={val_loss:.4f}, F1 Score={val_f1:.4f}\")\n",
        "\n",
        "        # Early stopping logic: monitor metric and save best model\n",
        "        if patience > 0:\n",
        "            current_metric = training_history[evaluation_metric][-1]\n",
        "            is_improvement = (current_metric > best_metric) if mode == 'max' else (current_metric < best_metric)\n",
        "\n",
        "            if is_improvement :\n",
        "                best_metric = current_metric\n",
        "                best_epoch = epoch\n",
        "                if save_model:\n",
        "                  torch.save(model.state_dict(), f\"{models_dir}/{experiment_name}_model.pt\")\n",
        "                patience_counter = 0\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "                if patience_counter >= patience:\n",
        "                    print(f\"Early stopping triggered after {epoch} epochs.\")\n",
        "                    break\n",
        "\n",
        "    # Restore best model weights if early stopping was used\n",
        "    if restore_best_weights and patience > 0 and save_model:\n",
        "        model.load_state_dict(torch.load(f\"{models_dir}/{experiment_name}_model.pt\"))\n",
        "        print(f\"Best model restored from epoch {best_epoch} with {evaluation_metric} {best_metric:.4f}\")\n",
        "\n",
        "    # Save final model if no early stopping\n",
        "    if patience == 0 and save_model:\n",
        "        torch.save(model.state_dict(), f\"{models_dir}/{experiment_name}_model.pt\")\n",
        "    if not save_model:\n",
        "        print(\"Model saving turned off.\")\n",
        "\n",
        "    # Close TensorBoard writer\n",
        "    if writer is not None:\n",
        "        writer.close()\n",
        "\n",
        "    return model, training_history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8g2Pr_To8R5"
      },
      "source": [
        "### 9.4 Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Hyperparameter Grid Search\n",
        "GRID_EPOCHS = 160\n",
        "GRID_PATIENCE = 20\n",
        "base_params = {\n",
        "    'RNN_TYPE': ['LSTM','GRU','RNN'],\n",
        "    'BIDIRECTIONAL': [True, False],\n",
        "    'HIDDEN_SIZE': [64],\n",
        "    'HIDDEN_LAYERS': [2],\n",
        "    'LEARNING_RATE': [1e-3],\n",
        "    'DROPOUT_RATE': [0.3],\n",
        "    'BATCH_SIZE': [512],\n",
        "    'L1_LAMBDA': [0.0001],\n",
        "    'L2_LAMBDA': [0.001],\n",
        "    'WINDOW': [40],\n",
        "    'STRIDE': [5],\n",
        "    'CRITERION': ['CROSS', 'FOCAL'],\n",
        "}\n",
        "\n",
        "# Conditional additions\n",
        "weights = [[1.0, 1.0, 1.0]]\n",
        "gammas = [1.3]\n",
        "\n",
        "param_grid = []\n",
        "\n",
        "for criterion in base_params['CRITERION']:\n",
        "    params = base_params.copy()\n",
        "    params = {k: v for k, v in base_params.items() if k != 'CRITERION'}\n",
        "    params['CRITERION'] = [criterion]\n",
        "    \n",
        "    if criterion == 'CROSS':\n",
        "        params['WEIGHTS'] = weights\n",
        "    elif criterion == 'FOCAL':\n",
        "        params['GAMMA'] = gammas\n",
        "    \n",
        "    # Expand all combinations\n",
        "    keys, values = zip(*params.items())\n",
        "    for combo in product(*values):\n",
        "        param_grid.append(dict(zip(keys, combo)))\n",
        "\n",
        "print(f\"Generated {len(param_grid)} combinations.\")\n",
        "results = []\n",
        "grid = list(ParameterGrid(param_grid))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00tkWSbCQDXB",
        "outputId": "704cb1b8-d732-4804-d4ca-d74fcbdb7f5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------------------------------------------------------------------------\n",
            "Layer (type)              Output Shape                 Param #           \n",
            "===============================================================================\n",
            "classifier (Linear)       [-1, 3]                      99             \n",
            "===============================================================================\n",
            "Total params: 99\n",
            "Trainable params: 99\n",
            "Non-trainable params: 0\n",
            "-------------------------------------------------------------------------------\n",
            "Training 500 epochs...\n",
            "Epoch   1/500 | Train: Loss=1.2342, F1 Score=0.1460 | Val: Loss=0.6214, F1 Score=0.0352\n",
            "Epoch   2/500 | Train: Loss=1.1291, F1 Score=0.5525 | Val: Loss=0.5341, F1 Score=0.6768\n",
            "Epoch   3/500 | Train: Loss=1.0163, F1 Score=0.6717 | Val: Loss=0.4369, F1 Score=0.6768\n",
            "Epoch   4/500 | Train: Loss=0.9195, F1 Score=0.6747 | Val: Loss=0.3881, F1 Score=0.6768\n",
            "Epoch   5/500 | Train: Loss=0.8731, F1 Score=0.6725 | Val: Loss=0.3931, F1 Score=0.6768\n",
            "Epoch   6/500 | Train: Loss=0.8485, F1 Score=0.6727 | Val: Loss=0.3889, F1 Score=0.6768\n",
            "Epoch   7/500 | Train: Loss=0.8155, F1 Score=0.6725 | Val: Loss=0.3828, F1 Score=0.6768\n",
            "Epoch   8/500 | Train: Loss=0.7855, F1 Score=0.6724 | Val: Loss=0.3825, F1 Score=0.6768\n",
            "Epoch   9/500 | Train: Loss=0.7629, F1 Score=0.6725 | Val: Loss=0.3822, F1 Score=0.6768\n",
            "Epoch  10/500 | Train: Loss=0.7377, F1 Score=0.6726 | Val: Loss=0.3818, F1 Score=0.6768\n",
            "Epoch  11/500 | Train: Loss=0.7117, F1 Score=0.6726 | Val: Loss=0.3818, F1 Score=0.6768\n",
            "Epoch  12/500 | Train: Loss=0.6890, F1 Score=0.6726 | Val: Loss=0.3817, F1 Score=0.6768\n",
            "Epoch  13/500 | Train: Loss=0.6667, F1 Score=0.6726 | Val: Loss=0.3816, F1 Score=0.6768\n",
            "Epoch  14/500 | Train: Loss=0.6494, F1 Score=0.6726 | Val: Loss=0.3816, F1 Score=0.6768\n",
            "Epoch  15/500 | Train: Loss=0.6284, F1 Score=0.6726 | Val: Loss=0.3815, F1 Score=0.6768\n",
            "Epoch  16/500 | Train: Loss=0.6105, F1 Score=0.6726 | Val: Loss=0.3814, F1 Score=0.6768\n",
            "Epoch  17/500 | Train: Loss=0.5951, F1 Score=0.6726 | Val: Loss=0.3813, F1 Score=0.6768\n",
            "Epoch  18/500 | Train: Loss=0.5795, F1 Score=0.6726 | Val: Loss=0.3813, F1 Score=0.6768\n",
            "Epoch  19/500 | Train: Loss=0.5636, F1 Score=0.6726 | Val: Loss=0.3813, F1 Score=0.6768\n",
            "Epoch  20/500 | Train: Loss=0.5459, F1 Score=0.6726 | Val: Loss=0.3812, F1 Score=0.6768\n",
            "Epoch  21/500 | Train: Loss=0.5338, F1 Score=0.6726 | Val: Loss=0.3811, F1 Score=0.6768\n",
            "Epoch  22/500 | Train: Loss=0.5248, F1 Score=0.6726 | Val: Loss=0.3811, F1 Score=0.6768\n",
            "Epoch  23/500 | Train: Loss=0.5160, F1 Score=0.6726 | Val: Loss=0.3811, F1 Score=0.6768\n",
            "Epoch  24/500 | Train: Loss=0.5072, F1 Score=0.6726 | Val: Loss=0.3811, F1 Score=0.6768\n",
            "Epoch  25/500 | Train: Loss=0.4975, F1 Score=0.6726 | Val: Loss=0.3808, F1 Score=0.6768\n",
            "Epoch  26/500 | Train: Loss=0.4890, F1 Score=0.6726 | Val: Loss=0.3808, F1 Score=0.6768\n",
            "Epoch  27/500 | Train: Loss=0.4835, F1 Score=0.6726 | Val: Loss=0.3808, F1 Score=0.6768\n",
            "Epoch  28/500 | Train: Loss=0.4776, F1 Score=0.6726 | Val: Loss=0.3805, F1 Score=0.6768\n",
            "Epoch  29/500 | Train: Loss=0.4715, F1 Score=0.6726 | Val: Loss=0.3801, F1 Score=0.6768\n",
            "Epoch  30/500 | Train: Loss=0.4629, F1 Score=0.6726 | Val: Loss=0.3793, F1 Score=0.6768\n",
            "Epoch  31/500 | Train: Loss=0.4572, F1 Score=0.6726 | Val: Loss=0.3773, F1 Score=0.6768\n",
            "Epoch  32/500 | Train: Loss=0.4450, F1 Score=0.6726 | Val: Loss=0.3744, F1 Score=0.6768\n",
            "Epoch  33/500 | Train: Loss=0.4230, F1 Score=0.6726 | Val: Loss=0.3666, F1 Score=0.6768\n",
            "Epoch  34/500 | Train: Loss=0.4045, F1 Score=0.7062 | Val: Loss=0.3747, F1 Score=0.7006\n",
            "Epoch  35/500 | Train: Loss=0.4103, F1 Score=0.7411 | Val: Loss=0.3809, F1 Score=0.6910\n",
            "Epoch  36/500 | Train: Loss=0.4013, F1 Score=0.7494 | Val: Loss=0.3611, F1 Score=0.7059\n",
            "Epoch  37/500 | Train: Loss=0.3851, F1 Score=0.7529 | Val: Loss=0.3657, F1 Score=0.7064\n",
            "Epoch  38/500 | Train: Loss=0.3801, F1 Score=0.7550 | Val: Loss=0.3640, F1 Score=0.7105\n",
            "Epoch  39/500 | Train: Loss=0.3821, F1 Score=0.7534 | Val: Loss=0.3762, F1 Score=0.7104\n",
            "Epoch  40/500 | Train: Loss=0.3780, F1 Score=0.7508 | Val: Loss=0.3585, F1 Score=0.7111\n",
            "Epoch  41/500 | Train: Loss=0.3722, F1 Score=0.7581 | Val: Loss=0.3635, F1 Score=0.7123\n",
            "Epoch  42/500 | Train: Loss=0.3735, F1 Score=0.7562 | Val: Loss=0.3639, F1 Score=0.7168\n",
            "Epoch  43/500 | Train: Loss=0.3671, F1 Score=0.7601 | Val: Loss=0.3641, F1 Score=0.7153\n",
            "Epoch  44/500 | Train: Loss=0.3647, F1 Score=0.7610 | Val: Loss=0.3625, F1 Score=0.7108\n",
            "Epoch  45/500 | Train: Loss=0.3621, F1 Score=0.7646 | Val: Loss=0.3634, F1 Score=0.7128\n",
            "Epoch  46/500 | Train: Loss=0.3620, F1 Score=0.7633 | Val: Loss=0.3623, F1 Score=0.7096\n",
            "Epoch  47/500 | Train: Loss=0.3586, F1 Score=0.7614 | Val: Loss=0.3672, F1 Score=0.7164\n",
            "Epoch  48/500 | Train: Loss=0.3578, F1 Score=0.7592 | Val: Loss=0.3648, F1 Score=0.7161\n",
            "Epoch  49/500 | Train: Loss=0.3572, F1 Score=0.7624 | Val: Loss=0.3626, F1 Score=0.7106\n",
            "Epoch  50/500 | Train: Loss=0.3540, F1 Score=0.7664 | Val: Loss=0.3629, F1 Score=0.7100\n",
            "Epoch  51/500 | Train: Loss=0.3540, F1 Score=0.7664 | Val: Loss=0.3628, F1 Score=0.7131\n",
            "Epoch  52/500 | Train: Loss=0.3518, F1 Score=0.7708 | Val: Loss=0.3633, F1 Score=0.7149\n",
            "Epoch  53/500 | Train: Loss=0.3504, F1 Score=0.7687 | Val: Loss=0.3643, F1 Score=0.7155\n",
            "Epoch  54/500 | Train: Loss=0.3492, F1 Score=0.7676 | Val: Loss=0.3639, F1 Score=0.7142\n",
            "Epoch  55/500 | Train: Loss=0.3487, F1 Score=0.7743 | Val: Loss=0.3639, F1 Score=0.7124\n",
            "Epoch  56/500 | Train: Loss=0.3484, F1 Score=0.7690 | Val: Loss=0.3635, F1 Score=0.7104\n",
            "Epoch  57/500 | Train: Loss=0.3491, F1 Score=0.7733 | Val: Loss=0.3642, F1 Score=0.7100\n",
            "Epoch  58/500 | Train: Loss=0.3477, F1 Score=0.7707 | Val: Loss=0.3625, F1 Score=0.6996\n",
            "Epoch  59/500 | Train: Loss=0.3486, F1 Score=0.7684 | Val: Loss=0.3612, F1 Score=0.7003\n",
            "Epoch  60/500 | Train: Loss=0.3441, F1 Score=0.7727 | Val: Loss=0.3614, F1 Score=0.7070\n",
            "Epoch  61/500 | Train: Loss=0.3442, F1 Score=0.7701 | Val: Loss=0.3681, F1 Score=0.7105\n",
            "Epoch  62/500 | Train: Loss=0.3449, F1 Score=0.7711 | Val: Loss=0.3655, F1 Score=0.7103\n",
            "Epoch  63/500 | Train: Loss=0.3425, F1 Score=0.7746 | Val: Loss=0.3619, F1 Score=0.7077\n",
            "Epoch  64/500 | Train: Loss=0.3438, F1 Score=0.7707 | Val: Loss=0.3610, F1 Score=0.7040\n",
            "Epoch  65/500 | Train: Loss=0.3444, F1 Score=0.7725 | Val: Loss=0.3608, F1 Score=0.7039\n",
            "Epoch  66/500 | Train: Loss=0.3404, F1 Score=0.7701 | Val: Loss=0.3623, F1 Score=0.7047\n",
            "Epoch  67/500 | Train: Loss=0.3416, F1 Score=0.7747 | Val: Loss=0.3646, F1 Score=0.7005\n",
            "Epoch  68/500 | Train: Loss=0.3408, F1 Score=0.7747 | Val: Loss=0.3608, F1 Score=0.7035\n",
            "Epoch  69/500 | Train: Loss=0.3398, F1 Score=0.7742 | Val: Loss=0.3607, F1 Score=0.7060\n",
            "Epoch  70/500 | Train: Loss=0.3380, F1 Score=0.7710 | Val: Loss=0.3604, F1 Score=0.7016\n",
            "Epoch  71/500 | Train: Loss=0.3378, F1 Score=0.7749 | Val: Loss=0.3640, F1 Score=0.7094\n",
            "Epoch  72/500 | Train: Loss=0.3381, F1 Score=0.7751 | Val: Loss=0.3628, F1 Score=0.7153\n",
            "Epoch  73/500 | Train: Loss=0.3355, F1 Score=0.7727 | Val: Loss=0.3598, F1 Score=0.7078\n",
            "Epoch  74/500 | Train: Loss=0.3356, F1 Score=0.7758 | Val: Loss=0.3602, F1 Score=0.7024\n",
            "Epoch  75/500 | Train: Loss=0.3351, F1 Score=0.7729 | Val: Loss=0.3674, F1 Score=0.7099\n",
            "Epoch  76/500 | Train: Loss=0.3362, F1 Score=0.7765 | Val: Loss=0.3617, F1 Score=0.7029\n",
            "Epoch  77/500 | Train: Loss=0.3380, F1 Score=0.7729 | Val: Loss=0.3585, F1 Score=0.7064\n",
            "Epoch  78/500 | Train: Loss=0.3351, F1 Score=0.7739 | Val: Loss=0.3586, F1 Score=0.7045\n",
            "Epoch  79/500 | Train: Loss=0.3347, F1 Score=0.7740 | Val: Loss=0.3692, F1 Score=0.7102\n",
            "Epoch  80/500 | Train: Loss=0.3343, F1 Score=0.7764 | Val: Loss=0.3598, F1 Score=0.7016\n",
            "Epoch  81/500 | Train: Loss=0.3332, F1 Score=0.7764 | Val: Loss=0.3580, F1 Score=0.7055\n",
            "Epoch  82/500 | Train: Loss=0.3311, F1 Score=0.7745 | Val: Loss=0.3619, F1 Score=0.7031\n",
            "Epoch  83/500 | Train: Loss=0.3337, F1 Score=0.7732 | Val: Loss=0.3580, F1 Score=0.7025\n",
            "Epoch  84/500 | Train: Loss=0.3330, F1 Score=0.7736 | Val: Loss=0.3569, F1 Score=0.7080\n",
            "Epoch  85/500 | Train: Loss=0.3327, F1 Score=0.7708 | Val: Loss=0.3654, F1 Score=0.7090\n",
            "Epoch  86/500 | Train: Loss=0.3335, F1 Score=0.7743 | Val: Loss=0.3617, F1 Score=0.7153\n",
            "Epoch  87/500 | Train: Loss=0.3296, F1 Score=0.7763 | Val: Loss=0.3573, F1 Score=0.7057\n",
            "Epoch  88/500 | Train: Loss=0.3309, F1 Score=0.7773 | Val: Loss=0.3581, F1 Score=0.7071\n",
            "Epoch  89/500 | Train: Loss=0.3326, F1 Score=0.7776 | Val: Loss=0.3578, F1 Score=0.7077\n",
            "Epoch  90/500 | Train: Loss=0.3289, F1 Score=0.7736 | Val: Loss=0.3595, F1 Score=0.7145\n",
            "Epoch  91/500 | Train: Loss=0.3286, F1 Score=0.7738 | Val: Loss=0.3616, F1 Score=0.7173\n",
            "Epoch  92/500 | Train: Loss=0.3288, F1 Score=0.7744 | Val: Loss=0.3597, F1 Score=0.7150\n",
            "Epoch  93/500 | Train: Loss=0.3301, F1 Score=0.7763 | Val: Loss=0.3605, F1 Score=0.7067\n",
            "Epoch  94/500 | Train: Loss=0.3272, F1 Score=0.7771 | Val: Loss=0.3579, F1 Score=0.7083\n",
            "Epoch  95/500 | Train: Loss=0.3273, F1 Score=0.7781 | Val: Loss=0.3581, F1 Score=0.7114\n",
            "Epoch  96/500 | Train: Loss=0.3258, F1 Score=0.7798 | Val: Loss=0.3628, F1 Score=0.7172\n",
            "Epoch  97/500 | Train: Loss=0.3266, F1 Score=0.7743 | Val: Loss=0.3545, F1 Score=0.7033\n",
            "Epoch  98/500 | Train: Loss=0.3239, F1 Score=0.7753 | Val: Loss=0.3550, F1 Score=0.7041\n",
            "Epoch  99/500 | Train: Loss=0.3278, F1 Score=0.7775 | Val: Loss=0.3555, F1 Score=0.7059\n",
            "Epoch 100/500 | Train: Loss=0.3270, F1 Score=0.7734 | Val: Loss=0.3605, F1 Score=0.7123\n",
            "Epoch 101/500 | Train: Loss=0.3254, F1 Score=0.7781 | Val: Loss=0.3557, F1 Score=0.7089\n",
            "Epoch 102/500 | Train: Loss=0.3239, F1 Score=0.7800 | Val: Loss=0.3588, F1 Score=0.7175\n",
            "Epoch 103/500 | Train: Loss=0.3233, F1 Score=0.7774 | Val: Loss=0.3586, F1 Score=0.7119\n",
            "Epoch 104/500 | Train: Loss=0.3242, F1 Score=0.7800 | Val: Loss=0.3545, F1 Score=0.7087\n",
            "Epoch 105/500 | Train: Loss=0.3218, F1 Score=0.7807 | Val: Loss=0.3591, F1 Score=0.7173\n",
            "Epoch 106/500 | Train: Loss=0.3220, F1 Score=0.7808 | Val: Loss=0.3585, F1 Score=0.7174\n",
            "Epoch 107/500 | Train: Loss=0.3212, F1 Score=0.7825 | Val: Loss=0.3534, F1 Score=0.7113\n",
            "Epoch 108/500 | Train: Loss=0.3205, F1 Score=0.7843 | Val: Loss=0.3530, F1 Score=0.7183\n",
            "Epoch 109/500 | Train: Loss=0.3242, F1 Score=0.7760 | Val: Loss=0.3506, F1 Score=0.7069\n",
            "Epoch 110/500 | Train: Loss=0.3225, F1 Score=0.7783 | Val: Loss=0.3497, F1 Score=0.7093\n",
            "Epoch 111/500 | Train: Loss=0.3243, F1 Score=0.7721 | Val: Loss=0.3505, F1 Score=0.7042\n",
            "Epoch 112/500 | Train: Loss=0.3219, F1 Score=0.7746 | Val: Loss=0.3517, F1 Score=0.7098\n",
            "Epoch 113/500 | Train: Loss=0.3204, F1 Score=0.7756 | Val: Loss=0.3519, F1 Score=0.7085\n",
            "Epoch 114/500 | Train: Loss=0.3193, F1 Score=0.7744 | Val: Loss=0.3570, F1 Score=0.7084\n",
            "Epoch 115/500 | Train: Loss=0.3182, F1 Score=0.7765 | Val: Loss=0.3533, F1 Score=0.7076\n",
            "Epoch 116/500 | Train: Loss=0.3200, F1 Score=0.7793 | Val: Loss=0.3518, F1 Score=0.7069\n",
            "Epoch 117/500 | Train: Loss=0.3172, F1 Score=0.7849 | Val: Loss=0.3537, F1 Score=0.7066\n",
            "Epoch 118/500 | Train: Loss=0.3185, F1 Score=0.7802 | Val: Loss=0.3488, F1 Score=0.7098\n",
            "Epoch 119/500 | Train: Loss=0.3152, F1 Score=0.7771 | Val: Loss=0.3472, F1 Score=0.7120\n",
            "Epoch 120/500 | Train: Loss=0.3149, F1 Score=0.7850 | Val: Loss=0.3443, F1 Score=0.7135\n",
            "Epoch 121/500 | Train: Loss=0.3141, F1 Score=0.7865 | Val: Loss=0.3444, F1 Score=0.7126\n",
            "Epoch 122/500 | Train: Loss=0.3164, F1 Score=0.7814 | Val: Loss=0.3389, F1 Score=0.7114\n",
            "Epoch 123/500 | Train: Loss=0.3133, F1 Score=0.7823 | Val: Loss=0.3312, F1 Score=0.7176\n",
            "Epoch 124/500 | Train: Loss=0.3077, F1 Score=0.7877 | Val: Loss=0.3202, F1 Score=0.7333\n",
            "Epoch 125/500 | Train: Loss=0.3093, F1 Score=0.7902 | Val: Loss=0.3060, F1 Score=0.7570\n",
            "Epoch 126/500 | Train: Loss=0.3255, F1 Score=0.7820 | Val: Loss=0.3191, F1 Score=0.7441\n",
            "Epoch 127/500 | Train: Loss=0.3114, F1 Score=0.7828 | Val: Loss=0.3251, F1 Score=0.7297\n",
            "Epoch 128/500 | Train: Loss=0.3092, F1 Score=0.7860 | Val: Loss=0.3169, F1 Score=0.7363\n",
            "Epoch 129/500 | Train: Loss=0.3054, F1 Score=0.7933 | Val: Loss=0.3044, F1 Score=0.7477\n",
            "Epoch 130/500 | Train: Loss=0.3016, F1 Score=0.7899 | Val: Loss=0.2974, F1 Score=0.7496\n",
            "Epoch 131/500 | Train: Loss=0.2981, F1 Score=0.7929 | Val: Loss=0.2785, F1 Score=0.7930\n",
            "Epoch 132/500 | Train: Loss=0.2984, F1 Score=0.7950 | Val: Loss=0.2775, F1 Score=0.7944\n",
            "Epoch 133/500 | Train: Loss=0.3027, F1 Score=0.7897 | Val: Loss=0.2764, F1 Score=0.7961\n",
            "Epoch 134/500 | Train: Loss=0.2995, F1 Score=0.7940 | Val: Loss=0.3021, F1 Score=0.7438\n",
            "Epoch 135/500 | Train: Loss=0.3060, F1 Score=0.7906 | Val: Loss=0.2799, F1 Score=0.7817\n",
            "Epoch 136/500 | Train: Loss=0.2970, F1 Score=0.7941 | Val: Loss=0.2798, F1 Score=0.7886\n",
            "Epoch 137/500 | Train: Loss=0.2941, F1 Score=0.7940 | Val: Loss=0.2673, F1 Score=0.8009\n",
            "Epoch 138/500 | Train: Loss=0.2887, F1 Score=0.8017 | Val: Loss=0.2693, F1 Score=0.7988\n",
            "Epoch 139/500 | Train: Loss=0.2881, F1 Score=0.8008 | Val: Loss=0.2627, F1 Score=0.8019\n",
            "Epoch 140/500 | Train: Loss=0.2839, F1 Score=0.8024 | Val: Loss=0.2572, F1 Score=0.8054\n",
            "Epoch 141/500 | Train: Loss=0.2862, F1 Score=0.8020 | Val: Loss=0.2669, F1 Score=0.7978\n",
            "Epoch 142/500 | Train: Loss=0.2880, F1 Score=0.7993 | Val: Loss=0.2601, F1 Score=0.8041\n",
            "Epoch 143/500 | Train: Loss=0.2830, F1 Score=0.8019 | Val: Loss=0.2591, F1 Score=0.8062\n",
            "Epoch 144/500 | Train: Loss=0.2824, F1 Score=0.8059 | Val: Loss=0.2602, F1 Score=0.8062\n",
            "Epoch 145/500 | Train: Loss=0.2811, F1 Score=0.8062 | Val: Loss=0.2562, F1 Score=0.8069\n",
            "Epoch 146/500 | Train: Loss=0.2827, F1 Score=0.8070 | Val: Loss=0.2625, F1 Score=0.7999\n",
            "Epoch 147/500 | Train: Loss=0.2843, F1 Score=0.8031 | Val: Loss=0.2757, F1 Score=0.7920\n",
            "Epoch 148/500 | Train: Loss=0.2856, F1 Score=0.8023 | Val: Loss=0.2682, F1 Score=0.7971\n",
            "Epoch 149/500 | Train: Loss=0.2846, F1 Score=0.8048 | Val: Loss=0.2578, F1 Score=0.8069\n",
            "Epoch 150/500 | Train: Loss=0.2846, F1 Score=0.8020 | Val: Loss=0.2590, F1 Score=0.8044\n",
            "Epoch 151/500 | Train: Loss=0.2867, F1 Score=0.8043 | Val: Loss=0.2596, F1 Score=0.8043\n",
            "Epoch 152/500 | Train: Loss=0.2868, F1 Score=0.8018 | Val: Loss=0.2572, F1 Score=0.8056\n",
            "Epoch 153/500 | Train: Loss=0.2809, F1 Score=0.8034 | Val: Loss=0.2568, F1 Score=0.8056\n",
            "Epoch 154/500 | Train: Loss=0.2809, F1 Score=0.8056 | Val: Loss=0.2570, F1 Score=0.8072\n",
            "Epoch 155/500 | Train: Loss=0.2790, F1 Score=0.8063 | Val: Loss=0.2566, F1 Score=0.8050\n",
            "Epoch 156/500 | Train: Loss=0.2773, F1 Score=0.8075 | Val: Loss=0.2544, F1 Score=0.8073\n",
            "Epoch 157/500 | Train: Loss=0.2770, F1 Score=0.8079 | Val: Loss=0.2537, F1 Score=0.8083\n",
            "Epoch 158/500 | Train: Loss=0.2803, F1 Score=0.8063 | Val: Loss=0.2547, F1 Score=0.8090\n",
            "Epoch 159/500 | Train: Loss=0.2741, F1 Score=0.8072 | Val: Loss=0.2552, F1 Score=0.8095\n",
            "Epoch 160/500 | Train: Loss=0.2762, F1 Score=0.8076 | Val: Loss=0.2538, F1 Score=0.8111\n",
            "Epoch 161/500 | Train: Loss=0.2754, F1 Score=0.8078 | Val: Loss=0.2562, F1 Score=0.8071\n",
            "Epoch 162/500 | Train: Loss=0.2802, F1 Score=0.8045 | Val: Loss=0.2596, F1 Score=0.8072\n",
            "Epoch 163/500 | Train: Loss=0.2802, F1 Score=0.8048 | Val: Loss=0.2546, F1 Score=0.8098\n",
            "Epoch 164/500 | Train: Loss=0.2766, F1 Score=0.8102 | Val: Loss=0.2534, F1 Score=0.8086\n",
            "Epoch 165/500 | Train: Loss=0.2757, F1 Score=0.8090 | Val: Loss=0.2596, F1 Score=0.8070\n",
            "Epoch 166/500 | Train: Loss=0.2751, F1 Score=0.8091 | Val: Loss=0.2561, F1 Score=0.8079\n",
            "Epoch 167/500 | Train: Loss=0.2745, F1 Score=0.8070 | Val: Loss=0.2521, F1 Score=0.8089\n",
            "Epoch 168/500 | Train: Loss=0.2728, F1 Score=0.8108 | Val: Loss=0.2574, F1 Score=0.8033\n",
            "Epoch 169/500 | Train: Loss=0.2827, F1 Score=0.8015 | Val: Loss=0.2646, F1 Score=0.8019\n",
            "Epoch 170/500 | Train: Loss=0.2801, F1 Score=0.8058 | Val: Loss=0.2567, F1 Score=0.8074\n",
            "Epoch 171/500 | Train: Loss=0.2806, F1 Score=0.8036 | Val: Loss=0.2682, F1 Score=0.7930\n",
            "Epoch 172/500 | Train: Loss=0.2789, F1 Score=0.8035 | Val: Loss=0.2608, F1 Score=0.7997\n",
            "Epoch 173/500 | Train: Loss=0.2764, F1 Score=0.8079 | Val: Loss=0.2573, F1 Score=0.8023\n",
            "Epoch 174/500 | Train: Loss=0.2741, F1 Score=0.8074 | Val: Loss=0.2593, F1 Score=0.8020\n",
            "Epoch 175/500 | Train: Loss=0.2731, F1 Score=0.8076 | Val: Loss=0.2485, F1 Score=0.8077\n",
            "Epoch 176/500 | Train: Loss=0.2700, F1 Score=0.8093 | Val: Loss=0.2546, F1 Score=0.8063\n",
            "Epoch 177/500 | Train: Loss=0.2719, F1 Score=0.8086 | Val: Loss=0.2520, F1 Score=0.8037\n",
            "Epoch 178/500 | Train: Loss=0.2710, F1 Score=0.8099 | Val: Loss=0.2544, F1 Score=0.8058\n",
            "Epoch 179/500 | Train: Loss=0.2732, F1 Score=0.8097 | Val: Loss=0.2650, F1 Score=0.7958\n",
            "Epoch 180/500 | Train: Loss=0.2749, F1 Score=0.8072 | Val: Loss=0.2539, F1 Score=0.8070\n",
            "Epoch 181/500 | Train: Loss=0.2711, F1 Score=0.8093 | Val: Loss=0.2525, F1 Score=0.8074\n",
            "Epoch 182/500 | Train: Loss=0.2715, F1 Score=0.8117 | Val: Loss=0.2540, F1 Score=0.8041\n",
            "Epoch 183/500 | Train: Loss=0.2713, F1 Score=0.8096 | Val: Loss=0.2533, F1 Score=0.8058\n",
            "Epoch 184/500 | Train: Loss=0.2693, F1 Score=0.8107 | Val: Loss=0.2514, F1 Score=0.8054\n",
            "Epoch 185/500 | Train: Loss=0.2696, F1 Score=0.8099 | Val: Loss=0.2489, F1 Score=0.8077\n",
            "Epoch 186/500 | Train: Loss=0.2656, F1 Score=0.8123 | Val: Loss=0.2590, F1 Score=0.8013\n",
            "Epoch 187/500 | Train: Loss=0.2601, F1 Score=0.8143 | Val: Loss=0.2488, F1 Score=0.8063\n",
            "Epoch 188/500 | Train: Loss=0.2568, F1 Score=0.8145 | Val: Loss=0.2592, F1 Score=0.7975\n",
            "Epoch 189/500 | Train: Loss=0.2551, F1 Score=0.8132 | Val: Loss=0.2693, F1 Score=0.7961\n",
            "Epoch 190/500 | Train: Loss=0.2603, F1 Score=0.8121 | Val: Loss=0.2516, F1 Score=0.8064\n",
            "Epoch 191/500 | Train: Loss=0.2555, F1 Score=0.8131 | Val: Loss=0.2523, F1 Score=0.8018\n",
            "Epoch 192/500 | Train: Loss=0.2476, F1 Score=0.8189 | Val: Loss=0.2584, F1 Score=0.8009\n",
            "Epoch 193/500 | Train: Loss=0.2425, F1 Score=0.8200 | Val: Loss=0.2532, F1 Score=0.8046\n",
            "Epoch 194/500 | Train: Loss=0.2434, F1 Score=0.8189 | Val: Loss=0.2571, F1 Score=0.8094\n",
            "Epoch 195/500 | Train: Loss=0.2355, F1 Score=0.8273 | Val: Loss=0.2504, F1 Score=0.8093\n",
            "Epoch 196/500 | Train: Loss=0.2329, F1 Score=0.8255 | Val: Loss=0.2521, F1 Score=0.8097\n",
            "Epoch 197/500 | Train: Loss=0.2317, F1 Score=0.8281 | Val: Loss=0.2527, F1 Score=0.8173\n",
            "Epoch 198/500 | Train: Loss=0.2310, F1 Score=0.8303 | Val: Loss=0.2511, F1 Score=0.8141\n",
            "Epoch 199/500 | Train: Loss=0.2270, F1 Score=0.8301 | Val: Loss=0.2519, F1 Score=0.8155\n",
            "Epoch 200/500 | Train: Loss=0.2295, F1 Score=0.8329 | Val: Loss=0.2570, F1 Score=0.8192\n",
            "Epoch 201/500 | Train: Loss=0.2246, F1 Score=0.8333 | Val: Loss=0.2516, F1 Score=0.8140\n",
            "Epoch 202/500 | Train: Loss=0.2265, F1 Score=0.8309 | Val: Loss=0.2531, F1 Score=0.8182\n",
            "Epoch 203/500 | Train: Loss=0.2239, F1 Score=0.8339 | Val: Loss=0.2540, F1 Score=0.8173\n",
            "Epoch 204/500 | Train: Loss=0.2275, F1 Score=0.8324 | Val: Loss=0.2570, F1 Score=0.8142\n",
            "Epoch 205/500 | Train: Loss=0.2247, F1 Score=0.8312 | Val: Loss=0.2553, F1 Score=0.8163\n",
            "Epoch 206/500 | Train: Loss=0.2237, F1 Score=0.8357 | Val: Loss=0.2505, F1 Score=0.8209\n",
            "Epoch 207/500 | Train: Loss=0.2195, F1 Score=0.8370 | Val: Loss=0.2538, F1 Score=0.8187\n",
            "Epoch 208/500 | Train: Loss=0.2211, F1 Score=0.8366 | Val: Loss=0.2524, F1 Score=0.8179\n",
            "Epoch 209/500 | Train: Loss=0.2245, F1 Score=0.8336 | Val: Loss=0.2511, F1 Score=0.8194\n",
            "Epoch 210/500 | Train: Loss=0.2252, F1 Score=0.8337 | Val: Loss=0.2555, F1 Score=0.8101\n",
            "Epoch 211/500 | Train: Loss=0.2213, F1 Score=0.8346 | Val: Loss=0.2515, F1 Score=0.8209\n",
            "Epoch 212/500 | Train: Loss=0.2174, F1 Score=0.8366 | Val: Loss=0.2466, F1 Score=0.8200\n",
            "Epoch 213/500 | Train: Loss=0.2160, F1 Score=0.8374 | Val: Loss=0.2553, F1 Score=0.8200\n",
            "Epoch 214/500 | Train: Loss=0.2198, F1 Score=0.8361 | Val: Loss=0.2523, F1 Score=0.8225\n",
            "Epoch 215/500 | Train: Loss=0.2188, F1 Score=0.8372 | Val: Loss=0.2583, F1 Score=0.8097\n",
            "Epoch 216/500 | Train: Loss=0.2246, F1 Score=0.8348 | Val: Loss=0.2498, F1 Score=0.8232\n",
            "Epoch 217/500 | Train: Loss=0.2177, F1 Score=0.8378 | Val: Loss=0.2504, F1 Score=0.8175\n",
            "Epoch 218/500 | Train: Loss=0.2144, F1 Score=0.8387 | Val: Loss=0.2511, F1 Score=0.8258\n",
            "Epoch 219/500 | Train: Loss=0.2118, F1 Score=0.8412 | Val: Loss=0.2454, F1 Score=0.8255\n",
            "Epoch 220/500 | Train: Loss=0.2134, F1 Score=0.8391 | Val: Loss=0.2598, F1 Score=0.8169\n",
            "Epoch 221/500 | Train: Loss=0.2183, F1 Score=0.8389 | Val: Loss=0.2550, F1 Score=0.8192\n",
            "Epoch 222/500 | Train: Loss=0.2267, F1 Score=0.8342 | Val: Loss=0.2499, F1 Score=0.8202\n",
            "Epoch 223/500 | Train: Loss=0.2270, F1 Score=0.8311 | Val: Loss=0.2525, F1 Score=0.8077\n",
            "Epoch 224/500 | Train: Loss=0.2240, F1 Score=0.8350 | Val: Loss=0.2532, F1 Score=0.8228\n",
            "Epoch 225/500 | Train: Loss=0.2201, F1 Score=0.8360 | Val: Loss=0.2761, F1 Score=0.8080\n",
            "Epoch 226/500 | Train: Loss=0.2222, F1 Score=0.8371 | Val: Loss=0.2485, F1 Score=0.8237\n",
            "Epoch 227/500 | Train: Loss=0.2165, F1 Score=0.8371 | Val: Loss=0.2511, F1 Score=0.8174\n",
            "Epoch 228/500 | Train: Loss=0.2101, F1 Score=0.8400 | Val: Loss=0.2486, F1 Score=0.8281\n",
            "Epoch 229/500 | Train: Loss=0.2157, F1 Score=0.8360 | Val: Loss=0.2565, F1 Score=0.8169\n",
            "Epoch 230/500 | Train: Loss=0.2154, F1 Score=0.8385 | Val: Loss=0.2400, F1 Score=0.8280\n",
            "Epoch 231/500 | Train: Loss=0.2150, F1 Score=0.8400 | Val: Loss=0.2449, F1 Score=0.8197\n",
            "Epoch 232/500 | Train: Loss=0.2139, F1 Score=0.8401 | Val: Loss=0.2472, F1 Score=0.8213\n",
            "Epoch 233/500 | Train: Loss=0.2124, F1 Score=0.8409 | Val: Loss=0.2500, F1 Score=0.8199\n",
            "Epoch 234/500 | Train: Loss=0.2107, F1 Score=0.8414 | Val: Loss=0.2406, F1 Score=0.8263\n",
            "Epoch 235/500 | Train: Loss=0.2093, F1 Score=0.8419 | Val: Loss=0.2389, F1 Score=0.8293\n",
            "Epoch 236/500 | Train: Loss=0.2087, F1 Score=0.8413 | Val: Loss=0.2442, F1 Score=0.8268\n",
            "Epoch 237/500 | Train: Loss=0.2122, F1 Score=0.8412 | Val: Loss=0.2508, F1 Score=0.8208\n",
            "Epoch 238/500 | Train: Loss=0.2168, F1 Score=0.8379 | Val: Loss=0.2446, F1 Score=0.8231\n",
            "Epoch 239/500 | Train: Loss=0.2109, F1 Score=0.8418 | Val: Loss=0.2460, F1 Score=0.8212\n",
            "Epoch 240/500 | Train: Loss=0.2069, F1 Score=0.8426 | Val: Loss=0.2411, F1 Score=0.8246\n",
            "Epoch 241/500 | Train: Loss=0.2093, F1 Score=0.8418 | Val: Loss=0.2433, F1 Score=0.8263\n",
            "Epoch 242/500 | Train: Loss=0.2099, F1 Score=0.8411 | Val: Loss=0.2442, F1 Score=0.8267\n",
            "Epoch 243/500 | Train: Loss=0.2122, F1 Score=0.8414 | Val: Loss=0.2454, F1 Score=0.8235\n",
            "Epoch 244/500 | Train: Loss=0.2153, F1 Score=0.8395 | Val: Loss=0.2414, F1 Score=0.8281\n",
            "Epoch 245/500 | Train: Loss=0.2088, F1 Score=0.8413 | Val: Loss=0.2548, F1 Score=0.8151\n",
            "Epoch 246/500 | Train: Loss=0.2122, F1 Score=0.8410 | Val: Loss=0.2413, F1 Score=0.8265\n",
            "Epoch 247/500 | Train: Loss=0.2143, F1 Score=0.8397 | Val: Loss=0.2455, F1 Score=0.8192\n",
            "Epoch 248/500 | Train: Loss=0.2108, F1 Score=0.8419 | Val: Loss=0.2363, F1 Score=0.8287\n",
            "Epoch 249/500 | Train: Loss=0.2075, F1 Score=0.8424 | Val: Loss=0.2399, F1 Score=0.8265\n",
            "Epoch 250/500 | Train: Loss=0.2051, F1 Score=0.8445 | Val: Loss=0.2449, F1 Score=0.8235\n",
            "Epoch 251/500 | Train: Loss=0.2059, F1 Score=0.8444 | Val: Loss=0.2451, F1 Score=0.8238\n",
            "Epoch 252/500 | Train: Loss=0.2054, F1 Score=0.8444 | Val: Loss=0.2391, F1 Score=0.8288\n",
            "Epoch 253/500 | Train: Loss=0.2054, F1 Score=0.8448 | Val: Loss=0.2403, F1 Score=0.8304\n",
            "Epoch 254/500 | Train: Loss=0.2062, F1 Score=0.8453 | Val: Loss=0.2459, F1 Score=0.8230\n",
            "Epoch 255/500 | Train: Loss=0.2027, F1 Score=0.8469 | Val: Loss=0.2399, F1 Score=0.8281\n",
            "Epoch 256/500 | Train: Loss=0.2019, F1 Score=0.8451 | Val: Loss=0.2497, F1 Score=0.8203\n",
            "Epoch 257/500 | Train: Loss=0.2005, F1 Score=0.8458 | Val: Loss=0.2438, F1 Score=0.8300\n",
            "Epoch 258/500 | Train: Loss=0.1997, F1 Score=0.8465 | Val: Loss=0.2467, F1 Score=0.8226\n",
            "Epoch 259/500 | Train: Loss=0.2012, F1 Score=0.8456 | Val: Loss=0.2464, F1 Score=0.8256\n",
            "Epoch 260/500 | Train: Loss=0.1995, F1 Score=0.8448 | Val: Loss=0.2430, F1 Score=0.8260\n",
            "Epoch 261/500 | Train: Loss=0.2010, F1 Score=0.8466 | Val: Loss=0.2486, F1 Score=0.8227\n",
            "Epoch 262/500 | Train: Loss=0.1996, F1 Score=0.8472 | Val: Loss=0.2445, F1 Score=0.8281\n",
            "Epoch 263/500 | Train: Loss=0.2020, F1 Score=0.8468 | Val: Loss=0.2557, F1 Score=0.8182\n",
            "Epoch 264/500 | Train: Loss=0.2007, F1 Score=0.8473 | Val: Loss=0.2368, F1 Score=0.8280\n",
            "Epoch 265/500 | Train: Loss=0.1999, F1 Score=0.8457 | Val: Loss=0.2519, F1 Score=0.8202\n",
            "Epoch 266/500 | Train: Loss=0.1983, F1 Score=0.8489 | Val: Loss=0.2514, F1 Score=0.8194\n",
            "Epoch 267/500 | Train: Loss=0.2069, F1 Score=0.8431 | Val: Loss=0.2389, F1 Score=0.8299\n",
            "Epoch 268/500 | Train: Loss=0.2177, F1 Score=0.8402 | Val: Loss=0.2606, F1 Score=0.8106\n",
            "Epoch 269/500 | Train: Loss=0.2152, F1 Score=0.8382 | Val: Loss=0.2428, F1 Score=0.8196\n",
            "Epoch 270/500 | Train: Loss=0.2066, F1 Score=0.8429 | Val: Loss=0.2556, F1 Score=0.8215\n",
            "Epoch 271/500 | Train: Loss=0.2031, F1 Score=0.8460 | Val: Loss=0.2392, F1 Score=0.8256\n",
            "Epoch 272/500 | Train: Loss=0.2004, F1 Score=0.8451 | Val: Loss=0.2365, F1 Score=0.8318\n",
            "Epoch 273/500 | Train: Loss=0.1991, F1 Score=0.8473 | Val: Loss=0.2443, F1 Score=0.8251\n",
            "Epoch 274/500 | Train: Loss=0.2001, F1 Score=0.8488 | Val: Loss=0.2402, F1 Score=0.8289\n",
            "Epoch 275/500 | Train: Loss=0.1934, F1 Score=0.8505 | Val: Loss=0.2372, F1 Score=0.8315\n",
            "Epoch 276/500 | Train: Loss=0.1971, F1 Score=0.8492 | Val: Loss=0.2422, F1 Score=0.8269\n",
            "Epoch 277/500 | Train: Loss=0.2005, F1 Score=0.8471 | Val: Loss=0.2574, F1 Score=0.8198\n",
            "Epoch 278/500 | Train: Loss=0.2022, F1 Score=0.8471 | Val: Loss=0.2399, F1 Score=0.8262\n",
            "Epoch 279/500 | Train: Loss=0.1984, F1 Score=0.8490 | Val: Loss=0.2370, F1 Score=0.8297\n",
            "Epoch 280/500 | Train: Loss=0.1963, F1 Score=0.8501 | Val: Loss=0.2362, F1 Score=0.8287\n",
            "Epoch 281/500 | Train: Loss=0.1991, F1 Score=0.8471 | Val: Loss=0.2557, F1 Score=0.8183\n",
            "Epoch 282/500 | Train: Loss=0.1965, F1 Score=0.8481 | Val: Loss=0.2379, F1 Score=0.8292\n",
            "Epoch 283/500 | Train: Loss=0.1962, F1 Score=0.8492 | Val: Loss=0.2370, F1 Score=0.8311\n",
            "Epoch 284/500 | Train: Loss=0.1938, F1 Score=0.8508 | Val: Loss=0.2575, F1 Score=0.8210\n",
            "Epoch 285/500 | Train: Loss=0.1957, F1 Score=0.8488 | Val: Loss=0.2409, F1 Score=0.8275\n",
            "Epoch 286/500 | Train: Loss=0.1926, F1 Score=0.8511 | Val: Loss=0.2548, F1 Score=0.8214\n",
            "Epoch 287/500 | Train: Loss=0.1996, F1 Score=0.8480 | Val: Loss=0.2435, F1 Score=0.8287\n",
            "Epoch 288/500 | Train: Loss=0.1943, F1 Score=0.8488 | Val: Loss=0.2358, F1 Score=0.8320\n",
            "Epoch 289/500 | Train: Loss=0.1938, F1 Score=0.8493 | Val: Loss=0.2307, F1 Score=0.8320\n",
            "Epoch 290/500 | Train: Loss=0.2015, F1 Score=0.8458 | Val: Loss=0.2527, F1 Score=0.8219\n",
            "Epoch 291/500 | Train: Loss=0.2023, F1 Score=0.8475 | Val: Loss=0.2371, F1 Score=0.8287\n",
            "Epoch 292/500 | Train: Loss=0.2002, F1 Score=0.8460 | Val: Loss=0.2281, F1 Score=0.8300\n",
            "Epoch 293/500 | Train: Loss=0.1971, F1 Score=0.8470 | Val: Loss=0.2371, F1 Score=0.8296\n",
            "Epoch 294/500 | Train: Loss=0.1928, F1 Score=0.8515 | Val: Loss=0.2398, F1 Score=0.8286\n",
            "Epoch 295/500 | Train: Loss=0.1932, F1 Score=0.8504 | Val: Loss=0.2445, F1 Score=0.8283\n",
            "Epoch 296/500 | Train: Loss=0.1904, F1 Score=0.8510 | Val: Loss=0.2345, F1 Score=0.8305\n",
            "Epoch 297/500 | Train: Loss=0.1930, F1 Score=0.8488 | Val: Loss=0.2491, F1 Score=0.8228\n",
            "Epoch 298/500 | Train: Loss=0.1967, F1 Score=0.8498 | Val: Loss=0.2357, F1 Score=0.8303\n",
            "Epoch 299/500 | Train: Loss=0.1936, F1 Score=0.8513 | Val: Loss=0.2445, F1 Score=0.8270\n",
            "Epoch 300/500 | Train: Loss=0.1892, F1 Score=0.8531 | Val: Loss=0.2469, F1 Score=0.8243\n",
            "Epoch 301/500 | Train: Loss=0.1878, F1 Score=0.8550 | Val: Loss=0.2418, F1 Score=0.8308\n",
            "Epoch 302/500 | Train: Loss=0.1908, F1 Score=0.8513 | Val: Loss=0.2392, F1 Score=0.8315\n",
            "Epoch 303/500 | Train: Loss=0.1897, F1 Score=0.8519 | Val: Loss=0.2368, F1 Score=0.8317\n",
            "Epoch 304/500 | Train: Loss=0.1911, F1 Score=0.8515 | Val: Loss=0.2563, F1 Score=0.8211\n",
            "Epoch 305/500 | Train: Loss=0.1883, F1 Score=0.8529 | Val: Loss=0.2423, F1 Score=0.8278\n",
            "Epoch 306/500 | Train: Loss=0.1898, F1 Score=0.8514 | Val: Loss=0.2361, F1 Score=0.8325\n",
            "Epoch 307/500 | Train: Loss=0.1927, F1 Score=0.8499 | Val: Loss=0.2541, F1 Score=0.8224\n",
            "Epoch 308/500 | Train: Loss=0.1989, F1 Score=0.8465 | Val: Loss=0.2419, F1 Score=0.8264\n",
            "Epoch 309/500 | Train: Loss=0.1968, F1 Score=0.8477 | Val: Loss=0.2344, F1 Score=0.8313\n",
            "Epoch 310/500 | Train: Loss=0.1927, F1 Score=0.8515 | Val: Loss=0.2324, F1 Score=0.8323\n",
            "Epoch 311/500 | Train: Loss=0.1902, F1 Score=0.8523 | Val: Loss=0.2370, F1 Score=0.8329\n",
            "Epoch 312/500 | Train: Loss=0.1977, F1 Score=0.8475 | Val: Loss=0.2964, F1 Score=0.8080\n",
            "Epoch 313/500 | Train: Loss=0.1996, F1 Score=0.8480 | Val: Loss=0.2416, F1 Score=0.8268\n",
            "Epoch 314/500 | Train: Loss=0.1948, F1 Score=0.8497 | Val: Loss=0.2323, F1 Score=0.8319\n",
            "Epoch 315/500 | Train: Loss=0.1945, F1 Score=0.8493 | Val: Loss=0.2378, F1 Score=0.8308\n",
            "Epoch 316/500 | Train: Loss=0.1914, F1 Score=0.8507 | Val: Loss=0.2308, F1 Score=0.8353\n",
            "Epoch 317/500 | Train: Loss=0.1882, F1 Score=0.8528 | Val: Loss=0.2534, F1 Score=0.8244\n",
            "Epoch 318/500 | Train: Loss=0.1916, F1 Score=0.8510 | Val: Loss=0.2320, F1 Score=0.8329\n",
            "Epoch 319/500 | Train: Loss=0.1911, F1 Score=0.8515 | Val: Loss=0.2348, F1 Score=0.8294\n",
            "Epoch 320/500 | Train: Loss=0.1946, F1 Score=0.8521 | Val: Loss=0.2331, F1 Score=0.8306\n",
            "Epoch 321/500 | Train: Loss=0.1919, F1 Score=0.8500 | Val: Loss=0.2265, F1 Score=0.8342\n",
            "Epoch 322/500 | Train: Loss=0.1878, F1 Score=0.8533 | Val: Loss=0.2376, F1 Score=0.8310\n",
            "Epoch 323/500 | Train: Loss=0.1857, F1 Score=0.8548 | Val: Loss=0.2323, F1 Score=0.8338\n",
            "Epoch 324/500 | Train: Loss=0.1866, F1 Score=0.8541 | Val: Loss=0.2430, F1 Score=0.8327\n",
            "Epoch 325/500 | Train: Loss=0.1872, F1 Score=0.8525 | Val: Loss=0.2280, F1 Score=0.8348\n",
            "Epoch 326/500 | Train: Loss=0.1905, F1 Score=0.8532 | Val: Loss=0.2387, F1 Score=0.8322\n",
            "Epoch 327/500 | Train: Loss=0.1937, F1 Score=0.8508 | Val: Loss=0.2308, F1 Score=0.8346\n",
            "Epoch 328/500 | Train: Loss=0.1892, F1 Score=0.8522 | Val: Loss=0.2726, F1 Score=0.8198\n",
            "Epoch 329/500 | Train: Loss=0.1907, F1 Score=0.8510 | Val: Loss=0.2526, F1 Score=0.8260\n",
            "Epoch 330/500 | Train: Loss=0.1901, F1 Score=0.8526 | Val: Loss=0.2270, F1 Score=0.8354\n",
            "Epoch 331/500 | Train: Loss=0.1889, F1 Score=0.8519 | Val: Loss=0.2558, F1 Score=0.8254\n",
            "Epoch 332/500 | Train: Loss=0.1883, F1 Score=0.8533 | Val: Loss=0.2684, F1 Score=0.8219\n",
            "Epoch 333/500 | Train: Loss=0.1875, F1 Score=0.8516 | Val: Loss=0.2395, F1 Score=0.8323\n",
            "Epoch 334/500 | Train: Loss=0.1944, F1 Score=0.8494 | Val: Loss=0.2340, F1 Score=0.8341\n",
            "Epoch 335/500 | Train: Loss=0.1983, F1 Score=0.8471 | Val: Loss=0.2288, F1 Score=0.8338\n",
            "Epoch 336/500 | Train: Loss=0.1944, F1 Score=0.8487 | Val: Loss=0.2314, F1 Score=0.8337\n",
            "Epoch 337/500 | Train: Loss=0.1914, F1 Score=0.8512 | Val: Loss=0.2350, F1 Score=0.8324\n",
            "Epoch 338/500 | Train: Loss=0.1852, F1 Score=0.8534 | Val: Loss=0.2289, F1 Score=0.8353\n",
            "Epoch 339/500 | Train: Loss=0.1840, F1 Score=0.8548 | Val: Loss=0.2370, F1 Score=0.8320\n",
            "Epoch 340/500 | Train: Loss=0.1831, F1 Score=0.8548 | Val: Loss=0.2282, F1 Score=0.8356\n",
            "Epoch 341/500 | Train: Loss=0.1848, F1 Score=0.8550 | Val: Loss=0.2436, F1 Score=0.8307\n",
            "Epoch 342/500 | Train: Loss=0.1792, F1 Score=0.8555 | Val: Loss=0.2728, F1 Score=0.8213\n",
            "Epoch 343/500 | Train: Loss=0.1912, F1 Score=0.8513 | Val: Loss=0.2330, F1 Score=0.8360\n",
            "Epoch 344/500 | Train: Loss=0.1907, F1 Score=0.8513 | Val: Loss=0.2324, F1 Score=0.8331\n",
            "Epoch 345/500 | Train: Loss=0.1846, F1 Score=0.8524 | Val: Loss=0.2354, F1 Score=0.8352\n",
            "Epoch 346/500 | Train: Loss=0.1819, F1 Score=0.8552 | Val: Loss=0.2346, F1 Score=0.8346\n",
            "Epoch 347/500 | Train: Loss=0.1840, F1 Score=0.8550 | Val: Loss=0.2335, F1 Score=0.8346\n",
            "Epoch 348/500 | Train: Loss=0.1829, F1 Score=0.8561 | Val: Loss=0.2342, F1 Score=0.8357\n",
            "Epoch 349/500 | Train: Loss=0.1825, F1 Score=0.8566 | Val: Loss=0.2386, F1 Score=0.8344\n",
            "Epoch 350/500 | Train: Loss=0.1831, F1 Score=0.8552 | Val: Loss=0.2442, F1 Score=0.8318\n",
            "Epoch 351/500 | Train: Loss=0.1796, F1 Score=0.8564 | Val: Loss=0.2392, F1 Score=0.8323\n",
            "Epoch 352/500 | Train: Loss=0.1797, F1 Score=0.8571 | Val: Loss=0.2439, F1 Score=0.8314\n",
            "Epoch 353/500 | Train: Loss=0.1781, F1 Score=0.8558 | Val: Loss=0.2416, F1 Score=0.8333\n",
            "Epoch 354/500 | Train: Loss=0.1794, F1 Score=0.8570 | Val: Loss=0.2405, F1 Score=0.8346\n",
            "Epoch 355/500 | Train: Loss=0.1789, F1 Score=0.8583 | Val: Loss=0.2403, F1 Score=0.8339\n",
            "Epoch 356/500 | Train: Loss=0.1811, F1 Score=0.8561 | Val: Loss=0.2733, F1 Score=0.8225\n",
            "Epoch 357/500 | Train: Loss=0.1925, F1 Score=0.8484 | Val: Loss=0.2720, F1 Score=0.8219\n",
            "Epoch 358/500 | Train: Loss=0.1852, F1 Score=0.8540 | Val: Loss=0.2545, F1 Score=0.8282\n",
            "Epoch 359/500 | Train: Loss=0.1805, F1 Score=0.8558 | Val: Loss=0.2275, F1 Score=0.8374\n",
            "Epoch 360/500 | Train: Loss=0.1866, F1 Score=0.8532 | Val: Loss=0.2300, F1 Score=0.8347\n",
            "Epoch 361/500 | Train: Loss=0.1813, F1 Score=0.8557 | Val: Loss=0.2343, F1 Score=0.8348\n",
            "Early stopping triggered after 361 epochs.\n",
            "Best model restored from epoch 321 with val_loss 0.2265 and with val_f1 0.8342\n",
            "CPU times: user 1min 18s, sys: 1min 1s, total: 2min 19s\n",
            "Wall time: 3min 46s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "print(f'Running {len(grid)} configurations...')\n",
        "\n",
        "import os, math, copy, time\n",
        "\n",
        "# --- ensure output directories exist ---\n",
        "os.makedirs(str(logs_dir), exist_ok=True)\n",
        "\n",
        "best_val_f1 = float('-inf')\n",
        "best_params = None\n",
        "best_training_history = None\n",
        "best_state_dict = None\n",
        "best_model_path = None\n",
        "best_run_idx = None\n",
        "best_epoch_in_run = None\n",
        "results = []\n",
        "\n",
        "\n",
        "for idx, params in enumerate(grid, 1):\n",
        "    start_time = time.perf_counter()\n",
        "    print(f\"\\nConfiguration {idx}/{len(grid)}: {params}\")\n",
        "    #Set up Criterion\n",
        "    weights = torch.tensor(params['WEIGHTS']).to(device)\n",
        "\n",
        "    if params['CRITERION'] == 'CROSS':\n",
        "      criterion = nn.CrossEntropyLoss(weight=weights)\n",
        "    else:\n",
        "      criterion = FocalLoss(alpha=None, gamma=params['GAMMA'])\n",
        "    #Build Sequence for the grid step\n",
        "    WINDOW_GRID = params['WINDOW']\n",
        "    STRIDE_GRID = params['STRIDE']\n",
        "    # Generate sequences and labels for the training set\n",
        "    X_train, y_train = build_sequences(df_train, WINDOW_GRID, STRIDE_GRID)\n",
        "    # Generate sequences and labels for the validation set\n",
        "    X_val, y_val = build_sequences(df_val, WINDOW_GRID, STRIDE_GRID)\n",
        "    X_train = X_train.astype('float32')\n",
        "    X_val = X_val.astype('float32')\n",
        "    # Define the input shape based on the training data\n",
        "    input_shape = X_train.shape[1:]\n",
        "    # Define the number of classes based on the categorical labels\n",
        "    num_classes = len(np.unique(y_train))\n",
        "    # Discard nan values from the dataset\n",
        "    if np.isnan(X_train).any() or np.isnan(X_val).any() or np.isnan(X_test).any():\n",
        "        X_train = np.nan_to_num(X_train)\n",
        "        X_val = np.nan_to_num(X_val)\n",
        "    train_ds = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n",
        "    val_ds   = TensorDataset(torch.from_numpy(X_val), torch.from_numpy(y_val))\n",
        "    train_loader = make_loader(train_ds, batch_size=params['BATCH_SIZE'], shuffle=True, drop_last=False)\n",
        "    val_loader   = make_loader(val_ds, batch_size=params['BATCH_SIZE'], shuffle=False, drop_last=False)\n",
        "    print(f\"Training set size: {len(train_ds)}\")\n",
        "    print(f\"Validation set size: {len(val_ds)}\")\n",
        "\n",
        "\n",
        "    # Build model\n",
        "    rnn_model = FlexibleRecurrentClassifier(\n",
        "        input_size=input_shape[-1],\n",
        "        hidden_size=params['HIDDEN_SIZE'],\n",
        "        num_classes=num_classes,\n",
        "        dropout_rate=params['DROPOUT_RATE'],\n",
        "        bidirectional=BIDIRECTIONAL,\n",
        "        rnn_type=RNN_TYPE\n",
        "    ).to(device)\n",
        "\n",
        "    # Display architecture summary\n",
        "    try:\n",
        "        recurrent_summary(rnn_model, input_size=input_shape)\n",
        "    except Exception as e:\n",
        "        print(f\"[warn] recurrent_summary failed: {e}\")\n",
        "\n",
        "    # Set up TensorBoard writer\n",
        "    experiment_id = f\"{EXPERIMENT_NAME}_{idx}\"\n",
        "    writer = SummaryWriter(f\"./{logs_dir}/{experiment_id}\")\n",
        "\n",
        "    # Add model graph only once to save time/disk\n",
        "    if idx == 1:\n",
        "        try:\n",
        "            x = torch.randn(1, input_shape[0], input_shape[1]).to(device)\n",
        "            x_for_graph = x if getattr(rnn_model, \"batch_first\", True) else x.permute(1, 0, 2)\n",
        "            writer.add_graph(rnn_model, x_for_graph)\n",
        "        except Exception as e:\n",
        "            print(f\"[warn] Skipping add_graph: {e}\")\n",
        "\n",
        "    # Optimizer and AMP scaler\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        rnn_model.parameters(),\n",
        "        lr=params['LEARNING_RATE'],\n",
        "        weight_decay=params['L2_LAMBDA']\n",
        "    )\n",
        "    scaler = torch.cuda.amp.GradScaler(enabled=(device.type == 'cuda'))\n",
        "\n",
        "    # --- train model ---\n",
        "    try:\n",
        "        rnn_model, training_history = fit(\n",
        "            model=rnn_model,\n",
        "            train_loader=train_loader,\n",
        "            val_loader=val_loader,\n",
        "            l1_lambda=params['L1_LAMBDA'],\n",
        "            l2_lambda=0.0, # Always set to zero because we are applying L2 Regularization in Optimizer\n",
        "            epochs=GRID_EPOCHS,\n",
        "            criterion=criterion,\n",
        "            optimizer=optimizer,\n",
        "            scaler=scaler,\n",
        "            device=device,\n",
        "            writer=None, # No Tensorboard saving\n",
        "            verbose=10,\n",
        "            experiment_name=experiment_id,\n",
        "            patience=GRID_PATIENCE,\n",
        "            save_model=False # No model saving for grid search runs\n",
        "        )\n",
        "\n",
        "        # Extract metrics\n",
        "        val_f1_series = [float(v) for v in training_history.get('val_f1', [])\n",
        "                         if isinstance(v, (int, float)) and math.isfinite(v)]\n",
        "        val_loss_series = [float(v) for v in training_history.get('val_loss', [])\n",
        "                           if isinstance(v, (int, float)) and math.isfinite(v)]\n",
        "\n",
        "        if val_f1_series and val_loss_series:\n",
        "            run_best_f1 = max(val_f1_series)\n",
        "            run_best_epoch = val_f1_series.index(run_best_f1) + 1\n",
        "            run_best_val_loss = val_loss_series[run_best_epoch - 1]\n",
        "            elapsed = time.perf_counter() - start_time\n",
        "\n",
        "            print(f\"[Run {idx}] Best val_f1 = {run_best_f1:.4f} (epoch {run_best_epoch})\")\n",
        "\n",
        "            # Save metrics for summary table\n",
        "            results.append({\n",
        "                'Run': idx,\n",
        "                'Best_Epoch': run_best_epoch,\n",
        "                'Best_Val_F1': run_best_f1,\n",
        "                'Best_Val_Loss': run_best_val_loss,\n",
        "                'Elapsed_s': elapsed,\n",
        "                **params\n",
        "            })\n",
        "\n",
        "            # Track best model\n",
        "            if run_best_f1 > best_val_f1:\n",
        "                best_val_f1 = run_best_f1\n",
        "                best_params = params\n",
        "                best_training_history = training_history\n",
        "                best_state_dict = copy.deepcopy(rnn_model.state_dict())\n",
        "                best_run_idx = idx\n",
        "                best_epoch_in_run = run_best_epoch\n",
        "                #best_model_path = f\"./{logs_dir}/{EXPERIMENT_NAME}_best.pt\"\n",
        "                #torch.save(best_state_dict, best_model_path)\n",
        "\n",
        "        else:\n",
        "            print(\"[warn] No valid val_f1 or val_loss values recorded for this run.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"[error] Training failed for configuration {idx}: {e}\")\n",
        "\n",
        "    finally:\n",
        "        try:\n",
        "            writer.close()\n",
        "        except Exception:\n",
        "            pass\n",
        "        del optimizer, scaler, rnn_model\n",
        "        if device.type == 'cuda':\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    print(f\"Configuration {idx} completed in {time.perf_counter() - start_time:.1f}s\")\n",
        "\n",
        "# --- summary ---\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"                GRID SEARCH COMPLETE\")\n",
        "print(\"=\"*50)\n",
        "if best_params is not None:\n",
        "    print(f\"Best run: #{best_run_idx} (epoch {best_epoch_in_run})\")\n",
        "    print(f\"Best Validation F1 Score: {best_val_f1:.4f}\")\n",
        "    print(f\"Best Parameters: {best_params}\")\n",
        "    if best_model_path:\n",
        "        print(f\"Best model saved to: {best_model_path}\")\n",
        "else:\n",
        "    print(\"No successful runs (val_f1 was empty or invalid).\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# --- results table ---\n",
        "if results:\n",
        "    df_results = pd.DataFrame(results)\n",
        "    df_results = df_results.sort_values(by='Best_Val_F1', ascending=False).reset_index(drop=True)\n",
        "\n",
        "    print(\"\\nGrid Search Results Summary:\")\n",
        "    print(df_results.to_string(index=False))\n",
        "\n",
        "    # Optionally save results to CSV\n",
        "    results_path = f\"./{logs_dir}/{EXPERIMENT_NAME}_grid_results.csv\"\n",
        "    df_results.to_csv(results_path, index=False)\n",
        "    print(f\"\\nResults saved to: {results_path}\")\n",
        "else:\n",
        "    print(\"No results to display.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
